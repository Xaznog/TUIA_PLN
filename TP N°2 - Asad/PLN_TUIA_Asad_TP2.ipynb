{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t5u1osKts6Ef",
        "i6WFbNiVvj2_",
        "NQ7AQaF6vnI1",
        "jekVJmQhv8xd",
        "ifQkA3t3v_V-",
        "fZmkhuNOo8uf",
        "Am4bHggtpmMD",
        "hkzICjMlpu6g",
        "lFJrYJ2KTnTO",
        "ahBg486gAfbQ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del Lenguaje Natural - Trabajo Práctico N°2 - 2024\n",
        "## Rajas de Ganges Chatbot\n",
        "\n",
        "Desarrollado por:\n",
        "- Asad, Gonzalo (A-4595/1)"
      ],
      "metadata": {
        "id": "MtoTIADFs00z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "RiIIka9gs3W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación del Entorno"
      ],
      "metadata": {
        "id": "t5u1osKts6Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install -y chromium-chromedriver\n",
        "# !pip install selenium\n",
        "# import os\n",
        "# os.environ[\"PATH\"] += \":/usr/bin/chromedriver\""
      ],
      "metadata": {
        "id": "ELYCCnCp6_in"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de librerías"
      ],
      "metadata": {
        "id": "HvuaR9EAs9OC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ZIj5HVPdi3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d03de23f-1e78-42f5-b2a7-ce636835bac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting redisgraph\n",
            "  Downloading redisgraph-2.4.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Collecting hiredis<3.0.0,>=2.0.0 (from redisgraph)\n",
            "  Downloading hiredis-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting prettytable<3.0.0,>=2.1.0 (from redisgraph)\n",
            "  Downloading prettytable-2.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting redis\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable<3.0.0,>=2.1.0->redisgraph) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading redisgraph-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hiredis-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.5/166.5 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prettytable-2.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: redis, prettytable, hiredis, redisgraph\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.12.0\n",
            "    Uninstalling prettytable-3.12.0:\n",
            "      Successfully uninstalled prettytable-3.12.0\n",
            "Successfully installed hiredis-2.4.0 prettytable-2.5.0 redis-3.5.3 redisgraph-2.4.4\n",
            "--2024-12-14 12:56:07--  http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
            "Resolving nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)... 185.125.190.82, 185.125.190.83, 91.189.91.82, ...\n",
            "Connecting to nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)|185.125.190.82|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318204 (1.3M) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M  2.21MB/s    in 0.6s    \n",
            "\n",
            "2024-12-14 12:56:07 (2.21 MB/s) - ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’ saved [1318204/1318204]\n",
            "\n",
            "Selecting previously unselected package libssl1.1:amd64.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "./\n",
            "./redis-stack-server-6.2.6-v7/\n",
            "./redis-stack-server-6.2.6-v7/bin/\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-benchmark\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-cli\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-sentinel\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-stack-server\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-check-rdb\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-check-aof\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-server\n",
            "./redis-stack-server-6.2.6-v7/share/\n",
            "./redis-stack-server-6.2.6-v7/share/RSAL_LICENSE\n",
            "./redis-stack-server-6.2.6-v7/share/APACHE_LICENSE\n",
            "./redis-stack-server-6.2.6-v7/lib/\n",
            "./redis-stack-server-6.2.6-v7/lib/redisgraph.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redistimeseries.so\n",
            "./redis-stack-server-6.2.6-v7/lib/rejson.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redisbloom.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redisearch.so\n",
            "./redis-stack-server-6.2.6-v7/etc/\n",
            "./redis-stack-server-6.2.6-v7/etc/README\n",
            "./redis-stack-server-6.2.6-v7/etc/redis-stack.conf\n",
            "./redis-stack-server-6.2.6-v7/etc/redis-stack-service.conf\n",
            "Starting redis-stack-server, database path ./redis-stack-server-6.2.6-v7/var/db/redis-stack\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow_text)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow_text)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.1.2)\n",
            "Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow_text-2.18.0\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (4.25.5)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.17.0)\n",
            "Collecting tensorflow<2.18,>=2.17 (from tf-keras>=2.14.1->tensorflow_hub)\n",
            "  Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.68.1)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.1.2)\n",
            "Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.0 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.17.1 tensorflow-2.17.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.1.2)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.10.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.33.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.26.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.33.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=d2f28e119792c30ce2ebec21b831104f0ec0b8216bba2747f4728dfb62aead60\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 uvicorn-0.33.0 uvloop-0.21.0 watchfiles-1.0.3\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
            "Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.3\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Para base de datos de grafos\n",
        "!pip install networkx matplotlib redis redisgraph\n",
        "!wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!curl -fsSL https://packages.redis.io/redis-stack/redis-stack-server-6.2.6-v7.focal.x86_64.tar.gz -o redis-stack-server.tar.gz\n",
        "!tar -xvf redis-stack-server.tar.gz\n",
        "!./redis-stack-server-6.2.6-v7/bin/redis-stack-server --daemonize yes\n",
        "\n",
        "# Para base de datos vectorial\n",
        "!pip install tensorflow_text\n",
        "!pip install tensorflow_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow_text\n",
        "!pip install chromadb\n",
        "!pip install langchain\n",
        "!pip install youtube-transcript-api\n",
        "!pip install PyPDF2\n",
        "\n",
        "# Para web-scrapping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# Para uso general\n",
        "!pip install gdown\n",
        "\n",
        "# Para clasificación\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga de librerías"
      ],
      "metadata": {
        "id": "W0-ywdcgs-8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para uso general\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Para base de datos de grafos\n",
        "import redis\n",
        "from redisgraph import Graph, Node, Edge\n",
        "import networkx as nx\n",
        "\n",
        "# Para base de datos vectorial\n",
        "import tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import chromadb\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import PyPDF2\n",
        "\n",
        "# Para web-scrapping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Para clasificación\n",
        "from huggingface_hub import InferenceClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Para re-ranking\n",
        "from sentence_transformers import CrossEncoder"
      ],
      "metadata": {
        "id": "c8RqDlfNtBUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de Bases de Datos"
      ],
      "metadata": {
        "id": "i6WFbNiVvj2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuentes de las bases de datos:\n",
        "*   [Board Game Geek](https://boardgamegeek.com/boardgame/220877/rajas-of-the-ganges)\n",
        "*   [Meesut Meeple](https://misutmeeple.com/2018/01/resena-rajas-of-the-ganges/)\n",
        "\n"
      ],
      "metadata": {
        "id": "YgMAJwFCxMce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentos de texto\n",
        "\n",
        "Contienen reglas del juego en sus diferentes modos y reseñas."
      ],
      "metadata": {
        "id": "NQ7AQaF6vnI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición de clases y funciones."
      ],
      "metadata": {
        "id": "Xdn3xFEs4IdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTranscripts(video_ids: list) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con transcripciones de videos de reseñas.\n",
        "\n",
        "  Parámetros:\n",
        "    - video_ids: Lista de identificadores de videos.\n",
        "\n",
        "  Retorno:\n",
        "    - transcript_list: Lista con transcripciones de los videos.\n",
        "  '''\n",
        "  # Incializar lista\n",
        "  transcript_list = []\n",
        "\n",
        "  # Extraer transcripciones\n",
        "  for video_id in video_ids:\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    transcript_list.append(transcript)\n",
        "\n",
        "  # Inicializar lista\n",
        "  final_transcripts = []\n",
        "\n",
        "  # Extraer cadenas de texto\n",
        "  for transcript in transcript_list:\n",
        "    text = \"\"\n",
        "    for line in transcript:\n",
        "      text += line['text'] + \" \"\n",
        "\n",
        "    # Limpieza de texto\n",
        "    text.replace('\\n', '')\n",
        "    text.replace('  ', ' ')\n",
        "\n",
        "    # Transcripciones finales\n",
        "    final_transcripts.append(text)\n",
        "\n",
        "  return final_transcripts\n",
        "\n",
        "\n",
        "def getPDFs(file_ids: list) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con documentos de texto.\n",
        "\n",
        "  Parámetros:\n",
        "    - file_ids: Lista de identificadores de archivos.\n",
        "\n",
        "  Retorno:\n",
        "    - file_transcripts: Lista con documentos de texto.\n",
        "  '''\n",
        "\n",
        "  pdfs_transcripts = []\n",
        "  i = 0\n",
        "\n",
        "  for file_id in file_ids:\n",
        "\n",
        "    # Creación la URL de descarga\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    # Descarga del archivo\n",
        "    output = f'file{i}.pdf'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "    # Abre el archivo en modo binario de lectura ('rb')\n",
        "    with open(f'file{i}.pdf', 'rb') as file:\n",
        "      # Crea un objeto PdfFileReader\n",
        "      reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "      # Inicializa una cadena vacía para almacenar el texto\n",
        "      text = ''\n",
        "\n",
        "      # Itera sobre todas las páginas del PDF\n",
        "      for i in range(len(reader.pages)):\n",
        "        # Obtiene la página\n",
        "        pagina = reader.pages[i]\n",
        "\n",
        "        # Extrae el texto de la página y lo añade a la cadena de texto\n",
        "        text += pagina.extract_text()\n",
        "\n",
        "      # Limpieza de texto\n",
        "      text.replace('\\n', '')\n",
        "      text.replace('  ', ' ')\n",
        "\n",
        "    pdfs_transcripts.append(text)\n",
        "    i += 1\n",
        "\n",
        "  return pdfs_transcripts\n",
        "\n",
        "\n",
        "def splitter(texts: list, metadatas: list, ids_names: str) -> tuple[list]:\n",
        "  '''\n",
        "  Crea splits de los textos recibidos, además de crear sus metadatas e IDs.\n",
        "\n",
        "  Parámetros:\n",
        "    - texts: lista con elementos a splittear.\n",
        "    - metadatas: lista con los nombres a usar como metadata.\n",
        "    - ids_names: lista con los nombres a usar en los IDs.\n",
        "\n",
        "  Retorna:\n",
        "    Listas con los splits de los textos, sus metadatas y sus IDs.\n",
        "  '''\n",
        "\n",
        "  # Creación de splitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "  # Definición de listas de salida\n",
        "  splitted_texts = []\n",
        "  splitted_metadatas = []\n",
        "  splitted_ids = []\n",
        "\n",
        "  # Creación de splits de texto y metadatas\n",
        "  for i in range(len(texts)):\n",
        "    splitted_text = text_splitter.split_text(texts[i])\n",
        "    splitted_texts += splitted_text\n",
        "    for j in range(len(splitted_text)):\n",
        "      splitted_metadatas.append(metadatas[i] + '_' + str(j))\n",
        "      # splitted_metadatas.append(metadatas[i])\n",
        "\n",
        "  # Creación de IDs\n",
        "  for k in range(len(splitted_texts)):\n",
        "    splitted_ids.append(ids_names + '_' + str(k))\n",
        "\n",
        "  return splitted_texts, splitted_metadatas, splitted_ids\n",
        "\n",
        "\n",
        "def createCollection(client: chromadb.api.client.Client) -> chromadb.api.models.Collection.Collection:\n",
        "  '''\n",
        "  Crea una colección de documentos con sus respectivos embeddings, metadata y IDs.\n",
        "\n",
        "  Parámetros:\n",
        "    - client: cliente de ChromaDB.\n",
        "\n",
        "  Retorno:\n",
        "    - collection: colección de documentos.\n",
        "  '''\n",
        "\n",
        "  # Carga de transcripciones de videos de reseñas\n",
        "  video_ids = [\"EpG7XML-Vm8\", \"mSRN0DShdFM\", \"yd5DEeEv59U\", \"18__Sw5Jq54\"]\n",
        "  video_transcripts = getTranscripts(video_ids)\n",
        "\n",
        "  # Carga de PDFs de manuales\n",
        "  normal_rules_file_id = '1xKZv2r58VlZ2JzFyYhKtBbQc0mVBWo-1'\n",
        "  solo_rules_file_id = '1NLZE1hS9TmU-O7lTLgt0kVv9AohZ64io'\n",
        "  solo_ai_rules_file_id = '1p2geJvRyIYCdZSGmP403qqHApyibbvnH'\n",
        "  automa_rules_file_id = '1oyYWB_O2mQcUc8AHHflySVMqAWpjvo48'\n",
        "  pdf_ids = [normal_rules_file_id, solo_rules_file_id, solo_ai_rules_file_id, automa_rules_file_id]\n",
        "  pdfs_transcripts = getPDFs(pdf_ids)\n",
        "\n",
        "  # Definición de fuentes (metadata)\n",
        "  video_metadatas = [\"review_1\", \"review_2\", \"review_3\", \"review_4\"]\n",
        "  # video_metadatas = [\"review\", \"review\", \"review\", \"review\"]\n",
        "  pdf_metadatas = [\"normal_rules\", \"solo_rules\", \"ai_rules\", \"automa_rules\"]\n",
        "\n",
        "  # Definición de IDs para collections\n",
        "  # video_coll_ids = [f\"vid{i}\" for i in range(1, len(video_transcripts)+1)]\n",
        "  # pdf_coll_ids = [f\"pdf{i}\" for i in range(1, len(pdfs_transcripts)+1)]\n",
        "\n",
        "  # Splits de transcripciones\n",
        "  video_transcripts, video_metadatas, video_coll_ids = splitter(video_transcripts, video_metadatas, \"vid\")\n",
        "  pdfs_transcripts, pdf_metadatas, pdf_coll_ids = splitter(pdfs_transcripts, pdf_metadatas, \"pdf\")\n",
        "\n",
        "  # Calcular embeddings para los documentos\n",
        "  collection = client.get_or_create_collection(\"all-my-documents\")\n",
        "\n",
        "  texts = video_transcripts + pdfs_transcripts\n",
        "  metadatas = video_metadatas + pdf_metadatas\n",
        "  texts_ids = video_coll_ids + pdf_coll_ids\n",
        "\n",
        "  embeddings = embed(texts).numpy().tolist()  # Convertir a lista para que sea serializable\n",
        "\n",
        "  collection.add(\n",
        "      documents=texts,\n",
        "      metadatas=[{\"source\": metadata} for metadata in metadatas],\n",
        "      ids=texts_ids,\n",
        "      embeddings=embeddings\n",
        "  )\n",
        "\n",
        "  return collection"
      ],
      "metadata": {
        "id": "xIxb7AKY0HUW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tablas\n",
        "\n",
        "Contienen atributos del juego."
      ],
      "metadata": {
        "id": "jekVJmQhv8xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de clases y funciones"
      ],
      "metadata": {
        "id": "pnwo7HfDhOtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDataFrame(file_id: str, encoding: str ='utf-8', delimiter: str = ',', show_head: bool = False) -> pd.DataFrame:\n",
        "  '''\n",
        "  Crea un DataFrame a partir de un archivo CSV alojado en Google Drive.\n",
        "\n",
        "  Parámetros:\n",
        "    - file_id: ID del archivo (codificado según Google Drive).\n",
        "    - encoding: Codificación de caracteres que se utilizará para leer el archivo.\n",
        "    - delimiter: Especifica el carácter que separa los valores en el archivo.\n",
        "    - show_head: Si es True, muestra las primeras filas del DataFrame.\n",
        "\n",
        "  Retorno:\n",
        "    - DataFrame: El DataFrame creado a partir del archivo.\n",
        "  '''\n",
        "\n",
        "  # Creación la URL de descarga\n",
        "  download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "  # Descarga del archivo\n",
        "  output = 'file'\n",
        "  gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "  df= pd.read_csv('file', encoding='utf-8', delimiter=',')\n",
        "\n",
        "  if show_head:\n",
        "    df.head()\n",
        "\n",
        "  return df\n",
        "\n",
        "def createTables() -> pd.DataFrame:\n",
        "  '''\n",
        "  Genera DataFrames con datos estadísticos y atributos del juego.\n",
        "\n",
        "  Retorno:\n",
        "    - Múltiples DataFrames con los datos de los juegos.\n",
        "  '''\n",
        "\n",
        "  # File IDs\n",
        "  # collection_stats_file_id = '153_EsmrYPTsnps_Y7t77FCbOUimN3YY6'\n",
        "  # game_attributes_file_id = '1f-_itA2DiGfUBYHrYKyW_Gakv1GN9PW5'\n",
        "  # game_ranks_file_id = '1EOJzdOAIQTrfeoL2Ryv0ZJe1IgQuH6GC'\n",
        "  # game_stats_file_id = '1ShOsowQX4BjsE2q7SdFcvbNXMoVvivKJ'\n",
        "  # parts_exchange_file_id = '1Qy6RV1a5Tv2wQIUCSPnDMJ2stqMRRBdN'\n",
        "  # play_stats_file_id = '1cjAHtDmMkGOL6OXv00GGMKSOAot6yKP1'\n",
        "  table_file_id = '1TBVbwYMja-DPXW8QrL8dwdwU586iEOO7'\n",
        "\n",
        "  # Descarga de DataFrames\n",
        "  # collection_stats = getDataFrame(collection_stats_file_id)\n",
        "  # game_attributes = getDataFrame(game_attributes_file_id)\n",
        "  # game_ranks = getDataFrame(game_ranks_file_id)\n",
        "  # game_stats = getDataFrame(game_stats_file_id)\n",
        "  # parts_exchange = getDataFrame(parts_exchange_file_id)\n",
        "  # play_stats = getDataFrame(play_stats_file_id)\n",
        "  table = getDataFrame(table_file_id)\n",
        "\n",
        "  return table # collection_stats, game_attributes, game_ranks, game_stats, parts_exchange, play_stats"
      ],
      "metadata": {
        "id": "2PUSIuPJv-9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grafos\n",
        "\n",
        "Contienen información de los creadores y artistas."
      ],
      "metadata": {
        "id": "ifQkA3t3v_V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de clases y funciones."
      ],
      "metadata": {
        "id": "B2U44GKROLxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RequestError(Exception):\n",
        "    '''Excepción personalizada para errores en las solicitudes.'''\n",
        "    pass\n",
        "\n",
        "def getValidation(url: str, retries: int = 3, backoff_factor: float = 0.5) -> str:\n",
        "  '''Valida la petición GET a la URL y en caso exitoso retorna la respuesta.\n",
        "     Implementa reintentos con backoff exponencial para manejar errores de timeout.'''\n",
        "  for i in range(retries):\n",
        "      try:\n",
        "          # Generación de la solicitud GET a la URL\n",
        "          response = requests.get(url)\n",
        "          # Verificación de si la respuesta tiene un código de error\n",
        "          response.raise_for_status()\n",
        "      except requests.exceptions.HTTPError as http_err:\n",
        "          if response.status_code == 504 and i < retries - 1:\n",
        "              time.sleep(backoff_factor * (2 ** i))  # Se espera antes de reintentar\n",
        "              print(f\"Reintentando {url} (intento {i + 1}/{retries})\")\n",
        "              continue # Se reintenta la solicitud\n",
        "          raise RequestError(f\"Error HTTP: {http_err}\")\n",
        "      except requests.exceptions.RequestException as err:\n",
        "          raise RequestError(f\"Error en la solicitud: {err}\")\n",
        "      else:\n",
        "          # Si no hay error se retorna la respuesta\n",
        "          return response\n",
        "\n",
        "def createGameList(url: str, credit_type: str) -> list:\n",
        "  '''\n",
        "  Crea una lista con nombres de juegos mediante web-scrapping.\n",
        "\n",
        "  Parámetros:\n",
        "    - url: Dirección URL hacia donde realizar el web-scrapping.\n",
        "    - credit_type: Cadena de texto que indica el tipo de crédido (creador o artista).\n",
        "\n",
        "  Retorno:\n",
        "    - game_list: Lista con nombres de juegos.\n",
        "  '''\n",
        "\n",
        "  # Petición GET a la URL.\n",
        "  response = getValidation(url)\n",
        "\n",
        "  # Extraigo el contenido\n",
        "  data = response.text\n",
        "\n",
        "  # Convertir el HTML a JSON (simulación del proceso que realmente realiza el frontend)\n",
        "  start_marker = 'GEEK.geekitemPreload = '\n",
        "  end_marker = 'GEEK.geekitemSettings'\n",
        "  start_idx = data.find(start_marker) + len(start_marker)\n",
        "  end_idx = data.find(end_marker, start_idx)\n",
        "  json_data = json.loads(data[start_idx:end_idx-3])\n",
        "\n",
        "  # Extraer nombres de los juegos de mesa\n",
        "  game_list = [item['name'] for item in json_data['item']['links'][credit_type]]\n",
        "\n",
        "  return game_list\n",
        "\n",
        "def createCreditData(game_names_inka_brand: list, game_names_markus_brand: list, game_names_dennis_lohaussen: list) -> pd.DataFrame:\n",
        "  '''\n",
        "  Genera DataFrames con datos de créditos.\n",
        "\n",
        "  Parámetros:\n",
        "    - game_names_inka_brand: Lista con nombres de juegos de mesa de Inka Brand.\n",
        "    - game_names_markus_brand: Lista con nombres de juegos de mesa de Markus Brand.\n",
        "    - game_names_dennis_lohaussen: Lista con nombres de juegos de mesa de Dennis Lohausen.\n",
        "\n",
        "  Retorno:\n",
        "    - Múltiples DataFrames con los datos de los créditos del juego.\n",
        "  '''\n",
        "  # Datos de créditos\n",
        "  designers = pd.DataFrame({\n",
        "      'name': ['Inka Brand', 'Markus Brand'],\n",
        "      'sex': ['Female', 'Male']\n",
        "  })\n",
        "\n",
        "  artists = pd.DataFrame({\n",
        "      'name': ['Dennis Lohausen'],\n",
        "      'sex': ['Male']\n",
        "  })\n",
        "\n",
        "  primary_names = pd.DataFrame({\n",
        "      'name': ['Rajas of the Ganges'],\n",
        "      'year': [2017]\n",
        "  })\n",
        "\n",
        "  alternative_names = pd.DataFrame({\n",
        "      'name': ['I Ragià del Gange', 'Rijs van de Gang', 'Rajas de la Gange', 'Rajas der Gange', 'Rajas del Ganges', 'Раджи Ганга', 'ガンジスの霊王', '갠지스의 라자', 'Rajas do Gange'],\n",
        "      'language': ['Italian', 'Dutch', 'French', 'German', 'Spanish', 'Russian', 'Japanese', 'Korean', 'Portuguese']\n",
        "  })\n",
        "\n",
        "  categories = pd.DataFrame({\n",
        "      'name': ['Dice', 'Economic', 'Renaissance', 'Territory Building']\n",
        "  })\n",
        "\n",
        "  mechanisms = pd.DataFrame({\n",
        "      'name': ['Connections', 'Dice Rolling', 'Race', 'Tile Placement', 'Track Movement', 'Worker Placement', 'Worker Placement with Dice Workers']\n",
        "  })\n",
        "\n",
        "  publishers = pd.DataFrame({\n",
        "      'name': ['HUCH!', '999 Games', 'Devir', 'Dice Realm', 'DV Games', 'Egmont Polska', 'Fabrika Igr', 'Game Harbor', 'HOT Games', 'nostalgia III', 'R&R Games']\n",
        "  })\n",
        "\n",
        "  inka_brand_designed_games = pd.DataFrame({\n",
        "      'name': game_names_inka_brand\n",
        "  })\n",
        "\n",
        "  markus_brand_designed_games = pd.DataFrame({\n",
        "      'name': game_names_markus_brand\n",
        "  })\n",
        "\n",
        "  dennis_lohaussen_artworked_games = pd.DataFrame({\n",
        "      'name': game_names_dennis_lohaussen\n",
        "  })\n",
        "\n",
        "  expansions = pd.DataFrame({\n",
        "      'name': ['Rajas of the Ganges: Goodie Box 2', 'Rajas of the Ganges: Goodie Box 1', 'Rajas of the Ganges: Blessings of Kedarnath', 'Rajas of the Ganges: Tiger Expansion',\n",
        "               'Rajas of the Ganges: Snake Expansion', 'Deutscher Spielepreis 2018 Goodie Box', 'Brettspiel Adventskalender 2018', 'Rajas of the Ganges: Mango Village', 'Brettspiel Adventskalender 2017'],\n",
        "      'year': ['2020', '2019', '2019', '2018', '2018', '2018', '2018', '2017', '2017']\n",
        "  })\n",
        "\n",
        "  return designers, artists, primary_names, alternative_names, categories, mechanisms, publishers, inka_brand_designed_games, markus_brand_designed_games, dennis_lohaussen_artworked_games, expansions\n",
        "\n",
        "def createGraph(graph: Graph):\n",
        "  '''\n",
        "  Crea un grafo de RedisGraph.\n",
        "\n",
        "  Parámetros:\n",
        "    - graph: Objeto Graph de RedisGraph.\n",
        "  '''\n",
        "  # URLs de la páginas de donde quiero extraer las infos\n",
        "  url_inka = 'https://boardgamegeek.com/boardgamedesigner/6940/inka-brand/linkeditems/boardgamedesigner'\n",
        "  url_markus = 'https://boardgamegeek.com/boardgamedesigner/6941/markus-brand/linkeditems/boardgamedesigner'\n",
        "  url_dennis = 'https://boardgamegeek.com/boardgameartist/12484/dennis-lohausen/linkeditems/boardgameartist'\n",
        "\n",
        "  # Extraer nombres de los juegos de mesa\n",
        "  game_names_inka_brand = createGameList(url_inka, 'boardgamedesigner')\n",
        "  game_names_markus_brand = createGameList(url_markus, 'boardgamedesigner')\n",
        "  game_names_dennis_lohaussen = createGameList(url_dennis, 'boardgameartist')\n",
        "\n",
        "  # Crear DataFrames de créditos\n",
        "  designers, artists, primary_names, alternative_names, categories, mechanisms, publishers, inka_brand_designed_games, markus_brand_designed_games, dennis_lohaussen_artworked_games, expansions = createCreditData(game_names_inka_brand, game_names_markus_brand, game_names_dennis_lohaussen)\n",
        "\n",
        "  # Crear nodos\n",
        "  designer_node_list = []\n",
        "  for index, row in designers.iterrows():\n",
        "      designer_node_list.append(Node(label='Designer', properties={'name': row['name'], 'sex': row['sex']}))\n",
        "\n",
        "  artist_node_list = []\n",
        "  for index, row in artists.iterrows():\n",
        "      artist_node_list.append(Node(label='Artist', properties={'name': row['name'], 'sex': row['sex']}))\n",
        "\n",
        "  primary_name_node_list = []\n",
        "  for index, row in primary_names.iterrows():\n",
        "      primary_name_node_list.append(Node(label='Primary_Name', properties={'name': row['name'], 'year': row['year']}))\n",
        "\n",
        "  alternative_name_node_list = []\n",
        "  for index, row in alternative_names.iterrows():\n",
        "      alternative_name_node_list.append(Node(label='Alternative_Name', properties={'name': row['name'], 'language': row['language']}))\n",
        "\n",
        "  category_node_list = []\n",
        "  for index, row in categories.iterrows():\n",
        "      category_node_list.append(Node(label='Category', properties={'name': row['name']}))\n",
        "\n",
        "  mechanism_node_list = []\n",
        "  for index, row in mechanisms.iterrows():\n",
        "      mechanism_node_list.append(Node(label='Mechanism', properties={'name': row['name']}))\n",
        "\n",
        "  publisher_node_list = []\n",
        "  for index, row in publishers.iterrows():\n",
        "      publisher_node_list.append(Node(label='Publisher', properties={'name': row['name']}))\n",
        "\n",
        "  inka_brand_designed_games_list = []\n",
        "  for index, row in inka_brand_designed_games.iterrows():\n",
        "      inka_brand_designed_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  markus_brand_designed_games_list = []\n",
        "  for index, row in markus_brand_designed_games.iterrows():\n",
        "      markus_brand_designed_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  dennis_lohaussen_artworked_games_list = []\n",
        "  for index, row in dennis_lohaussen_artworked_games.iterrows():\n",
        "      dennis_lohaussen_artworked_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  expansions_node_list = []\n",
        "  for index, row in expansions.iterrows():\n",
        "      expansions_node_list.append(Node(label='Expansion', properties={'name': row['name'], 'year': row['year']}))\n",
        "\n",
        "  # Agregar los nodos al gráfico\n",
        "  for node in designer_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in artist_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in primary_name_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in alternative_name_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in category_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in mechanism_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in publisher_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in inka_brand_designed_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in markus_brand_designed_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in dennis_lohaussen_artworked_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in expansions_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  # Crear más relaciones (aristas)\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for designer in designer_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_DESIGNER', designer))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for artist in artist_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_ARTIST', artist))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for alternative_name in alternative_name_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_ALTERNATIVE_NAME', alternative_name))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for category in category_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_CATEGORY', category))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for mechanism in mechanism_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_MECHANISM', mechanism))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for publisher in publisher_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_PUBLISHER', publisher))\n",
        "\n",
        "  for game in inka_brand_designed_games_list:\n",
        "          graph.add_edge(Edge(designer_node_list[0], 'DESIGNED', game))\n",
        "\n",
        "  for game in markus_brand_designed_games_list:\n",
        "          graph.add_edge(Edge(designer_node_list[1], 'DESIGNED', game))\n",
        "\n",
        "  for game in dennis_lohaussen_artworked_games_list:\n",
        "          graph.add_edge(Edge(artist_node_list[0], 'ARTWORKED', game))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for expansion in expansions_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_EXPANSION', expansion))"
      ],
      "metadata": {
        "id": "eILPIJNYJZi4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebas sobre la base de datos de Grafos"
      ],
      "metadata": {
        "id": "-_h8KbWCZVP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ejecutar una consulta para comprobar los nodos\n",
        "# query = \"\"\"MATCH (p:Designer) RETURN p.name, p.sex\"\"\"\n",
        "# result = graph.query(query)\n",
        "\n",
        "# # Imprimir los resultados de la consulta\n",
        "# for record in result.result_set:\n",
        "#     print(f\"Person: {record[0]}, Sex: {record[1]}\")"
      ],
      "metadata": {
        "id": "FP7ZY9WXCKB0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de la base de datos de Grafos"
      ],
      "metadata": {
        "id": "_RMC6CWeZZ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Realizar la consulta para obtener nodos y relaciones\n",
        "# query = \"\"\"MATCH (a)-[r]->(b) RETURN a.name, b.name, type(r)\"\"\"\n",
        "# result = graph.query(query)\n",
        "\n",
        "# # Crear un grafo dirigido con networkx\n",
        "# G = nx.DiGraph()\n",
        "\n",
        "# # Agregar los nodos y relaciones a networkx\n",
        "# for record in result.result_set:\n",
        "#     record_1 = record[0]\n",
        "#     record_2 = record[1]\n",
        "#     relation = record[2]\n",
        "\n",
        "#     G.add_node(record_1)\n",
        "#     G.add_node(record_2)\n",
        "#     G.add_edge(record_1, record_2, label=relation)\n",
        "\n",
        "# # Ajustar layout para una mayor separación entre los nodos\n",
        "# pos = nx.spring_layout(G, k=1.5, iterations=50)  # 'k' controla la distancia entre nodos\n",
        "\n",
        "# plt.figure(figsize=(10, 8))  # Aumentar el tamaño de la figura\n",
        "\n",
        "# # Dibujar nodos y etiquetas\n",
        "# nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=12, font_weight=\"bold\", arrows=True)\n",
        "\n",
        "# # Dibujar las etiquetas de las relaciones (aristas) con un desplazamiento\n",
        "# edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', label_pos=0.7)  # Ajustar label_pos para desplazar etiquetas\n",
        "\n",
        "# # Mejorar la disposición de los nodos para que no se superpongan\n",
        "# for label in pos:\n",
        "#     pos[label][1] += 0.05  # Elevar un poco las etiquetas de los nodos\n",
        "\n",
        "# plt.title(\"Visualización del grafo de RedisGraph\", fontsize=14)\n",
        "# plt.show()\n",
        "\n",
        "# # Imprimir todas las relaciones encontradas en formato tríada (Node)-[Relation]->(Node)\n",
        "# for edge in G.edges(data=True):\n",
        "#     print(f\"({edge[0]})-[{edge[2]['label']}]->({edge[1]})\")"
      ],
      "metadata": {
        "id": "8WH-pR7HEU8e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador"
      ],
      "metadata": {
        "id": "fZmkhuNOo8uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basada en modelo entrenado con ejemplos y embeddings"
      ],
      "metadata": {
        "id": "Am4bHggtpmMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randomForestClassifierCreator(production: bool = False) -> RandomForestClassifier:\n",
        "  '''\n",
        "  Genera un clasificador de textos basado en RandomForest.\n",
        "  Clasifica en cuatro categorías: reviews, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - production: si es False, se calculan y muestran las métricas de prueba.\n",
        "\n",
        "  Retorno:\n",
        "    - Modelo de RandomForest de clasificación.\n",
        "  '''\n",
        "  # Carga del modelo desde HuggingFace\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "  # Carga del dataset\n",
        "  file_id = '1j2MCy0CACVHFVa3YfirqssUNcbQmq2Gn'\n",
        "  df = getDataFrame(file_id)\n",
        "\n",
        "  # Generación de estructura con el dataset recibido\n",
        "  dataset = []\n",
        "  for row in df.itertuples():\n",
        "      dataset.append((row.prompt, row.classification))\n",
        "\n",
        "  # Preparación de X e y\n",
        "  X = [prompt.lower() for prompt, classification in dataset]\n",
        "  y = [classification for prompt, classification in dataset]\n",
        "\n",
        "  # División del dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#, stratify=y)\n",
        "\n",
        "  # Obtención los embeddings de BERT para los conjuntos de entrenamiento\n",
        "  X_train_vectorized = model.encode(X_train)\n",
        "\n",
        "  ## Creación y entrenamiento del modelo de RandomForest\n",
        "\n",
        "  # Definición de los parámetros\n",
        "  param_grid = {\n",
        "      'n_estimators': [200, 500, 700, 900],   #[50, 100, 150, 200, 500, 700],\n",
        "      'max_depth': [3, 5, 10, 15, 20]         #[None, 1, 2, 3, 4, 5, 10, 15]\n",
        "  }\n",
        "\n",
        "  # Creación el objeto GridSearchCV\n",
        "  grid_search_cv = GridSearchCV(\n",
        "      estimator=RandomForestClassifier(random_state=42),  # Modelo Random Forest\n",
        "      param_grid=param_grid,\n",
        "      scoring='accuracy',  # Métrica para evaluar\n",
        "      n_jobs=-1,           # Usar todos los núcleos disponibles\n",
        "      cv=5,                # 5 particiones para validación cruzada\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  # Ajuste del modelo con los datos de entrenamiento\n",
        "  grid_search_cv.fit(X_train_vectorized, y_train)\n",
        "\n",
        "  # Obtención del mejor estimador\n",
        "  modelo = grid_search_cv.best_estimator_\n",
        "  best_params = grid_search_cv.best_params_\n",
        "\n",
        "  # Entrenamiento del modelo\n",
        "  modelo.fit(X_train_vectorized, y_train)\n",
        "\n",
        "  if not production:\n",
        "    # Obtención de los embeddings de BERT para los conjuntos de prueba\n",
        "    X_test_vectorized = model.encode(X_test)\n",
        "\n",
        "    # Evaluación del modelo\n",
        "    y_pred_train = modelo.predict(X_train_vectorized)\n",
        "    y_pred_test = modelo.predict(X_test_vectorized)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_test)\n",
        "    report = classification_report(y_test, y_pred_test, zero_division=1)\n",
        "\n",
        "    print(\"\\nExactitud Entrenamiento:\", train_acc)\n",
        "    print(\"Exactitud Prueba:\", test_acc)\n",
        "    print(\"\\nMejores parámetros:\", best_params)\n",
        "    print(\"\\nMatriz de confusión:\", matrix)\n",
        "    print(\"\\nReporte de clasificación:\", report)\n",
        "\n",
        "  return modelo\n",
        "\n",
        "\n",
        "def rfClassifier(modelo: RandomForestClassifier, prompt: str) -> str:\n",
        "  '''\n",
        "  Clasificador de textos basado en RandomForest. Recibe un prompt por parte del usuario\n",
        "  y lo clasifica en cuatro categorías: reviews, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - modelo: Modelo de clasificación.\n",
        "    - prompt: Texto a clasificar.\n",
        "\n",
        "  Retorno:\n",
        "    - Clasificación del texto.\n",
        "  '''\n",
        "  # Carga del modelo desde HuggingFace\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "  # Adaptación del texto\n",
        "  prompt = [prompt.lower()]\n",
        "\n",
        "  # Preprocesamiento y vectorización de las nuevas frases\n",
        "  prompt_vectorized = model.encode(prompt)\n",
        "\n",
        "  # Predicción con el modelo entrenado\n",
        "  classification = modelo.predict(prompt_vectorized)\n",
        "\n",
        "  # Impresión del prompt y su etiquetado\n",
        "  # print(f\"\\nPrompt: '{prompt}'\")\n",
        "  # print(f\"Clasificación predicha: {classification}\\n\")\n",
        "\n",
        "  return classification"
      ],
      "metadata": {
        "id": "6P9acraLpAXU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basada en LLM"
      ],
      "metadata": {
        "id": "hkzICjMlpu6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llmClassifier(query_str: str, client: InferenceClient) -> str:\n",
        "  '''\n",
        "  Clasificador de textos basado en LLM. Recibe un prompt por parte del usuario\n",
        "  y lo clasifica en cuatro categorías: reviews, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - query_str: Cadena de texto que representa la consulta del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Clasificación del texto.\n",
        "  '''\n",
        "  # Armado de la query\n",
        "  prompt = (\n",
        "        \"Classify text strictly into just one of the following words: reviews, rules, stats and credits.\\n\"\n",
        "        \"Do not use any other words on your answer.\\n\"\n",
        "        \"Categories and mechanisms should be classified as credits.\\n\"\n",
        "        \"Amount of players, playing time and ages should be classified as stats.\\n\"\n",
        "        f\"Question: {query_str}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "mFfU8I0PpxOf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever"
      ],
      "metadata": {
        "id": "lFJrYJ2KTnTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retriever(query: str, classification: str, graph: Graph, table_df: pd.DataFrame, collection: chromadb.api.models.Collection.Collection, client: InferenceClient, embed) -> list[str]:\n",
        "  '''\n",
        "  Función que decide a qué base de datos extraer en base a la clasificación, obteniendo los textos contextuales.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - classification: Clasificación del texto, puede ser reviews, rules, credits o stats.\n",
        "    - graph: Base de datos de grafos.\n",
        "    - table_df: Base de datos tabular.\n",
        "    - collection: Base de datos vectorial.\n",
        "\n",
        "  Retorno:\n",
        "    - Textos contextuales.\n",
        "  '''\n",
        "  # Caso 1: Base de datos vectorial\n",
        "  if classification == \"reviews\" or classification == \"rules\":\n",
        "    results = vectorialDbRetriever(query, classification, collection, embed, client)\n",
        "\n",
        "    # Re-ranking\n",
        "    results = reranker(query, results[\"documents\"][0], 3)\n",
        "\n",
        "  # Caso 2: Base de datos tabular\n",
        "  elif classification == \"stats\":\n",
        "    results = tableDbRetriever(query, table_df, client)\n",
        "\n",
        "  # Caso 3: Base de datos de grafos\n",
        "  elif classification == \"credits\":\n",
        "    results = graphDbRetriever(query, graph, client)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def vectorialDbRetriever(query: str, classification: str, collection: chromadb.api.models.Collection.Collection, embed, client: InferenceClient) -> list[str]:\n",
        "  '''\n",
        "  Retriever de texto basado en vectores. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - embed: función de embedding.\n",
        "    - client: cliente de HuggingFace.\n",
        "\n",
        "  Retorno:\n",
        "    - Lista de textos contextuales.\n",
        "  '''\n",
        "  # Inicialización de lista de retorno\n",
        "  retriever_list = []\n",
        "\n",
        "  # Generación de embedding de la query\n",
        "  embedding_query = embed([query]).numpy().tolist()\n",
        "\n",
        "  # Refinado de metadata de reglas\n",
        "  if classification == \"rules\":\n",
        "    # Armado de la query\n",
        "    prompt = (\n",
        "          f\"Classify text strictly into just one of the following rules category: normal, solo, ai and automa.\\n\"\n",
        "          \"If none of the classifications seems correct enough, classify the text as normal.\"\n",
        "          \"Do not use any other words on your answer.\\n\"\n",
        "          f\"Question: {query}\\n\"\n",
        "          \"Answer: \"\n",
        "      )\n",
        "\n",
        "    # Armado de los mensajes\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Inferencia\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "      messages=messages,\n",
        "      max_tokens=1000\n",
        "    )\n",
        "\n",
        "    # Columna elegida\n",
        "    classification = completion.choices[0].message.content\n",
        "\n",
        "  # Filtrado de metadatas\n",
        "  filtered_metadatas = [\n",
        "      metadata\n",
        "      for metadata in collection.get(include=[\"metadatas\"])[\"metadatas\"]\n",
        "      if classification in metadata.get(\"source\", \"\")\n",
        "  ]\n",
        "\n",
        "  # Búsqueda de resultados\n",
        "  results = collection.query(\n",
        "      query_embeddings = embedding_query,  # Aquí pasamos el embedding de la consulta\n",
        "      n_results = 10,  # Traemos los 10 resultados más cercanos\n",
        "      where = {\"source\": {\"$in\": filtered_metadatas}}  # Filtrar por metadatos\n",
        "  )\n",
        "\n",
        "  # Adjuntado de resultados\n",
        "  for doc in results[\"documents\"]:\n",
        "    retriever_list.append(doc)\n",
        "\n",
        "  return retriever_list\n",
        "\n",
        "\n",
        "def tableDbRetriever(query: str, table_df: pd.DataFrame, client: InferenceClient) -> str:\n",
        "  '''\n",
        "  Retriever de texto basado en tablas. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - table_df: base de datos tabular.\n",
        "    - client: cliente de HuggingFace.\n",
        "\n",
        "  Retorno:\n",
        "    - DataFrame con la información recuperada.\n",
        "  '''\n",
        "  # Columnas del DataFrame\n",
        "  columns = table_df.columns.tolist()\n",
        "\n",
        "  # Armado de la query\n",
        "  prompt = (\n",
        "        f\"Classify text strictly into just one of the following words: {columns}.\\n\"\n",
        "        \"Do not use any other words on your answer.\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        "  )\n",
        "\n",
        "  # Columna elegida\n",
        "  selected_column = completion.choices[0].message.content\n",
        "\n",
        "  # Selección de resultados\n",
        "  results = selected_column + \": \" + str(table_df.selected_column.iloc[0])\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def graphDbRetriever(query: str, graph: Graph, client: InferenceClient) -> str:\n",
        "  '''\n",
        "  Retriever de texto basado en grafos. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - graph: base de datos de grafos.\n",
        "    - client: cliente de HuggingFace.\n",
        "\n",
        "  Retorno:\n",
        "    - Lista de textos contextuales.\n",
        "  '''\n",
        "  # Lista de relaciones\n",
        "  relations = [\n",
        "      (\"Inka Brand\", \"DESIGNED\", [\"Sausage Sizzle!\", \"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\",\n",
        "                                  \"Andor: The Family Fantasy Game\", \"Andor: The Family Fantasy Game – The Danger in the Shadows\",\n",
        "                                  \"Bibi & Tina: Das Spiel zum Film\", \"Bitte nicht öffnen: Bissig!\"]),\n",
        "      (\"Markus Brand\", \"DESIGNED\", [\"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\", \"Andor: The Family Fantasy Game\",\n",
        "                                    \"Andor: The Family Fantasy Game – The Danger in the Shadows\", \"Bibi & Tina: Das Spiel zum Film\",\n",
        "                                    \"Bitte nicht öffnen: Bissig!\", \"Black Stories: Das Spiel\"]),\n",
        "      (\"Dennis Lohausen\", \"ARTWORKED\", [\"Coal Baron: The Great Card Game\", \"100!\", \"1001 Karawane\",\n",
        "                                        \"1001 Karawane: Sonderchips\", \"112: Brandgefährlich\", \"15\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_DESIGNER\", [\"Inka Brand\", \"Markus Brand\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_ARTIST\", [\"Dennis Lohausen\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_ALTERNATIVE_NAME\", [\"I Ragià del Gange\", \"Rijs van de Gang\", \"Rajas de la Gange\",\n",
        "                                                       \"Rajas der Gange\", \"Rajas del Ganges\", \"Раджи Ганга\", \"ガンジスの霊王\",\n",
        "                                                       \"갠지스의 라자\", \"Rajas do Gange\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_CATEGORY\", [\"Dice\", \"Economic\", \"Renaissance\", \"Territory Building\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_MECHANISM\", [\"Connections\", \"Dice Rolling\", \"Race\", \"Tile Placement\", \"Track Movement\",\n",
        "                                                \"Worker Placement\", \"Worker Placement with Dice Workers\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_PUBLISHER\", [\"HUCH!\", \"999 Games\", \"Devir\", \"Dice Realm\", \"DV Games\", \"Egmont Polska\",\n",
        "                                                \"Fabrika Igr\", \"Game Harbor\", \"HOT Games\", \"nostalgia III\", \"R&R Games\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_EXPANSION\", [\"Rajas of the Ganges: Goodie Box 2\", \"Rajas of the Ganges: Goodie Box 1\",\n",
        "                                                \"Rajas of the Ganges: Blessings of Kedarnath\", \"Rajas of the Ganges: Tiger Expansion\",\n",
        "                                                \"Rajas of the Ganges: Snake Expansion\", \"Deutscher Spielepreis 2018 Goodie Box\",\n",
        "                                                \"Brettspiel Adventskalender 2018\", \"Rajas of the Ganges: Mango Village\",\n",
        "                                                \"Brettspiel Adventskalender 2017\"])\n",
        "  ]\n",
        "\n",
        "  # Lista de entidades\n",
        "  entities = {\n",
        "      \"Game\": [\"Sausage Sizzle!\", \"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\",\n",
        "                \"Andor: The Family Fantasy Game\", \"Andor: The Family Fantasy Game – The Danger in the Shadows\",\n",
        "                \"Bibi & Tina: Das Spiel zum Film\", \"Bitte nicht öffnen: Bissig!\", \"Black Stories: Das Spiel\",\n",
        "                \"Coal Baron: The Great Card Game\", \"100!\", \"1001 Karawane\", \"1001 Karawane: Sonderchips\",\n",
        "                \"112: Brandgefährlich\", \"15\"],\n",
        "      \"Primary_Name\": [\"Rajas of the Ganges\"],\n",
        "      \"Designer\": [\"Inka Brand\", \"Markus Brand\"],\n",
        "      \"Artist\": [\"Dennis Lohausen\"],\n",
        "      \"Alternative_Name\": [\"I Ragià del Gange\", \"Rijs van de Gang\", \"Rajas de la Gange\", \"Rajas der Gange\",\n",
        "                           \"Rajas del Ganges\", \"Раджи Ганга\", \"ガンジスの霊王\", \"갠지스의 라자\", \"Rajas do Gange\"],\n",
        "      \"Category\": [\"Dice\", \"Economic\", \"Renaissance\", \"Territory Building\"],\n",
        "      \"Mechanism\": [\"Connections\", \"Dice Rolling\", \"Race\", \"Tile Placement\", \"Track Movement\",\n",
        "                    \"Worker Placement\", \"Worker Placement with Dice Workers\"],\n",
        "      \"Publisher\": [\"HUCH!\", \"999 Games\", \"Devir\", \"Dice Realm\", \"DV Games\", \"Egmont Polska\",\n",
        "                    \"Fabrika Igr\", \"Game Harbor\", \"HOT Games\", \"nostalgia III\", \"R&R Games\"],\n",
        "      \"Expansion\": [\"Rajas of the Ganges: Goodie Box 2\", \"Rajas of the Ganges: Goodie Box 1\",\n",
        "                    \"Rajas of the Ganges: Blessings of Kedarnath\", \"Rajas of the Ganges: Tiger Expansion\",\n",
        "                    \"Rajas of the Ganges: Snake Expansion\", \"Deutscher Spielepreis 2018 Goodie Box\",\n",
        "                    \"Brettspiel Adventskalender 2018\", \"Rajas of the Ganges: Mango Village\", \"Brettspiel Adventskalender 2017\"]\n",
        "  }\n",
        "\n",
        "  # Armado de la query\n",
        "  prompt = (\"Write down a Cypher query based on a question from a user.\\n\"\n",
        "        \"It must strictly be just one query, it is important that you respect all of Cypher's syntax rules and use the entities and relationships provided.\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\\n\"\n",
        "          \"You are an expert in board games and understand their descriptive characteristics.\\n\"\n",
        "          \"As an expert on the game Rajas of the Ganges, you are also an expert on Cypher queries based on a question made by a user.\\n\"\n",
        "          \"The database is composed by entities and relations. The entities from the database can be found in the following Python dictionary:\\n\"\n",
        "          f\"{entities}, where the keys are the entities and the labels are the same ones they have on the database.\\n\"\n",
        "          \"The relations from the database can be found in the following Python list:\\n\"\n",
        "          f\"{relations}.\\n\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=50000\n",
        "  )\n",
        "\n",
        "  # Query para Cypher\n",
        "  cypher_query = completion.choices[0].message.content\n",
        "\n",
        "  # Resultado de la búsqueda\n",
        "  result = graph.query(cypher_query)\n",
        "\n",
        "  # Guardar los resultados de la consulta\n",
        "  results = \"\"\n",
        "  first = True\n",
        "  for record in result.result_set:\n",
        "    if first:\n",
        "      results += f\"{record[0]}\"\n",
        "      first = False\n",
        "    else:\n",
        "      results += f\", {record[0]}\"\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "AJZC8_EHTqQk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Ranker"
      ],
      "metadata": {
        "id": "ahBg486gAfbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reranker(query: str, documents: list, top_k: int = 3):\n",
        "  '''\n",
        "  Realiza un re-ranking sobre los documentos obtenidos tras una búsqueda en\n",
        "  la base de datos vectorial.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - documents: resultados obtenidos tras la búsqueda en base de datos vectorial.\n",
        "    - top_k: número de documentos a retornar.\n",
        "\n",
        "  Retorna:\n",
        "    - Lista de documentos ordenados por relevancia.\n",
        "  '''\n",
        "  # Carga del modelo para re-ranking\n",
        "  model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "  # Emparejamiento de prompt del usuario con documentos para hacer predicciones\n",
        "  pairs = [(query, document) for document in documents]\n",
        "  scores = model.predict(pairs)\n",
        "\n",
        "  # Emparejamiento de puntajes de predicción con documentos y ordenado\n",
        "  scored_docs = list(zip(scores, documents))\n",
        "  scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  # Toma de los documentos mejor puntuados\n",
        "  scored_docs = scored_docs[:top_k]\n",
        "\n",
        "  # Armado de string contextual final\n",
        "  complete_context = \"\"\n",
        "  for score, doc in scored_docs:\n",
        "    complete_context += doc + \";\"\n",
        "\n",
        "  return complete_context"
      ],
      "metadata": {
        "id": "p9k7ZMT-Akk7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main"
      ],
      "metadata": {
        "id": "jqqNcNrhZoge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de bases de datos de Grafos"
      ],
      "metadata": {
        "id": "3tf185wdSRF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS DE GRAFOS           ##\n",
        "#########################################################\n",
        "try:\n",
        "  graph\n",
        "  print(\"✔ Base de datos de grafos cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos de grafos.\")\n",
        "\n",
        "  # Conexión a Redis\n",
        "  redis_client = redis.Redis(host='localhost', port=6379)\n",
        "\n",
        "  # Crear una gráfica en RedisGraph\n",
        "  graph = Graph('credits', redis_client)\n",
        "\n",
        "  # Agregar nodos y aristas\n",
        "  createGraph(graph)\n",
        "\n",
        "  # Guardar los datos en RedisGraph\n",
        "  graph.commit()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##           CREACIÓN BASE DE DATOS TABULAR            ##\n",
        "#########################################################\n",
        "# Generación de tablas\n",
        "try:\n",
        "  table_df\n",
        "  print(\"✔ Base de datos tabular cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos tabular.\")\n",
        "  # collection_stats_df, game_attributes_df, game_ranks_df, game_stats_df, parts_exchange_df, play_stats_df = createTables()\n",
        "  table_df = createTables()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS VECTORIAL           ##\n",
        "#########################################################\n",
        "# Generación de collection\n",
        "try:\n",
        "  collection\n",
        "  print(\"✔ Base de datos vectorial cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos vectorial.\")\n",
        "  # Cargar Universal Sentence Encoder\n",
        "  embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "\n",
        "  # Configuración inicial de ChromaDB\n",
        "  client = chromadb.Client()\n",
        "\n",
        "  # Generación de collection\n",
        "  collection = createCollection(client) ################## Aquí debe ir lo que descarga collection del Drive!!!!\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##              CLASIFICACIÓN DE PROMPT                ##\n",
        "#########################################################\n",
        "\n",
        "llm_class_client = InferenceClient(api_key=\"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\")\n",
        "\n",
        "query_str = \"How many squares should the pawn move?\"\n",
        "\n",
        "print(llmClassifier(query_str, llm_class_client))\n",
        "\n",
        "# # Clasificación con modelo de Random Forest\n",
        "# # # Creación del modelo\n",
        "# # modelo_rf = randomForestClassifierCreator()\n",
        "# # # Descarga del modelo\n",
        "# # joblib.dump(modelo_rf, 'modelo_rf.pkl')\n",
        "# # File ID del modelo\n",
        "# model_file_id = '1t_P6sziWieZcUvNvEujveRfguCVH8LNE'\n",
        "# # Creación la URL de descarga\n",
        "# download_url = f'https://drive.google.com/uc?id={model_file_id}'\n",
        "# # Descarga del archivo\n",
        "# output = 'modelo_rf.pkl'\n",
        "# gdown.download(download_url, output, quiet=True)\n",
        "# # Crga del modelo\n",
        "# modelo_rf = joblib.load('modelo_rf.pkl')\n",
        "# print(rfClassifier(modelo_rf, query_str))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##                     RETRIEVER                       ##\n",
        "#########################################################\n"
      ],
      "metadata": {
        "id": "GolyC2bUEtDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6c2881-32f6-405b-ff2c-03564fb27592"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠ Cargando base de datos de grafos.\n",
            "⚠ Cargando base de datos tabular.\n",
            "⚠ Cargando base de datos vectorial.\n",
            "rules\n"
          ]
        }
      ]
    }
  ]
}