{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t5u1osKts6Ef",
        "i6WFbNiVvj2_",
        "NQ7AQaF6vnI1",
        "jekVJmQhv8xd",
        "ifQkA3t3v_V-",
        "fZmkhuNOo8uf",
        "Am4bHggtpmMD",
        "hkzICjMlpu6g",
        "lFJrYJ2KTnTO",
        "ahBg486gAfbQ",
        "VKOdtw9S94_G",
        "hnWHcDAyosKf",
        "jqqNcNrhZoge",
        "e_gAyUImh1_5",
        "BJKQ1Mnfh3tX"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del Lenguaje Natural - Trabajo Práctico N°2 - 2024\n",
        "## Rajas de Ganges Chatbot\n",
        "\n",
        "Desarrollado por:\n",
        "- Asad, Gonzalo (A-4595/1)"
      ],
      "metadata": {
        "id": "MtoTIADFs00z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "RiIIka9gs3W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación del Entorno"
      ],
      "metadata": {
        "id": "t5u1osKts6Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install -y chromium-chromedriver\n",
        "# !pip install selenium\n",
        "# import os\n",
        "# os.environ[\"PATH\"] += \":/usr/bin/chromedriver\""
      ],
      "metadata": {
        "id": "ELYCCnCp6_in"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de librerías"
      ],
      "metadata": {
        "id": "HvuaR9EAs9OC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ZIj5HVPdi3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed520069-3cf8-4412-fc24-134259f1f06e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting redisgraph\n",
            "  Downloading redisgraph-2.4.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Collecting hiredis<3.0.0,>=2.0.0 (from redisgraph)\n",
            "  Downloading hiredis-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting prettytable<3.0.0,>=2.1.0 (from redisgraph)\n",
            "  Downloading prettytable-2.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting redis\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable<3.0.0,>=2.1.0->redisgraph) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading redisgraph-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hiredis-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.5/166.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prettytable-2.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: redis, prettytable, hiredis, redisgraph\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.12.0\n",
            "    Uninstalling prettytable-3.12.0:\n",
            "      Successfully uninstalled prettytable-3.12.0\n",
            "Successfully installed hiredis-2.4.0 prettytable-2.5.0 redis-3.5.3 redisgraph-2.4.4\n",
            "--2024-12-15 16:14:31--  http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
            "Resolving nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)... 91.189.91.83, 185.125.190.83, 91.189.91.82, ...\n",
            "Connecting to nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)|91.189.91.83|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318204 (1.3M) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M  7.41MB/s    in 0.2s    \n",
            "\n",
            "2024-12-15 16:14:31 (7.41 MB/s) - ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’ saved [1318204/1318204]\n",
            "\n",
            "Selecting previously unselected package libssl1.1:amd64.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "./\n",
            "./redis-stack-server-6.2.6-v7/\n",
            "./redis-stack-server-6.2.6-v7/bin/\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-benchmark\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-cli\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-sentinel\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-stack-server\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-check-rdb\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-check-aof\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-server\n",
            "./redis-stack-server-6.2.6-v7/share/\n",
            "./redis-stack-server-6.2.6-v7/share/RSAL_LICENSE\n",
            "./redis-stack-server-6.2.6-v7/share/APACHE_LICENSE\n",
            "./redis-stack-server-6.2.6-v7/lib/\n",
            "./redis-stack-server-6.2.6-v7/lib/redisgraph.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redistimeseries.so\n",
            "./redis-stack-server-6.2.6-v7/lib/rejson.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redisbloom.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redisearch.so\n",
            "./redis-stack-server-6.2.6-v7/etc/\n",
            "./redis-stack-server-6.2.6-v7/etc/README\n",
            "./redis-stack-server-6.2.6-v7/etc/redis-stack.conf\n",
            "./redis-stack-server-6.2.6-v7/etc/redis-stack-service.conf\n",
            "Starting redis-stack-server, database path ./redis-stack-server-6.2.6-v7/var/db/redis-stack\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow_text)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow_text)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.1.2)\n",
            "Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow_text-2.18.0\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (4.25.5)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.17.0)\n",
            "Collecting tensorflow<2.18,>=2.17 (from tf-keras>=2.14.1->tensorflow_hub)\n",
            "  Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.68.1)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.1.2)\n",
            "Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m98.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.0 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.17.1 tensorflow-2.17.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.1.2)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.10.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.26.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=4c4c575570dd26288b70d272e8d33c2bff0861d59a3d1e881e055a3ce03186fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.3\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
            "Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.3\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Collecting python-decouple==3.8\n",
            "  Downloading python_decouple-3.8-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting llm-templates\n",
            "  Downloading llm_templates-0.1.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.10/dist-packages (from llm-templates) (2.32.3)\n",
            "Requirement already satisfied: Jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llm-templates) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.11.3->llm-templates) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (2024.8.30)\n",
            "Downloading python_decouple-3.8-py3-none-any.whl (9.9 kB)\n",
            "Downloading llm_templates-0.1.12-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: python-decouple, llm-templates\n",
            "Successfully installed llm-templates-0.1.12 python-decouple-3.8\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "nohup: appending output to 'nohup.out'\n",
            "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕▏    0 B/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕▏    0 B/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   0% ▕▏  22 KB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   1% ▕▏  28 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   4% ▕▏  74 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   4% ▕▏  80 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   5% ▕▏ 107 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   6% ▕▏ 128 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   7% ▕▏ 146 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...   9% ▕▏ 184 MB/2.0 GB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  11% ▕▏ 224 MB/2.0 GB  224 MB/s      8s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  12% ▕▏ 236 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  13% ▕▏ 257 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  14% ▕▏ 286 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  15% ▕▏ 300 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  17% ▕▏ 335 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  19% ▕▏ 377 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  20% ▕▏ 394 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  21% ▕▏ 428 MB/2.0 GB  224 MB/s      7s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  24% ▕▏ 481 MB/2.0 GB  224 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  25% ▕▏ 508 MB/2.0 GB  224 MB/s      6s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  27% ▕▏ 540 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  29% ▕▏ 585 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  30% ▕▏ 601 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  31% ▕▏ 627 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  32% ▕▏ 649 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  33% ▕▏ 663 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  35% ▕▏ 700 MB/2.0 GB  259 MB/s      5s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  36% ▕▏ 722 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  37% ▕▏ 747 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  38% ▕▏ 766 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  39% ▕▏ 791 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  40% ▕▏ 805 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  41% ▕▏ 833 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  43% ▕▏ 860 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  43% ▕▏ 871 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  44% ▕▏ 897 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  46% ▕▏ 928 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  47% ▕▏ 943 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  48% ▕▏ 969 MB/2.0 GB  259 MB/s      4s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  49% ▕▏ 998 MB/2.0 GB  259 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  50% ▕▏ 1.0 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  52% ▕▏ 1.0 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  52% ▕▏ 1.1 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  53% ▕▏ 1.1 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  54% ▕▏ 1.1 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  56% ▕▏ 1.1 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  56% ▕▏ 1.1 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  58% ▕▏ 1.2 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  59% ▕▏ 1.2 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  60% ▕▏ 1.2 GB/2.0 GB  253 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  62% ▕▏ 1.2 GB/2.0 GB  248 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  63% ▕▏ 1.3 GB/2.0 GB  248 MB/s      3s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  63% ▕▏ 1.3 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  65% ▕▏ 1.3 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  67% ▕▏ 1.3 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  67% ▕▏ 1.4 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  68% ▕▏ 1.4 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  70% ▕▏ 1.4 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  70% ▕▏ 1.4 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  71% ▕▏ 1.4 GB/2.0 GB  248 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  73% ▕▏ 1.5 GB/2.0 GB  244 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  73% ▕▏ 1.5 GB/2.0 GB  244 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  75% ▕▏ 1.5 GB/2.0 GB  244 MB/s      2s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  76% ▕▏ 1.5 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  77% ▕▏ 1.6 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  78% ▕▏ 1.6 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  79% ▕▏ 1.6 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  81% ▕▏ 1.6 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  83% ▕▏ 1.7 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  84% ▕▏ 1.7 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  85% ▕▏ 1.7 GB/2.0 GB  244 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  87% ▕▏ 1.8 GB/2.0 GB  247 MB/s      1s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  88% ▕▏ 1.8 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  88% ▕▏ 1.8 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  90% ▕▏ 1.8 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  91% ▕▏ 1.8 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  92% ▕▏ 1.9 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  95% ▕▏ 1.9 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  96% ▕▏ 1.9 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  97% ▕▏ 2.0 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff...  99% ▕▏ 2.0 GB/2.0 GB  247 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB/2.0 GB  251 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB/2.0 GB  251 MB/s      0s\u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6...   0% ▕▏    0 B/1.4 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da...   0% ▕▏    0 B/7.7 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9...   0% ▕▏    0 B/6.0 KB                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5...   0% ▕▏    0 B/  96 B                  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Gpulling manifest \n",
            "pulling dde5aa3fc5ff... 100% ▕▏ 2.0 GB                         \n",
            "pulling 966de95ca8a6... 100% ▕▏ 1.4 KB                         \n",
            "pulling fcc5a6bec9da... 100% ▕▏ 7.7 KB                         \n",
            "pulling a70ff7e570d9... 100% ▕▏ 6.0 KB                         \n",
            "pulling 56bb8bd477a5... 100% ▕▏   96 B                         \n",
            "pulling 34bb5ab01051... 100% ▕▏  561 B                         \n",
            "verifying sha256 digest \n",
            "writing manifest \n",
            "success \u001b[?25h\n",
            "Collecting llama-index-llms-ollama\n",
            "  Downloading llama_index_llms_ollama-0.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.5-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.4 (from llama-index-llms-ollama)\n",
            "  Downloading llama_index_core-0.12.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ollama>=0.4.3 (from llama-index-llms-ollama)\n",
            "  Downloading ollama-0.4.4-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.10-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.0-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.54.5)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.11.10)\n",
            "Collecting dataclasses-json (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2024.10.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.28.1)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.10.3)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (9.0.0)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.17.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.6-py3-none-any.whl.metadata (814 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.2.2)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.2.0 (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting openai>=1.14.0 (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (4.12.3)\n",
            "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.5.17-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.9.11)\n",
            "Collecting httpx (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index) (2.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.14.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.1.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama)\n",
            "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (2024.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-legacy<0.10.0,>=0.9.48->llama-index) (1.17.0)\n",
            "Downloading llama_index_llms_ollama-0.5.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading llama_index-0.12.5-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_agent_openai-0.4.0-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.5-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_legacy-0.9.48.post4-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_llms_openai-0.3.10-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading ollama-0.4.4-py3-none-any.whl (13 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_cloud-0.1.6-py3-none-any.whl (195 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.8/195.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.17-py3-none-any.whl (14 kB)\n",
            "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: striprtf, filetype, dirtyjson, tenacity, pypdf, mypy-extensions, marshmallow, typing-inspect, tiktoken, httpx, openai, ollama, llama-cloud, dataclasses-json, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-llms-ollama, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.54.5\n",
            "    Uninstalling openai-1.54.5:\n",
            "      Successfully uninstalled openai-1.54.5\n",
            "Successfully installed dataclasses-json-0.6.7 dirtyjson-1.0.8 filetype-1.2.0 httpx-0.27.2 llama-cloud-0.1.6 llama-index-0.12.5 llama-index-agent-openai-0.4.0 llama-index-cli-0.4.0 llama-index-core-0.12.5 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-legacy-0.9.48.post4 llama-index-llms-ollama-0.5.0 llama-index-llms-openai-0.3.10 llama-index-multi-modal-llms-openai-0.4.0 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.17 marshmallow-3.23.1 mypy-extensions-1.0.0 ollama-0.4.4 openai-1.57.4 pypdf-5.1.0 striprtf-0.0.26 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "# Para base de datos de grafos\n",
        "!pip install networkx matplotlib redis redisgraph\n",
        "!wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!curl -fsSL https://packages.redis.io/redis-stack/redis-stack-server-6.2.6-v7.focal.x86_64.tar.gz -o redis-stack-server.tar.gz\n",
        "!tar -xvf redis-stack-server.tar.gz\n",
        "!./redis-stack-server-6.2.6-v7/bin/redis-stack-server --daemonize yes\n",
        "\n",
        "# Para base de datos vectorial\n",
        "!pip install tensorflow_text\n",
        "!pip install tensorflow_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow_text\n",
        "!pip install chromadb\n",
        "!pip install langchain\n",
        "!pip install youtube-transcript-api\n",
        "!pip install PyPDF2\n",
        "\n",
        "# Para web-scrapping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# Para uso general\n",
        "!pip install gdown\n",
        "\n",
        "# Para clasificación\n",
        "!pip install sentence_transformers\n",
        "\n",
        "# Para RAG\n",
        "!pip install python-decouple==3.8 llm-templates\n",
        "\n",
        "# Para el Agent\n",
        "# Descarga de Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "# Iniciamos Ollama en background\n",
        "!rm -f ollama_start.sh\n",
        "!echo '#!/bin/bash' > ollama_start.sh\n",
        "!echo 'ollama serve' >> ollama_start.sh\n",
        "# Make the script executable\n",
        "!chmod +x ollama_start.sh\n",
        "!nohup ./ollama_start.sh &\n",
        "!ollama pull llama3.2 > ollama.log\n",
        "!pip install llama-index-llms-ollama llama-index\n",
        "!nohup litellm --model ollama/llama3.2:latest --port 8000 > litellm.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga de librerías"
      ],
      "metadata": {
        "id": "W0-ywdcgs-8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para uso general\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Para base de datos de grafos\n",
        "import redis\n",
        "from redisgraph import Graph, Node, Edge\n",
        "import networkx as nx\n",
        "\n",
        "# Para base de datos vectorial\n",
        "import tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import chromadb\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import PyPDF2\n",
        "\n",
        "# Para web-scrapping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Para clasificación\n",
        "from huggingface_hub import InferenceClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Para re-ranking\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Para RAG\n",
        "import requests\n",
        "from decouple import config\n",
        "from google.colab import userdata\n",
        "from llm_templates import Formatter, Conversation\n",
        "\n",
        "# Para el agente\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent.react.formatter import ReActChatFormatter"
      ],
      "metadata": {
        "id": "c8RqDlfNtBUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de Bases de Datos"
      ],
      "metadata": {
        "id": "i6WFbNiVvj2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuentes de las bases de datos:\n",
        "*   [Board Game Geek](https://boardgamegeek.com/boardgame/220877/rajas-of-the-ganges)\n",
        "*   [Meesut Meeple](https://misutmeeple.com/2018/01/resena-rajas-of-the-ganges/)"
      ],
      "metadata": {
        "id": "YgMAJwFCxMce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentos de texto\n",
        "\n",
        "Contienen reglas del juego en sus diferentes modos y reseñas."
      ],
      "metadata": {
        "id": "NQ7AQaF6vnI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición de clases y funciones."
      ],
      "metadata": {
        "id": "Xdn3xFEs4IdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTranscripts(video_ids: list) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con transcripciones de videos de reseñas.\n",
        "\n",
        "  Parámetros:\n",
        "    - video_ids: Lista de identificadores de videos.\n",
        "\n",
        "  Retorno:\n",
        "    - transcript_list: Lista con transcripciones de los videos.\n",
        "  '''\n",
        "  # Incializar lista\n",
        "  transcript_list = []\n",
        "\n",
        "  # Extraer transcripciones\n",
        "  for video_id in video_ids:\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    transcript_list.append(transcript)\n",
        "\n",
        "  # Inicializar lista\n",
        "  final_transcripts = []\n",
        "\n",
        "  # Extraer cadenas de texto\n",
        "  for transcript in transcript_list:\n",
        "    text = \"\"\n",
        "    for line in transcript:\n",
        "      text += line['text'] + \" \"\n",
        "\n",
        "    # Limpieza de texto\n",
        "    text.replace('\\n', '')\n",
        "    text.replace('  ', ' ')\n",
        "\n",
        "    # Transcripciones finales\n",
        "    final_transcripts.append(text)\n",
        "\n",
        "  return final_transcripts\n",
        "\n",
        "\n",
        "def getPDFs(file_ids: list) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con documentos de texto.\n",
        "\n",
        "  Parámetros:\n",
        "    - file_ids: Lista de identificadores de archivos.\n",
        "\n",
        "  Retorno:\n",
        "    - file_transcripts: Lista con documentos de texto.\n",
        "  '''\n",
        "\n",
        "  pdfs_transcripts = []\n",
        "  i = 0\n",
        "\n",
        "  for file_id in file_ids:\n",
        "\n",
        "    # Creación la URL de descarga\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    # Descarga del archivo\n",
        "    output = f'file{i}.pdf'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "    # Abre el archivo en modo binario de lectura ('rb')\n",
        "    with open(f'file{i}.pdf', 'rb') as file:\n",
        "      # Crea un objeto PdfFileReader\n",
        "      reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "      # Inicializa una cadena vacía para almacenar el texto\n",
        "      text = ''\n",
        "\n",
        "      # Itera sobre todas las páginas del PDF\n",
        "      for i in range(len(reader.pages)):\n",
        "        # Obtiene la página\n",
        "        pagina = reader.pages[i]\n",
        "\n",
        "        # Extrae el texto de la página y lo añade a la cadena de texto\n",
        "        text += pagina.extract_text()\n",
        "\n",
        "      # Limpieza de texto\n",
        "      text.replace('\\n', '')\n",
        "      text.replace('  ', ' ')\n",
        "\n",
        "    pdfs_transcripts.append(text)\n",
        "    i += 1\n",
        "\n",
        "  return pdfs_transcripts\n",
        "\n",
        "\n",
        "def splitter(texts: list, metadatas: list, ids_names: str) -> tuple[list]:\n",
        "  '''\n",
        "  Crea splits de los textos recibidos, además de crear sus metadatas e IDs.\n",
        "\n",
        "  Parámetros:\n",
        "    - texts: lista con elementos a splittear.\n",
        "    - metadatas: lista con los nombres a usar como metadata.\n",
        "    - ids_names: lista con los nombres a usar en los IDs.\n",
        "\n",
        "  Retorna:\n",
        "    Listas con los splits de los textos, sus metadatas y sus IDs.\n",
        "  '''\n",
        "\n",
        "  # Creación de splitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "  # Definición de listas de salida\n",
        "  splitted_texts = []\n",
        "  splitted_metadatas = []\n",
        "  splitted_ids = []\n",
        "\n",
        "  # Creación de splits de texto y metadatas\n",
        "  for i in range(len(texts)):\n",
        "    splitted_text = text_splitter.split_text(texts[i])\n",
        "    splitted_texts += splitted_text\n",
        "    for j in range(len(splitted_text)):\n",
        "      splitted_metadatas.append(metadatas[i] + '_' + str(j))\n",
        "      # splitted_metadatas.append(metadatas[i])\n",
        "\n",
        "  # Creación de IDs\n",
        "  for k in range(len(splitted_texts)):\n",
        "    splitted_ids.append(ids_names + '_' + str(k))\n",
        "\n",
        "  return splitted_texts, splitted_metadatas, splitted_ids\n",
        "\n",
        "\n",
        "def createCollection() -> chromadb.api.models.Collection.Collection:\n",
        "  '''\n",
        "  Crea una colección de documentos con sus respectivos embeddings, metadata y IDs.\n",
        "\n",
        "  Parámetros:\n",
        "    - None.\n",
        "\n",
        "  Retorno:\n",
        "    - collection: colección de documentos.\n",
        "  '''\n",
        "\n",
        "  # Carga de transcripciones de videos de reseñas\n",
        "  video_ids = [\"EpG7XML-Vm8\", \"mSRN0DShdFM\", \"yd5DEeEv59U\", \"18__Sw5Jq54\"]\n",
        "  video_transcripts = getTranscripts(video_ids)\n",
        "\n",
        "  # Carga de PDFs de manuales\n",
        "  normal_rules_file_id = '1xKZv2r58VlZ2JzFyYhKtBbQc0mVBWo-1'\n",
        "  solo_rules_file_id = '1NLZE1hS9TmU-O7lTLgt0kVv9AohZ64io'\n",
        "  solo_ai_rules_file_id = '1p2geJvRyIYCdZSGmP403qqHApyibbvnH'\n",
        "  automa_rules_file_id = '1oyYWB_O2mQcUc8AHHflySVMqAWpjvo48'\n",
        "  pdf_ids = [normal_rules_file_id, solo_rules_file_id, solo_ai_rules_file_id, automa_rules_file_id]\n",
        "  pdfs_transcripts = getPDFs(pdf_ids)\n",
        "\n",
        "  # Definición de fuentes (metadata)\n",
        "  video_metadatas = [\"review_1\", \"review_2\", \"review_3\", \"review_4\"]\n",
        "  # video_metadatas = [\"review\", \"review\", \"review\", \"review\"]\n",
        "  pdf_metadatas = [\"normal_rules\", \"solo_rules\", \"ai_rules\", \"automa_rules\"]\n",
        "\n",
        "  # Definición de IDs para collections\n",
        "  # video_coll_ids = [f\"vid{i}\" for i in range(1, len(video_transcripts)+1)]\n",
        "  # pdf_coll_ids = [f\"pdf{i}\" for i in range(1, len(pdfs_transcripts)+1)]\n",
        "\n",
        "  # Splits de transcripciones\n",
        "  video_transcripts, video_metadatas, video_coll_ids = splitter(video_transcripts, video_metadatas, \"vid\")\n",
        "  pdfs_transcripts, pdf_metadatas, pdf_coll_ids = splitter(pdfs_transcripts, pdf_metadatas, \"pdf\")\n",
        "\n",
        "  # Calcular embeddings para los documentos\n",
        "  collection = chromadb_client.get_or_create_collection(\"all-my-documents\")\n",
        "\n",
        "  texts = video_transcripts + pdfs_transcripts\n",
        "  metadatas = video_metadatas + pdf_metadatas\n",
        "  texts_ids = video_coll_ids + pdf_coll_ids\n",
        "\n",
        "  embeddings = embed(texts).numpy().tolist()  # Convertir a lista para que sea serializable\n",
        "\n",
        "  collection.add(\n",
        "      documents=texts,\n",
        "      metadatas=[{\"source\": metadata} for metadata in metadatas],\n",
        "      ids=texts_ids,\n",
        "      embeddings=embeddings\n",
        "  )\n",
        "\n",
        "  return collection"
      ],
      "metadata": {
        "id": "xIxb7AKY0HUW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tablas\n",
        "\n",
        "Contienen atributos del juego."
      ],
      "metadata": {
        "id": "jekVJmQhv8xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de clases y funciones"
      ],
      "metadata": {
        "id": "pnwo7HfDhOtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDataFrame(file_id: str, encoding: str ='utf-8', delimiter: str = ',', show_head: bool = False) -> pd.DataFrame:\n",
        "  '''\n",
        "  Crea un DataFrame a partir de un archivo CSV alojado en Google Drive.\n",
        "\n",
        "  Parámetros:\n",
        "    - file_id: ID del archivo (codificado según Google Drive).\n",
        "    - encoding: Codificación de caracteres que se utilizará para leer el archivo.\n",
        "    - delimiter: Especifica el carácter que separa los valores en el archivo.\n",
        "    - show_head: Si es True, muestra las primeras filas del DataFrame.\n",
        "\n",
        "  Retorno:\n",
        "    - DataFrame: El DataFrame creado a partir del archivo.\n",
        "  '''\n",
        "\n",
        "  # Creación la URL de descarga\n",
        "  download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "  # Descarga del archivo\n",
        "  output = 'file'\n",
        "  gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "  df= pd.read_csv('file', encoding='utf-8', delimiter=',')\n",
        "\n",
        "  if show_head:\n",
        "    df.head()\n",
        "\n",
        "  return df\n",
        "\n",
        "def createTables() -> pd.DataFrame:\n",
        "  '''\n",
        "  Genera DataFrames con datos estadísticos y atributos del juego.\n",
        "\n",
        "  Retorno:\n",
        "    - Múltiples DataFrames con los datos de los juegos.\n",
        "  '''\n",
        "\n",
        "  # File IDs\n",
        "  # collection_stats_file_id = '153_EsmrYPTsnps_Y7t77FCbOUimN3YY6'\n",
        "  # game_attributes_file_id = '1f-_itA2DiGfUBYHrYKyW_Gakv1GN9PW5'\n",
        "  # game_ranks_file_id = '1EOJzdOAIQTrfeoL2Ryv0ZJe1IgQuH6GC'\n",
        "  # game_stats_file_id = '1ShOsowQX4BjsE2q7SdFcvbNXMoVvivKJ'\n",
        "  # parts_exchange_file_id = '1Qy6RV1a5Tv2wQIUCSPnDMJ2stqMRRBdN'\n",
        "  # play_stats_file_id = '1cjAHtDmMkGOL6OXv00GGMKSOAot6yKP1'\n",
        "  table_file_id = '1TBVbwYMja-DPXW8QrL8dwdwU586iEOO7'\n",
        "\n",
        "  # Descarga de DataFrames\n",
        "  # collection_stats = getDataFrame(collection_stats_file_id)\n",
        "  # game_attributes = getDataFrame(game_attributes_file_id)\n",
        "  # game_ranks = getDataFrame(game_ranks_file_id)\n",
        "  # game_stats = getDataFrame(game_stats_file_id)\n",
        "  # parts_exchange = getDataFrame(parts_exchange_file_id)\n",
        "  # play_stats = getDataFrame(play_stats_file_id)\n",
        "  table = getDataFrame(table_file_id)\n",
        "\n",
        "  return table # collection_stats, game_attributes, game_ranks, game_stats, parts_exchange, play_stats"
      ],
      "metadata": {
        "id": "2PUSIuPJv-9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grafos\n",
        "\n",
        "Contienen información de los creadores y artistas."
      ],
      "metadata": {
        "id": "ifQkA3t3v_V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de clases y funciones."
      ],
      "metadata": {
        "id": "B2U44GKROLxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RequestError(Exception):\n",
        "    '''Excepción personalizada para errores en las solicitudes.'''\n",
        "    pass\n",
        "\n",
        "def getValidation(url: str, retries: int = 3, backoff_factor: float = 0.5) -> str:\n",
        "  '''Valida la petición GET a la URL y en caso exitoso retorna la respuesta.\n",
        "     Implementa reintentos con backoff exponencial para manejar errores de timeout.'''\n",
        "  for i in range(retries):\n",
        "      try:\n",
        "          # Generación de la solicitud GET a la URL\n",
        "          response = requests.get(url)\n",
        "          # Verificación de si la respuesta tiene un código de error\n",
        "          response.raise_for_status()\n",
        "      except requests.exceptions.HTTPError as http_err:\n",
        "          if response.status_code == 504 and i < retries - 1:\n",
        "              time.sleep(backoff_factor * (2 ** i))  # Se espera antes de reintentar\n",
        "              print(f\"Reintentando {url} (intento {i + 1}/{retries})\")\n",
        "              continue # Se reintenta la solicitud\n",
        "          raise RequestError(f\"Error HTTP: {http_err}\")\n",
        "      except requests.exceptions.RequestException as err:\n",
        "          raise RequestError(f\"Error en la solicitud: {err}\")\n",
        "      else:\n",
        "          # Si no hay error se retorna la respuesta\n",
        "          return response\n",
        "\n",
        "def createGameList(url: str, credit_type: str) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con nombres de juegos mediante web-scrapping.\n",
        "\n",
        "  Parámetros:\n",
        "    - url: Dirección URL hacia donde realizar el web-scrapping.\n",
        "    - credit_type: Cadena de texto que indica el tipo de crédido (creador o artista).\n",
        "\n",
        "  Retorno:\n",
        "    - game_list: Lista con nombres de juegos.\n",
        "  '''\n",
        "\n",
        "  # Petición GET a la URL.\n",
        "  response = getValidation(url)\n",
        "\n",
        "  # Extraigo el contenido\n",
        "  data = response.text\n",
        "\n",
        "  # Convertir el HTML a JSON (simulación del proceso que realmente realiza el frontend)\n",
        "  start_marker = 'GEEK.geekitemPreload = '\n",
        "  end_marker = 'GEEK.geekitemSettings'\n",
        "  start_idx = data.find(start_marker) + len(start_marker)\n",
        "  end_idx = data.find(end_marker, start_idx)\n",
        "  json_data = json.loads(data[start_idx:end_idx-3])\n",
        "\n",
        "  # Extraer nombres de los juegos de mesa\n",
        "  game_list = [item['name'] for item in json_data['item']['links'][credit_type]]\n",
        "\n",
        "  return game_list\n",
        "\n",
        "def createCreditData(game_names_inka_brand: list, game_names_markus_brand: list, game_names_dennis_lohaussen: list) -> pd.DataFrame:\n",
        "  '''\n",
        "  Genera DataFrames con datos de créditos.\n",
        "\n",
        "  Parámetros:\n",
        "    - game_names_inka_brand: Lista con nombres de juegos de mesa de Inka Brand.\n",
        "    - game_names_markus_brand: Lista con nombres de juegos de mesa de Markus Brand.\n",
        "    - game_names_dennis_lohaussen: Lista con nombres de juegos de mesa de Dennis Lohausen.\n",
        "\n",
        "  Retorno:\n",
        "    - Múltiples DataFrames con los datos de los créditos del juego.\n",
        "  '''\n",
        "  # Datos de créditos\n",
        "  designers = pd.DataFrame({\n",
        "      'name': ['Inka Brand', 'Markus Brand'],\n",
        "      'sex': ['Female', 'Male']\n",
        "  })\n",
        "\n",
        "  artists = pd.DataFrame({\n",
        "      'name': ['Dennis Lohausen'],\n",
        "      'sex': ['Male']\n",
        "  })\n",
        "\n",
        "  primary_names = pd.DataFrame({\n",
        "      'name': ['Rajas of the Ganges'],\n",
        "      'year': [2017]\n",
        "  })\n",
        "\n",
        "  alternative_names = pd.DataFrame({\n",
        "      'name': ['I Ragià del Gange', 'Rijs van de Gang', 'Rajas de la Gange', 'Rajas der Gange', 'Rajas del Ganges', 'Раджи Ганга', 'ガンジスの霊王', '갠지스의 라자', 'Rajas do Gange'],\n",
        "      'language': ['Italian', 'Dutch', 'French', 'German', 'Spanish', 'Russian', 'Japanese', 'Korean', 'Portuguese']\n",
        "  })\n",
        "\n",
        "  categories = pd.DataFrame({\n",
        "      'name': ['Dice', 'Economic', 'Renaissance', 'Territory Building']\n",
        "  })\n",
        "\n",
        "  mechanisms = pd.DataFrame({\n",
        "      'name': ['Connections', 'Dice Rolling', 'Race', 'Tile Placement', 'Track Movement', 'Worker Placement', 'Worker Placement with Dice Workers']\n",
        "  })\n",
        "\n",
        "  publishers = pd.DataFrame({\n",
        "      'name': ['HUCH!', '999 Games', 'Devir', 'Dice Realm', 'DV Games', 'Egmont Polska', 'Fabrika Igr', 'Game Harbor', 'HOT Games', 'nostalgia III', 'R&R Games']\n",
        "  })\n",
        "\n",
        "  inka_brand_designed_games = pd.DataFrame({\n",
        "      'name': game_names_inka_brand\n",
        "  })\n",
        "\n",
        "  markus_brand_designed_games = pd.DataFrame({\n",
        "      'name': game_names_markus_brand\n",
        "  })\n",
        "\n",
        "  dennis_lohaussen_artworked_games = pd.DataFrame({\n",
        "      'name': game_names_dennis_lohaussen\n",
        "  })\n",
        "\n",
        "  expansions = pd.DataFrame({\n",
        "      'name': ['Rajas of the Ganges: Goodie Box 2', 'Rajas of the Ganges: Goodie Box 1', 'Rajas of the Ganges: Blessings of Kedarnath', 'Rajas of the Ganges: Tiger Expansion',\n",
        "               'Rajas of the Ganges: Snake Expansion', 'Deutscher Spielepreis 2018 Goodie Box', 'Brettspiel Adventskalender 2018', 'Rajas of the Ganges: Mango Village', 'Brettspiel Adventskalender 2017'],\n",
        "      'year': ['2020', '2019', '2019', '2018', '2018', '2018', '2018', '2017', '2017']\n",
        "  })\n",
        "\n",
        "  return designers, artists, primary_names, alternative_names, categories, mechanisms, publishers, inka_brand_designed_games, markus_brand_designed_games, dennis_lohaussen_artworked_games, expansions\n",
        "\n",
        "def createGraph():\n",
        "  '''\n",
        "  Crea un grafo de RedisGraph.\n",
        "\n",
        "  Parámetros:\n",
        "    - None.\n",
        "\n",
        "  Retorno:\n",
        "    - None.\n",
        "  '''\n",
        "  # URLs de la páginas de donde quiero extraer las infos\n",
        "  url_inka = 'https://boardgamegeek.com/boardgamedesigner/6940/inka-brand/linkeditems/boardgamedesigner'\n",
        "  url_markus = 'https://boardgamegeek.com/boardgamedesigner/6941/markus-brand/linkeditems/boardgamedesigner'\n",
        "  url_dennis = 'https://boardgamegeek.com/boardgameartist/12484/dennis-lohausen/linkeditems/boardgameartist'\n",
        "\n",
        "  # Extraer nombres de los juegos de mesa\n",
        "  game_names_inka_brand = createGameList(url_inka, 'boardgamedesigner')\n",
        "  game_names_markus_brand = createGameList(url_markus, 'boardgamedesigner')\n",
        "  game_names_dennis_lohaussen = createGameList(url_dennis, 'boardgameartist')\n",
        "\n",
        "  # Crear DataFrames de créditos\n",
        "  designers, artists, primary_names, alternative_names, categories, mechanisms, publishers, inka_brand_designed_games, markus_brand_designed_games, dennis_lohaussen_artworked_games, expansions = createCreditData(game_names_inka_brand, game_names_markus_brand, game_names_dennis_lohaussen)\n",
        "\n",
        "  # Crear nodos\n",
        "  designer_node_list = []\n",
        "  for index, row in designers.iterrows():\n",
        "      designer_node_list.append(Node(label='Designer', properties={'name': row['name'], 'sex': row['sex']}))\n",
        "\n",
        "  artist_node_list = []\n",
        "  for index, row in artists.iterrows():\n",
        "      artist_node_list.append(Node(label='Artist', properties={'name': row['name'], 'sex': row['sex']}))\n",
        "\n",
        "  primary_name_node_list = []\n",
        "  for index, row in primary_names.iterrows():\n",
        "      primary_name_node_list.append(Node(label='Primary_Name', properties={'name': row['name'], 'year': row['year']}))\n",
        "\n",
        "  alternative_name_node_list = []\n",
        "  for index, row in alternative_names.iterrows():\n",
        "      alternative_name_node_list.append(Node(label='Alternative_Name', properties={'name': row['name'], 'language': row['language']}))\n",
        "\n",
        "  category_node_list = []\n",
        "  for index, row in categories.iterrows():\n",
        "      category_node_list.append(Node(label='Category', properties={'name': row['name']}))\n",
        "\n",
        "  mechanism_node_list = []\n",
        "  for index, row in mechanisms.iterrows():\n",
        "      mechanism_node_list.append(Node(label='Mechanism', properties={'name': row['name']}))\n",
        "\n",
        "  publisher_node_list = []\n",
        "  for index, row in publishers.iterrows():\n",
        "      publisher_node_list.append(Node(label='Publisher', properties={'name': row['name']}))\n",
        "\n",
        "  inka_brand_designed_games_list = []\n",
        "  for index, row in inka_brand_designed_games.iterrows():\n",
        "      inka_brand_designed_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  markus_brand_designed_games_list = []\n",
        "  for index, row in markus_brand_designed_games.iterrows():\n",
        "      markus_brand_designed_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  dennis_lohaussen_artworked_games_list = []\n",
        "  for index, row in dennis_lohaussen_artworked_games.iterrows():\n",
        "      dennis_lohaussen_artworked_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  expansions_node_list = []\n",
        "  for index, row in expansions.iterrows():\n",
        "      expansions_node_list.append(Node(label='Expansion', properties={'name': row['name'], 'year': row['year']}))\n",
        "\n",
        "  # Agregar los nodos al gráfico\n",
        "  for node in designer_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in artist_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in primary_name_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in alternative_name_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in category_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in mechanism_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in publisher_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in inka_brand_designed_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in markus_brand_designed_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in dennis_lohaussen_artworked_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in expansions_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  # Crear más relaciones (aristas)\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for designer in designer_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_DESIGNER', designer))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for artist in artist_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_ARTIST', artist))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for alternative_name in alternative_name_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_ALTERNATIVE_NAME', alternative_name))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for category in category_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_CATEGORY', category))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for mechanism in mechanism_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_MECHANISM', mechanism))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for publisher in publisher_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_PUBLISHER', publisher))\n",
        "\n",
        "  for game in inka_brand_designed_games_list:\n",
        "          graph.add_edge(Edge(designer_node_list[0], 'DESIGNED', game))\n",
        "\n",
        "  for game in markus_brand_designed_games_list:\n",
        "          graph.add_edge(Edge(designer_node_list[1], 'DESIGNED', game))\n",
        "\n",
        "  for game in dennis_lohaussen_artworked_games_list:\n",
        "          graph.add_edge(Edge(artist_node_list[0], 'ARTWORKED', game))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for expansion in expansions_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_EXPANSION', expansion))"
      ],
      "metadata": {
        "id": "eILPIJNYJZi4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebas sobre la base de datos de Grafos"
      ],
      "metadata": {
        "id": "-_h8KbWCZVP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ejecutar una consulta para comprobar los nodos\n",
        "# query = \"\"\"MATCH (p:Designer) RETURN p.name, p.sex\"\"\"\n",
        "# result = graph.query(query)\n",
        "\n",
        "# # Imprimir los resultados de la consulta\n",
        "# for record in result.result_set:\n",
        "#     print(f\"Person: {record[0]}, Sex: {record[1]}\")"
      ],
      "metadata": {
        "id": "FP7ZY9WXCKB0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de la base de datos de Grafos"
      ],
      "metadata": {
        "id": "_RMC6CWeZZ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Realizar la consulta para obtener nodos y relaciones\n",
        "# query = \"\"\"MATCH (a)-[r]->(b) RETURN a.name, b.name, type(r)\"\"\"\n",
        "# result = graph.query(query)\n",
        "\n",
        "# # Crear un grafo dirigido con networkx\n",
        "# G = nx.DiGraph()\n",
        "\n",
        "# # Agregar los nodos y relaciones a networkx\n",
        "# for record in result.result_set:\n",
        "#     record_1 = record[0]\n",
        "#     record_2 = record[1]\n",
        "#     relation = record[2]\n",
        "\n",
        "#     G.add_node(record_1)\n",
        "#     G.add_node(record_2)\n",
        "#     G.add_edge(record_1, record_2, label=relation)\n",
        "\n",
        "# # Ajustar layout para una mayor separación entre los nodos\n",
        "# pos = nx.spring_layout(G, k=1.5, iterations=50)  # 'k' controla la distancia entre nodos\n",
        "\n",
        "# plt.figure(figsize=(10, 8))  # Aumentar el tamaño de la figura\n",
        "\n",
        "# # Dibujar nodos y etiquetas\n",
        "# nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=12, font_weight=\"bold\", arrows=True)\n",
        "\n",
        "# # Dibujar las etiquetas de las relaciones (aristas) con un desplazamiento\n",
        "# edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', label_pos=0.7)  # Ajustar label_pos para desplazar etiquetas\n",
        "\n",
        "# # Mejorar la disposición de los nodos para que no se superpongan\n",
        "# for label in pos:\n",
        "#     pos[label][1] += 0.05  # Elevar un poco las etiquetas de los nodos\n",
        "\n",
        "# plt.title(\"Visualización del grafo de RedisGraph\", fontsize=14)\n",
        "# plt.show()\n",
        "\n",
        "# # Imprimir todas las relaciones encontradas en formato tríada (Node)-[Relation]->(Node)\n",
        "# for edge in G.edges(data=True):\n",
        "#     print(f\"({edge[0]})-[{edge[2]['label']}]->({edge[1]})\")"
      ],
      "metadata": {
        "id": "8WH-pR7HEU8e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador"
      ],
      "metadata": {
        "id": "fZmkhuNOo8uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basada en modelo entrenado con ejemplos y embeddings"
      ],
      "metadata": {
        "id": "Am4bHggtpmMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randomForestClassifierCreator(production: bool = False) -> RandomForestClassifier:\n",
        "  '''\n",
        "  Genera un clasificador de textos basado en RandomForest.\n",
        "  Clasifica en cuatro categorías: review, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - production: si es False, se calculan y muestran las métricas de prueba.\n",
        "\n",
        "  Retorno:\n",
        "    - Modelo de RandomForest de clasificación.\n",
        "  '''\n",
        "  # Carga del modelo desde HuggingFace\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "  # Carga del dataset\n",
        "  file_id = '1j2MCy0CACVHFVa3YfirqssUNcbQmq2Gn'\n",
        "  df = getDataFrame(file_id)\n",
        "\n",
        "  # Generación de estructura con el dataset recibido\n",
        "  dataset = []\n",
        "  for row in df.itertuples():\n",
        "      dataset.append((row.prompt, row.classification))\n",
        "\n",
        "  # Preparación de X e y\n",
        "  X = [prompt.lower() for prompt, classification in dataset]\n",
        "  y = [classification for prompt, classification in dataset]\n",
        "\n",
        "  # División del dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#, stratify=y)\n",
        "\n",
        "  # Obtención los embeddings de BERT para los conjuntos de entrenamiento\n",
        "  X_train_vectorized = model.encode(X_train)\n",
        "\n",
        "  ## Creación y entrenamiento del modelo de RandomForest\n",
        "\n",
        "  # Definición de los parámetros\n",
        "  param_grid = {\n",
        "      'n_estimators': [200, 500, 700, 900],   #[50, 100, 150, 200, 500, 700],\n",
        "      'max_depth': [3, 5, 10, 15, 20]         #[None, 1, 2, 3, 4, 5, 10, 15]\n",
        "  }\n",
        "\n",
        "  # Creación el objeto GridSearchCV\n",
        "  grid_search_cv = GridSearchCV(\n",
        "      estimator=RandomForestClassifier(random_state=42),  # Modelo Random Forest\n",
        "      param_grid=param_grid,\n",
        "      scoring='accuracy',  # Métrica para evaluar\n",
        "      n_jobs=-1,           # Usar todos los núcleos disponibles\n",
        "      cv=5,                # 5 particiones para validación cruzada\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  # Ajuste del modelo con los datos de entrenamiento\n",
        "  grid_search_cv.fit(X_train_vectorized, y_train)\n",
        "\n",
        "  # Obtención del mejor estimador\n",
        "  modelo = grid_search_cv.best_estimator_\n",
        "  best_params = grid_search_cv.best_params_\n",
        "\n",
        "  # Entrenamiento del modelo\n",
        "  modelo.fit(X_train_vectorized, y_train)\n",
        "\n",
        "  if not production:\n",
        "    # Obtención de los embeddings de BERT para los conjuntos de prueba\n",
        "    X_test_vectorized = model.encode(X_test)\n",
        "\n",
        "    # Evaluación del modelo\n",
        "    y_pred_train = modelo.predict(X_train_vectorized)\n",
        "    y_pred_test = modelo.predict(X_test_vectorized)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_test)\n",
        "    report = classification_report(y_test, y_pred_test, zero_division=1)\n",
        "\n",
        "    print(\"\\nExactitud Entrenamiento:\", train_acc)\n",
        "    print(\"Exactitud Prueba:\", test_acc)\n",
        "    print(\"\\nMejores parámetros:\", best_params)\n",
        "    print(\"\\nMatriz de confusión:\", matrix)\n",
        "    print(\"\\nReporte de clasificación:\", report)\n",
        "\n",
        "  return modelo\n",
        "\n",
        "\n",
        "def rfClassifier(modelo: RandomForestClassifier, prompt: str) -> str:\n",
        "  '''\n",
        "  Clasificador de textos basado en RandomForest. Recibe un prompt por parte del usuario\n",
        "  y lo clasifica en cuatro categorías: reviews, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - modelo: Modelo de clasificación.\n",
        "    - prompt: Texto a clasificar.\n",
        "\n",
        "  Retorno:\n",
        "    - Clasificación del texto.\n",
        "  '''\n",
        "  # Carga del modelo desde HuggingFace\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "  # Adaptación del texto\n",
        "  prompt = [prompt.lower()]\n",
        "\n",
        "  # Preprocesamiento y vectorización de las nuevas frases\n",
        "  prompt_vectorized = model.encode(prompt)\n",
        "\n",
        "  # Predicción con el modelo entrenado\n",
        "  classification = modelo.predict(prompt_vectorized)\n",
        "\n",
        "  # Impresión del prompt y su etiquetado\n",
        "  # print(f\"\\nPrompt: '{prompt}'\")\n",
        "  # print(f\"Clasificación predicha: {classification}\\n\")\n",
        "\n",
        "  return classification"
      ],
      "metadata": {
        "id": "6P9acraLpAXU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basada en LLM"
      ],
      "metadata": {
        "id": "hkzICjMlpu6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llmClassifier(query_str: str) -> str:\n",
        "  '''\n",
        "  Clasificador de textos basado en LLM. Recibe un prompt por parte del usuario\n",
        "  y lo clasifica en cuatro categorías: review, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - query_str: Cadena de texto que representa la consulta del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Clasificación del texto.\n",
        "  '''\n",
        "  # Armado de la query\n",
        "  prompt = (\n",
        "        \"Classify text strictly into just one of the following words: review, rules, stats and credits.\\n\"\n",
        "        \"Do not use any other words on your answer.\\n\"\n",
        "        \"Categories and mechanisms should be classified as credits.\\n\"\n",
        "        \"Amount of players, playing time and ages should be classified as stats.\\n\"\n",
        "        f\"Question: {query_str}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = llm_class_client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "mFfU8I0PpxOf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever"
      ],
      "metadata": {
        "id": "lFJrYJ2KTnTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retriever(query: str) -> list[str]:\n",
        "  '''\n",
        "  Función que decide a qué base de datos extraer en base a la clasificación, obteniendo los textos contextuales.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Textos contextuales.\n",
        "  '''\n",
        "  # Caso 1: Base de datos vectorial\n",
        "  if classification == \"review\" or classification == \"rules\":\n",
        "    results = vectorialDbRetriever(query)\n",
        "\n",
        "    # Re-ranking\n",
        "    results = reranker(query, results[0], 3)\n",
        "\n",
        "  # Caso 2: Base de datos tabular\n",
        "  elif classification == \"stats\":\n",
        "    results = tableDbRetriever(query)\n",
        "\n",
        "  # Caso 3: Base de datos de grafos\n",
        "  elif classification == \"credits\":\n",
        "    results = graphDbRetriever(query)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def vectorialDbRetriever(query: str) -> list[str]:\n",
        "  '''\n",
        "  Retriever de texto basado en vectores. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Lista de textos contextuales.\n",
        "  '''\n",
        "\n",
        "  # Inicialización de lista de retorno\n",
        "  retriever_list = []\n",
        "\n",
        "  # Generación de embedding de la query\n",
        "  embedding_query = embed([query]).numpy().tolist()\n",
        "\n",
        "  # Refinado de metadata de reglas\n",
        "  if classification == \"rules\":\n",
        "    # Armado de la query\n",
        "    prompt = (\n",
        "          f\"Classify text strictly into just one of the following rules category: normal, solo, ai and automa.\\n\"\n",
        "          \"If none of the classifications seems correct enough, classify the text as normal.\"\n",
        "          \"Do not use any other words on your answer.\\n\"\n",
        "          f\"Question: {query}\\n\"\n",
        "          \"Answer: \"\n",
        "      )\n",
        "\n",
        "    # Armado de los mensajes\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Inferencia\n",
        "    completion = llm_class_client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "      messages=messages,\n",
        "      max_tokens=1000\n",
        "    )\n",
        "\n",
        "    # Categría elegida\n",
        "    category = completion.choices[0].message.content\n",
        "  else:\n",
        "    category = classification\n",
        "\n",
        "  # Filtrado de metadatas\n",
        "  filtered_metadatas = [\n",
        "      metadata\n",
        "      for metadata in collection.get(include=[\"metadatas\"])[\"metadatas\"]\n",
        "      if category in metadata.get(\"source\", \"\")\n",
        "  ]\n",
        "\n",
        "  # Extraer sources\n",
        "  sources = []\n",
        "  for metadata in filtered_metadatas:\n",
        "      sources.append(metadata[\"source\"])\n",
        "\n",
        "  # Búsqueda de resultados\n",
        "  results = collection.query(\n",
        "      query_embeddings = embedding_query,  # Aquí pasamos el embedding de la consulta\n",
        "      n_results = 10,  # Traemos los 10 resultados más cercanos\n",
        "      where = {\"source\": {\"$in\": sources}}  # Filtrar por metadatos\n",
        "  )\n",
        "\n",
        "  # Adjuntado de resultados\n",
        "  for doc in results[\"documents\"]:\n",
        "    retriever_list.append(doc)\n",
        "\n",
        "  return retriever_list\n",
        "\n",
        "\n",
        "def tableDbRetriever(query: str) -> str:\n",
        "  '''\n",
        "  Retriever de texto basado en tablas. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - DataFrame con la información recuperada.\n",
        "  '''\n",
        "\n",
        "  # Columnas del DataFrame\n",
        "  columns = table_df.columns.tolist()\n",
        "\n",
        "  # Armado de la query\n",
        "  prompt = (\n",
        "        f\"Classify text strictly into just one of the following words: {columns}.\\n\"\n",
        "        \"Do not use any other words on your answer.\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = llm_class_client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        "  )\n",
        "\n",
        "  # Columna elegida\n",
        "  selected_column = completion.choices[0].message.content\n",
        "\n",
        "  # Selección de resultados\n",
        "  results = selected_column + \": \" + str(table_df[selected_column].iloc[0])\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def graphDbRetriever(query: str) -> str:\n",
        "  '''\n",
        "  Retriever de texto basado en grafos. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Lista de textos contextuales.\n",
        "  '''\n",
        "\n",
        "  # Lista de relaciones\n",
        "  relations = [\n",
        "      (\"Inka Brand\", \"DESIGNED\", [\"Sausage Sizzle!\", \"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\",\n",
        "                                  \"Andor: The Family Fantasy Game\", \"Andor: The Family Fantasy Game – The Danger in the Shadows\",\n",
        "                                  \"Bibi & Tina: Das Spiel zum Film\", \"Bitte nicht öffnen: Bissig!\"]),\n",
        "      (\"Markus Brand\", \"DESIGNED\", [\"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\", \"Andor: The Family Fantasy Game\",\n",
        "                                    \"Andor: The Family Fantasy Game – The Danger in the Shadows\", \"Bibi & Tina: Das Spiel zum Film\",\n",
        "                                    \"Bitte nicht öffnen: Bissig!\", \"Black Stories: Das Spiel\"]),\n",
        "      (\"Dennis Lohausen\", \"ARTWORKED\", [\"Coal Baron: The Great Card Game\", \"100!\", \"1001 Karawane\",\n",
        "                                        \"1001 Karawane: Sonderchips\", \"112: Brandgefährlich\", \"15\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_DESIGNER\", [\"Inka Brand\", \"Markus Brand\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_ARTIST\", [\"Dennis Lohausen\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_ALTERNATIVE_NAME\", [\"I Ragià del Gange\", \"Rijs van de Gang\", \"Rajas de la Gange\",\n",
        "                                                       \"Rajas der Gange\", \"Rajas del Ganges\", \"Раджи Ганга\", \"ガンジスの霊王\",\n",
        "                                                       \"갠지스의 라자\", \"Rajas do Gange\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_CATEGORY\", [\"Dice\", \"Economic\", \"Renaissance\", \"Territory Building\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_MECHANISM\", [\"Connections\", \"Dice Rolling\", \"Race\", \"Tile Placement\", \"Track Movement\",\n",
        "                                                \"Worker Placement\", \"Worker Placement with Dice Workers\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_PUBLISHER\", [\"HUCH!\", \"999 Games\", \"Devir\", \"Dice Realm\", \"DV Games\", \"Egmont Polska\",\n",
        "                                                \"Fabrika Igr\", \"Game Harbor\", \"HOT Games\", \"nostalgia III\", \"R&R Games\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_EXPANSION\", [\"Rajas of the Ganges: Goodie Box 2\", \"Rajas of the Ganges: Goodie Box 1\",\n",
        "                                                \"Rajas of the Ganges: Blessings of Kedarnath\", \"Rajas of the Ganges: Tiger Expansion\",\n",
        "                                                \"Rajas of the Ganges: Snake Expansion\", \"Deutscher Spielepreis 2018 Goodie Box\",\n",
        "                                                \"Brettspiel Adventskalender 2018\", \"Rajas of the Ganges: Mango Village\",\n",
        "                                                \"Brettspiel Adventskalender 2017\"])\n",
        "  ]\n",
        "\n",
        "  # Lista de entidades\n",
        "  entities = {\n",
        "      \"Game\": [\"Sausage Sizzle!\", \"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\",\n",
        "                \"Andor: The Family Fantasy Game\", \"Andor: The Family Fantasy Game – The Danger in the Shadows\",\n",
        "                \"Bibi & Tina: Das Spiel zum Film\", \"Bitte nicht öffnen: Bissig!\", \"Black Stories: Das Spiel\",\n",
        "                \"Coal Baron: The Great Card Game\", \"100!\", \"1001 Karawane\", \"1001 Karawane: Sonderchips\",\n",
        "                \"112: Brandgefährlich\", \"15\"],\n",
        "      \"Primary_Name\": [\"Rajas of the Ganges\"],\n",
        "      \"Designer\": [\"Inka Brand\", \"Markus Brand\"],\n",
        "      \"Artist\": [\"Dennis Lohausen\"],\n",
        "      \"Alternative_Name\": [\"I Ragià del Gange\", \"Rijs van de Gang\", \"Rajas de la Gange\", \"Rajas der Gange\",\n",
        "                           \"Rajas del Ganges\", \"Раджи Ганга\", \"ガンジスの霊王\", \"갠지스의 라자\", \"Rajas do Gange\"],\n",
        "      \"Category\": [\"Dice\", \"Economic\", \"Renaissance\", \"Territory Building\"],\n",
        "      \"Mechanism\": [\"Connections\", \"Dice Rolling\", \"Race\", \"Tile Placement\", \"Track Movement\",\n",
        "                    \"Worker Placement\", \"Worker Placement with Dice Workers\"],\n",
        "      \"Publisher\": [\"HUCH!\", \"999 Games\", \"Devir\", \"Dice Realm\", \"DV Games\", \"Egmont Polska\",\n",
        "                    \"Fabrika Igr\", \"Game Harbor\", \"HOT Games\", \"nostalgia III\", \"R&R Games\"],\n",
        "      \"Expansion\": [\"Rajas of the Ganges: Goodie Box 2\", \"Rajas of the Ganges: Goodie Box 1\",\n",
        "                    \"Rajas of the Ganges: Blessings of Kedarnath\", \"Rajas of the Ganges: Tiger Expansion\",\n",
        "                    \"Rajas of the Ganges: Snake Expansion\", \"Deutscher Spielepreis 2018 Goodie Box\",\n",
        "                    \"Brettspiel Adventskalender 2018\", \"Rajas of the Ganges: Mango Village\", \"Brettspiel Adventskalender 2017\"]\n",
        "  }\n",
        "\n",
        "  # Armado de la query\n",
        "  prompt = (\"Write down a Cypher query based on a question from a user.\\n\"\n",
        "        \"It must strictly be just one query, it is important that you respect all of Cypher's syntax rules and use the entities and relationships provided.\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\\n\"\n",
        "          \"You are an expert in board games and understand their descriptive characteristics.\\n\"\n",
        "          \"As an expert on the game Rajas of the Ganges, you are also an expert on Cypher queries based on a question made by a user.\\n\"\n",
        "          \"The database is composed by entities and relations. The entities from the database can be found in the following Python dictionary:\\n\"\n",
        "          f\"{entities}, where the keys are the entities and the labels are the same ones they have on the database.\\n\"\n",
        "          \"The relations from the database can be found in the following Python list:\\n\"\n",
        "          f\"{relations}.\\n\"\n",
        "          \"Make sure to give a full statement, for example, if the question is \"\"What are the mechanisms of the game?\"\"\\n\"\n",
        "          \"the answer should be: cypher MATCH (game:Primary_Name {name: 'Rajas of the Ganges'})-[:HAS_MECHANISM]->(mechanism:Mechanism) RETURN mechanism.name\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = llm_class_client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=30000\n",
        "  )\n",
        "\n",
        "  # Query para Cypher\n",
        "  cypher_query = completion.choices[0].message.content\n",
        "  cypher_query = cypher_query.replace(\"`\", \"\")\n",
        "  cypher_query = cypher_query.replace(\"'''\", \"\")\n",
        "  cypher_query = cypher_query.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "  # Resultado de la búsqueda\n",
        "  result = graph.query(cypher_query)\n",
        "\n",
        "  # Guardar los resultados de la consulta\n",
        "  results = \"\"\n",
        "  first = True\n",
        "  for record in result.result_set:\n",
        "    if first:\n",
        "      for key, values in entities.items():\n",
        "        if record[0] in values:\n",
        "          results += f\"{key}: {record[0]}\"\n",
        "      first = False\n",
        "    else:\n",
        "      for key, values in entities.items():\n",
        "        if record[0] in values:\n",
        "          results += f\", {key}: {record[0]}\"\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "AJZC8_EHTqQk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Ranker"
      ],
      "metadata": {
        "id": "ahBg486gAfbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reranker(query: str, documents: list, top_k: int = 3):\n",
        "  '''\n",
        "  Realiza un re-ranking sobre los documentos obtenidos tras una búsqueda en\n",
        "  la base de datos vectorial.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - documents: resultados obtenidos tras la búsqueda en base de datos vectorial.\n",
        "    - top_k: número de documentos a retornar.\n",
        "\n",
        "  Retorna:\n",
        "    - Lista de documentos ordenados por relevancia.\n",
        "  '''\n",
        "  # Carga del modelo para re-ranking\n",
        "  model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "  # Emparejamiento de prompt del usuario con documentos para hacer predicciones\n",
        "  pairs = [(query, document) for document in documents]\n",
        "  scores = model.predict(pairs)\n",
        "\n",
        "  # Emparejamiento de puntajes de predicción con documentos y ordenado\n",
        "  scored_docs = list(zip(scores, documents))\n",
        "  scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  # Toma de los documentos mejor puntuados\n",
        "  scored_docs = scored_docs[:top_k]\n",
        "\n",
        "  # Armado de string contextual final\n",
        "  complete_context = \"\"\n",
        "  for score, doc in scored_docs:\n",
        "    complete_context += doc + \";\"\n",
        "\n",
        "  return complete_context"
      ],
      "metadata": {
        "id": "p9k7ZMT-Akk7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "VKOdtw9S94_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG(query_str: str, context_str: str, api_key: str, add_generation_prompt: bool = True, max_new_tokens: int = 768) -> str:\n",
        "  '''\n",
        "  Realiza una consulta a un modelo de LLM basado en el prompt del usuario y\n",
        "  su información contextual obtenida de las consultas a las bases de datos.\n",
        "\n",
        "  Parámetros:\n",
        "    - query_str: prompt del usuario.\n",
        "    - context_str: datos contextuales obtenidos de las bases de datos.\n",
        "    - api_key: clave de la API de Hugging Face.\n",
        "    - add_generation_prompt: indica si se debe agregar el prompt que lo generó.\n",
        "    - max_new_tokens: número máximo de tokens a generar.\n",
        "\n",
        "  Retorna:\n",
        "    - String con el resultado del LLM.\n",
        "  '''\n",
        "  # Armado del prompt\n",
        "  TEXT_QA_PROMPT_TMPL = (\n",
        "      \"The contextual information is as follows:\\n\"\n",
        "      \"---------------------\\n\"\n",
        "      f\"{context_str}\\n\"\n",
        "      \"---------------------\\n\"\n",
        "      \"Given said contextual information and without using any previous knowledge, answer the following question.\\n\"\n",
        "      f\"Question: {query_str}\\n\"\n",
        "      \"Answer: \"\n",
        "  )\n",
        "\n",
        "  # Armado del mensaje\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": TEXT_QA_PROMPT_TMPL.format(context_str=context_str, query_str=query_str)},\n",
        "  ]\n",
        "\n",
        "  # Formateador del prompt para usar con Zephyr\n",
        "  formatter = Formatter()\n",
        "  conversation = Conversation(model='zephyr', messages=messages)\n",
        "  final_prompt = formatter.render(conversation, add_assistant_prompt=add_generation_prompt)\n",
        "\n",
        "  # URL de la API de Hugging Face para la generación de texto\n",
        "  api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "  # Cabeceras para la solicitud\n",
        "  headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "  # Datos para enviar en la solicitud POST\n",
        "  # Sobre los parámetros: https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "  data = {\n",
        "      \"inputs\": final_prompt,\n",
        "      \"parameters\": {\n",
        "          \"max_new_tokens\": max_new_tokens,\n",
        "          \"temperature\": 0.5,\n",
        "          \"top_k\": 50,\n",
        "          \"top_p\": 0.95\n",
        "      }\n",
        "  }\n",
        "\n",
        "  # Realizamos la solicitud POST\n",
        "  response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "  # Extraer respuesta\n",
        "  respuesta = response.json()[0][\"generated_text\"][len(final_prompt):]\n",
        "\n",
        "  return respuesta"
      ],
      "metadata": {
        "id": "db3eyI0396Ln"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agente"
      ],
      "metadata": {
        "id": "hnWHcDAyosKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent() -> str:\n",
        "  # Configurar el LLM de Ollama\n",
        "  llm = Ollama(\n",
        "      model=\"llama3.2:latest\",\n",
        "      request_timeout=30.0, # Aumentado el timeout\n",
        "      temperature=0.1, # Reducida la temperatura para respuestas más deterministas\n",
        "      context_window=4096 # Aumentado el contexto\n",
        "  )\n",
        "  Settings.llm = llm\n",
        "\n",
        "  # Crear las herramientas para el agente\n",
        "  tools = [\n",
        "      FunctionTool.from_defaults(fn=vectorialDbRetriever, description=\"It looks for information regarding game rules and game reviews.\"),\n",
        "      FunctionTool.from_defaults(fn=tableDbRetriever, description=\"It looks for information regarding amount of players, playing time, ages and stats of the game.\"),\n",
        "      FunctionTool.from_defaults(fn=graphDbRetriever, description=\"It looks for information regarding credits, mechanisms and categories of the game.\"),\n",
        "  ]\n",
        "\n",
        "  # Crear el agente ReAct\n",
        "  agent = ReActAgent.from_tools(\n",
        "      tools,\n",
        "      llm=llm,\n",
        "      verbose=True,\n",
        "      chat_formatter=ReActChatFormatter(),\n",
        "      system_prompt=\"\"\"\"You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\n",
        "  You are an expert in board games and understand their descriptive characteristics. You must STRICTLY follow the following format:\n",
        "\n",
        "  Thought: Here, I explain what I need to do.\n",
        "  Action: nameOfTheTool\n",
        "  Action Input: \"user prompt\"\n",
        "\n",
        "  Correct examples:\n",
        "  - For vectorialDbRetriever: Action Input: \"user prompt\"\n",
        "  - For tableDbRetriever: Action Input: \"user prompt\"\n",
        "  - For graphDbRetriever: Action Input: \"user prompt\"\n",
        "\n",
        "  Observation: [Result of the tool]\n",
        "  ... [Repeat process if necessary]\n",
        "  Final Answer: The final answer combining all the information.\n",
        "\n",
        "  For each query:\n",
        "  1. Analyze which information do you need.\n",
        "  2. Use the available tools one by one with the right format.\n",
        "  3. Combine results in a final answer.\n",
        "\n",
        "  IMPORTANT:\n",
        "  - Always follow the exact format of the examples for each tool.\n",
        "  - The user prompt will only require using JUST ONE of ALL the tools to obtain the correct answer.\n",
        "  - If you got an answer from one of the functions, ASUME IT'S THE CORRECT ANSWER AND FINISH.\"\"\",\n",
        "      react_chat_history=False,\n",
        "      context=\"\"\"\n",
        "      You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\n",
        "      You are an expert in board games and understand their descriptive characteristics.\n",
        "      Remember the outputs of the tools to use them in your final answer.\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  # Pregunta del usuario:\n",
        "  prompt = str(input('How can I help you today? '))\n",
        "\n",
        "  llm_classifier_api_key = \"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\"\n",
        "  llm_class_client = InferenceClient(api_key=llm_classifier_api_key)\n",
        "  classification = llmClassifier(prompt, llm_class_client)\n",
        "\n",
        "  print(f\"\\nQuery: {prompt}\")\n",
        "\n",
        "  try:\n",
        "      if not prompt.strip():\n",
        "          return \"The query is empty.\"\n",
        "      response = agent.chat(prompt)\n",
        "  except Exception as e:\n",
        "      response = f\"Error processing the query: {str(e)}\"\n",
        "\n",
        "  print(f\"Answer: {response}\")\n",
        "  print(\"------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "oMJmjax2rtEK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1"
      ],
      "metadata": {
        "id": "SAf4UFtrhu-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "jqqNcNrhZoge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de bases de datos de Grafos"
      ],
      "metadata": {
        "id": "3tf185wdSRF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS DE GRAFOS           ##\n",
        "#########################################################\n",
        "try:\n",
        "  graph\n",
        "  print(\"✔ Graph database loaded.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Loading graph database.\")\n",
        "\n",
        "  # Conexión a Redis\n",
        "  redis_client = redis.Redis(host='localhost', port=6379)\n",
        "\n",
        "  # Crear una gráfica en RedisGraph\n",
        "  graph = Graph('credits', redis_client)\n",
        "\n",
        "  # Agregar nodos y aristas\n",
        "  createGraph()\n",
        "\n",
        "  # Guardar los datos en RedisGraph\n",
        "  graph.commit()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##           CREACIÓN BASE DE DATOS TABULAR            ##\n",
        "#########################################################\n",
        "# Generación de tablas\n",
        "try:\n",
        "  table_df\n",
        "  print(\"✔ Table data base loaded.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Loading table database.\")\n",
        "  # collection_stats_df, game_attributes_df, game_ranks_df, game_stats_df, parts_exchange_df, play_stats_df = createTables()\n",
        "  table_df = createTables()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS VECTORIAL           ##\n",
        "#########################################################\n",
        "# Generación de collection\n",
        "try:\n",
        "  collection\n",
        "  print(\"✔ Vectorial data base loaded.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Loading vectorial database.\")\n",
        "  # Cargar Universal Sentence Encoder\n",
        "  embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "\n",
        "  # Configuración inicial de ChromaDB\n",
        "  chromadb_client = chromadb.Client()\n",
        "\n",
        "  # Generación de collection\n",
        "  collection = createCollection()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##              CLASIFICACIÓN DE PROMPT                ##\n",
        "#########################################################\n",
        "\n",
        "# # Clasificación con modelo de Random Forest\n",
        "# # # Creación del modelo\n",
        "# # modelo_rf = randomForestClassifierCreator()\n",
        "# # # Descarga del modelo\n",
        "# # joblib.dump(modelo_rf, 'modelo_rf.pkl')\n",
        "# # File ID del modelo\n",
        "# model_file_id = '1t_P6sziWieZcUvNvEujveRfguCVH8LNE'\n",
        "# # Creación la URL de descarga\n",
        "# download_url = f'https://drive.google.com/uc?id={model_file_id}'\n",
        "# # Descarga del archivo\n",
        "# output = 'modelo_rf.pkl'\n",
        "# gdown.download(download_url, output, quiet=True)\n",
        "# # Crga del modelo\n",
        "# modelo_rf = joblib.load('modelo_rf.pkl')\n",
        "# print(rfClassifier(modelo_rf, query_str))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##                     INTERFACE                       ##\n",
        "#########################################################\n",
        "print('\\n')\n",
        "print(\"////////////////////////////////////////////////////////////////////// \\n\")\n",
        "\n",
        "# Prompt del usuario\n",
        "print(\"Welcome to the Rajas of the Ganges chatbot!\\n\")\n",
        "print(\"I can provide you support regarding the game rules, credits, stats or even reviews!\\n\")\n",
        "print(\"How can I help you today?\")\n",
        "prompt = str(input('Question: '))\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"---------------------------------------------------------------------- \\n\")\n",
        "\n",
        "# Clasificación\n",
        "llm_classifier_api_key = \"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\"\n",
        "llm_class_client = InferenceClient(api_key=llm_classifier_api_key)\n",
        "classification = llmClassifier(prompt)\n",
        "\n",
        "# Retrieving\n",
        "context = retriever(prompt)\n",
        "\n",
        "# Consulta a LLM\n",
        "RAG_api_key = \"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\"\n",
        "results = RAG(prompt, context, RAG_api_key)\n",
        "\n",
        "print(\"Here's what I found!\\n\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "GolyC2bUEtDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded63597-c9bc-4a81-d1c5-ef371b596ea9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Graph database loaded.\n",
            "✔ Table data base loaded.\n",
            "✔ Vectorial data base loaded.\n",
            "\n",
            "\n",
            "////////////////////////////////////////////////////////////////////// \n",
            "\n",
            "Welcome to the Rajas of the Ganges chatbot!\n",
            "\n",
            "I can provide you support regarding the game rules, credits, stats or even reviews!\n",
            "\n",
            "How can I help you today?\n",
            "Question: What's the maximum playtime?\n",
            "\n",
            "\n",
            "---------------------------------------------------------------------- \n",
            "\n",
            "Here's what I found!\n",
            "\n",
            "The maximum playing time, as provided in the contextual information, is 75 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 2"
      ],
      "metadata": {
        "id": "e_gAyUImh1_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "BJKQ1Mnfh3tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS DE GRAFOS           ##\n",
        "#########################################################\n",
        "try:\n",
        "  graph\n",
        "  print(\"✔ Base de datos de grafos cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos de grafos.\")\n",
        "\n",
        "  # Conexión a Redis\n",
        "  redis_client = redis.Redis(host='localhost', port=6379)\n",
        "\n",
        "  # Crear una gráfica en RedisGraph\n",
        "  graph = Graph('credits', redis_client)\n",
        "\n",
        "  # Agregar nodos y aristas\n",
        "  createGraph(graph)\n",
        "\n",
        "  # Guardar los datos en RedisGraph\n",
        "  graph.commit()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##           CREACIÓN BASE DE DATOS TABULAR            ##\n",
        "#########################################################\n",
        "# Generación de tablas\n",
        "try:\n",
        "  table_df\n",
        "  print(\"✔ Base de datos tabular cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos tabular.\")\n",
        "  # collection_stats_df, game_attributes_df, game_ranks_df, game_stats_df, parts_exchange_df, play_stats_df = createTables()\n",
        "  table_df = createTables()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS VECTORIAL           ##\n",
        "#########################################################\n",
        "# Generación de collection\n",
        "try:\n",
        "  collection\n",
        "  print(\"✔ Base de datos vectorial cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos vectorial.\")\n",
        "  # Cargar Universal Sentence Encoder\n",
        "  embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "\n",
        "  # Configuración inicial de ChromaDB\n",
        "  client = chromadb.Client()\n",
        "\n",
        "  # Generación de collection\n",
        "  collection = createCollection(client)\n",
        "\n",
        "#########################################################\n",
        "##                     INTERFACE                       ##\n",
        "#########################################################\n",
        "print(\"\\n\")\n",
        "print(\"////////////////////////////////////////////////////////////////////// \\n\")\n",
        "print(\"Welcome to the Rajas of the Ganges chatbot!\\n\")\n",
        "print(\"I can provide you support regarding the game rules, credits, stats or even reviews!\\n\")\n",
        "\n",
        "# Ejecución del agente\n",
        "agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nQ3Ji_oh6pT",
        "outputId": "df2bd61d-e8f1-4cf1-9ec0-2d6d95b19ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Base de datos de grafos cargada.\n",
            "✔ Base de datos tabular cargada.\n",
            "✔ Base de datos vectorial cargada.\n",
            "\n",
            "Please, enter your question: What are the rules of the game?\n",
            "\n",
            "Query: What are the rules of the game?\n",
            "> Running step 4a1ab29e-4f93-4c35-9efb-5c48a67466a5. Step input: What are the rules of the game?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: vectorialDbRetriever\n",
            "Action Input: {'query': AttributedDict([('title', 'Query'), ('type', 'string')]), 'properties': AttributedDict([('query', AttributedDict([('title', 'Game Rules'), ('type', 'string')]))])}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: vectorialDbRetriever() got an unexpected keyword argument 'properties'\n",
            "\u001b[0m> Running step 76d472a8-7df9-48ee-95a2-ac79dbf9d815. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The tool vectorialDbRetriever does not support the 'properties' keyword argument. I need to adjust my input.\n",
            "Action: vectorialDbRetriever\n",
            "Action Input: {'query': 'Game Rules'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: [[\"and it actually does it makes this game feel like a race it really really feel like you have to optimize your actions like you have to really try to do the best every turn and you would also do that in all the games but you have to manage I can't just use money all the time because then I I'm gonna have to get so many points and you're not gonna get so many points so we see in this game it's easy to get some money and a lot of points or to get to balance it out or to try to get your heavy money strategy and just also get some points and there's feeling the feels like there's different ways going on how you play this game and I have felt that the game is different enough every time but I'm not sure like how much replayability it is it is all still fun yeah I think it's gonna be fun for like 10 12 plays and then it doesn't really matter because most gamers especially like me we don't play games that many times if you want to gain if you can play 50 times I'm not sure how goodies would\", \"it out because it is different enough from a lot of other work replacement or you grow games but this game really looks like the euro game that it is but the art is good it's it's not mind-blowing but it is it's good art I enjoy it it's clear the economy akin ography is it's clear everything is it's easy to understand everything is clearly laid out the rules are simple everything is is made so that this game will be easy to play also for players who don't enjoy heavier games who doesn't play that many games it's gonna be an easy game to get into and he's a game to understand so game play as I said I really enjoy it one thing I really like about this game is I was really looking forward to checking out and seeing how would this work with the two things going and them the points and the money going to cross each other how would this feel would it feel like something else than just getting most points or getting to the end of the point track and it actually does it makes this game feel\", \"point of this game because most games are about getting the most points and while that is also Impulse important in this game that's not actually the point of the game there isn't six rounds and then the person with the most points is the winner the point of this game is to be the player who gets the point marker here to cross with their money mark you're here so you had to balance your income the money you get the moaning you use while also getting points after that has happened everyone else gonna get one more turn placing one more action one more worker and then if more players have made it happen that I haven't reached each other then the person who I have have them Forest apart from each other is the winner of the game but how do you play this game you play it by placing workers and doing actions as I of course already said but that's isn't really helpful so let's go through the different aspects of this game the game is a lot revolving about around these dice getting the dice\", \"[Applause] [Music] hi there welcome to our channel i'm vic i'm nick and we are mv board gaming and today we're looking at raja's of the ganges this is a two to four player game it's designed by inka and marcus brand and it is published by hood hutch and r r games nick is going to go through how you play and then we're going to give our review all right welcome to the table we're set up here for a two-player game and we're just going to focus on one player though make it easier here i don't want to fit two of those boards over here so the green players who are going to focus on we're all set up except for the very first part where we roll the dice and see who the first player is the lower player is going to get the lower die roll is going to get the um the first player and the other player is going to get the extra coin so let's just say green player had the lower die roll so other player is getting the extra coin and green player is the first player so this game you're going to be\", \"the first player so this game you're going to be placing out workers you start with three and you can earn other workers in different spots on the board here once you advance the whole point of the game is we have a money track here along this perimeter and then a fame track along this perimeter going that way and when those two collide then that will trigger the end of the game um the person that anyone in between the start player and the person who triggered the end game is getting a turn and get one more turn but other than that the game's over so you're getting coins or you're getting fame or both so there's various ways of doing that on your turn you're going to take it you're going to go ahead and assign a worker to somewhere you want to go you're going to pay the cost associated with that action and then you're going to carry out that action that's all you're doing this whole game even cleanup is very easy so let's say this was my role i have some palace actions i can do over\", \"hello everyone this is Johannes and you are watching board gaming ramblings and today we are taking a look at Rogers of the Ganges from Inca and Marcus brand a euro game that came out at essence pealed 2017 a game I was really looking forward to playing and now I have played it and that's why I'm making this video to tell you about this game let's see how you play Rogers of the Yankees Rogers of the Ganges is a worker placement game so you're gonna take your workers you're gonna place them and do some actions like you're doing most worker placement games but this game also uses dice and some of the actions you take with your worker we'll need to discard dice some actions need dive so specific color some actions need dice with a specific pip number and some actions just want to do this card dice a die and get rid of it and some actions will give you more dice so you can use them for your actions so let's just for first talk about the point of this game because most games are about\", \"otherwise keep playing i like that as well so i i really like this game i like it a lot um the board is a little busy but you know it can be intimidating to new players but it's really not that difficult the game i'll say so despite the look of the game i don't think you're going to struggle too much with it um even you were able to learn it in a cafe in one sitting really yeah yeah once you realize okay at first maybe because you think it's gonna be a little struggle because you're saying what every spot does this is the quarry you do this there but once you get clicking i mean there's only a few things you're gonna be doing it's pretty intuitive once you get going yeah um so i feel like it's not an entry level i'm not saying that but i am saying it's uh just a step above that i would say um and which is a good thing because there's still plenty of decisions to be made it's not a light game in that um you know you're just going through the motions here you actually have quite a bit\", \"stands out immediately is this is going to be a dice placing game you see there are dice the Shiva character is manipulating dices different colors because that's exactly what ganas is you have the board where you will be placing Dice and workers I'll show you a little bit how it works but I would like to give you some flavor here before we go in so in the 16th century Indian the powerful Empire of the great Mongols Rises between the Indus and the ganes rivers taking on the role of the rajas and runnies runnies the female rajas the country's influen Nobles players race against each other in support of the Empire by developing their states into wealthy and magnificent provinces players must use their dice wisely and carefully plot where to place their workers while never underestimating the benefits of Good Karma success will bring them riches great riches and fame in their quests to become legendary rulers that's a good intro yep so let's see what is inside the Raja of gz so I have I\", \"the motions here you actually have quite a bit of decisions so my score for this one is an 8-7 what about you vic so a lot of the things you said i like the colors of the game i love rolling dice so i always have fun even though i don't always have the best luck at least they have a karma mechanism to adjust that dice and make changes to the dice i like the cleverness of having to think on the river track when you're going to stop you don't want to just blow through the whole river necessarily which i had done one time went for that sixth spot moved in the river and then looked back at all the stuff i didn't get and i want to do that um but i was you know thinking i want to get to the end and start scoring on karma but then you have to think about having those karma intervals like are having that amount of the value of karma to then score the uh the multipliers there that they have at the end and in the expansion which we will talk about it later but i think in the base game there's\", \"thank you hello and welcome to meeple Mentor reviews I'm Jared and with me is Holly today we're reviewing rajas of the ganjis this game is from Hutch and r r games and it's for two to four players plays in about an hour to hour and a half recommends ages 12 and up and this is a worker placement game set in the you know the the India River Ganges uh setting and you are but all trying to gain wealth and fame uh as a Raja you know and you know exerting your influence over the region now you have uh three workers I think you start with three you can get a fourth or did I start with two and get a third I think it'd start with three and get a fourth you start with three workers you can gain up to two more um but there's three ways that you can gain workers um there's this is a game with two tracks there's an income track around one side and then a Fame track that goes starts on the other side and so they run parallel to each other in opposite directions and allows you to basically have this\"]]\n",
            "\u001b[0m> Running step 62b30ab1-724e-4755-8189-284686a4d69c. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The tool vectorialDbRetriever did not provide useful information. I need to try another tool.\n",
            "Action: tableDbRetriever\n",
            "Action Input: {'query': 'Game Rules of Rogers of the Ganges'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Game: Rajas of the Ganges\n",
            "\u001b[0m> Running step f9182bab-8542-41e7-9519-777ee95a1c48. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The output from the tableDbRetriever tool contained information about the game \"Rajas of the Ganges\". I need to analyze this output to understand the rules of the game.\n",
            "Action: Review\n",
            "Action Input: {'query': 'Game Rules of Rogers of the Ganges', 'result': [AttributedDict([('title', 'Objective'), ('text', 'The objective of the game is to be the player who gets the point marker on the Fame track by accumulating points through their workers and dice rolls.')]), AttributedDict([('title', 'Gameplay'), ('text', 'Players take turns placing one worker at a time, with each worker having a specific action associated with it. The players also roll dice to determine which actions they can take.')]), AttributedDict([('title', 'Worker Placement'), ('text', 'Workers are placed on the board to perform actions such as collecting income, gaining fame, or taking other actions. Each worker has a specific action associated with it.')]), AttributedDict([('title', 'Dice Rolling'), ('text', 'Dice rolls determine which actions can be taken and how many points can be gained. The dice also have different pip numbers that correspond to different actions.')])]}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: No such tool named `Review`.\n",
            "\u001b[0m> Running step fdbc46f0-e336-4a92-aac8-6ec403b3e50f. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The tool Review does not exist. I need to use a different tool or adjust my approach.\n",
            "Action: tableDbRetriever\n",
            "Action Input: {'query': 'Gameplay Mechanics of Rogers of the Ganges'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Game: Rajas of the Ganges\n",
            "\u001b[0mAnswer: Error processing the query: Reached max iterations.\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}