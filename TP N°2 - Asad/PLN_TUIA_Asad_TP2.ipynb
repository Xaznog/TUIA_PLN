{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "t5u1osKts6Ef",
        "i6WFbNiVvj2_",
        "NQ7AQaF6vnI1",
        "jekVJmQhv8xd",
        "ifQkA3t3v_V-",
        "fZmkhuNOo8uf",
        "Am4bHggtpmMD",
        "hkzICjMlpu6g",
        "lFJrYJ2KTnTO",
        "ahBg486gAfbQ",
        "VKOdtw9S94_G",
        "hnWHcDAyosKf",
        "SAf4UFtrhu-M",
        "jqqNcNrhZoge",
        "e_gAyUImh1_5",
        "BJKQ1Mnfh3tX"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Procesamiento del Lenguaje Natural - Trabajo Práctico N°2 - 2024\n",
        "## Rajas de Ganges Chatbot\n",
        "\n",
        "Desarrollado por:\n",
        "- Asad, Gonzalo (A-4595/1)"
      ],
      "metadata": {
        "id": "MtoTIADFs00z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "RiIIka9gs3W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación del Entorno"
      ],
      "metadata": {
        "id": "t5u1osKts6Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt install -y chromium-chromedriver\n",
        "# !pip install selenium\n",
        "# import os\n",
        "# os.environ[\"PATH\"] += \":/usr/bin/chromedriver\""
      ],
      "metadata": {
        "id": "ELYCCnCp6_in"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de librerías"
      ],
      "metadata": {
        "id": "HvuaR9EAs9OC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5ZIj5HVPdi3N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82a2c0d-5a5d-4ae1-d92e-965d20652191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Collecting redis\n",
            "  Downloading redis-5.2.1-py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting redisgraph\n",
            "  Downloading redisgraph-2.4.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /usr/local/lib/python3.10/dist-packages (from redis) (4.0.3)\n",
            "Collecting hiredis<3.0.0,>=2.0.0 (from redisgraph)\n",
            "  Downloading hiredis-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting prettytable<3.0.0,>=2.1.0 (from redisgraph)\n",
            "  Downloading prettytable-2.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting redis\n",
            "  Downloading redis-3.5.3-py2.py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable<3.0.0,>=2.1.0->redisgraph) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading redisgraph-2.4.4-py3-none-any.whl (14 kB)\n",
            "Downloading redis-3.5.3-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.1/72.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hiredis-2.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.5/166.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prettytable-2.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: redis, prettytable, hiredis, redisgraph\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.12.0\n",
            "    Uninstalling prettytable-3.12.0:\n",
            "      Successfully uninstalled prettytable-3.12.0\n",
            "Successfully installed hiredis-2.4.0 prettytable-2.5.0 redis-3.5.3 redisgraph-2.4.4\n",
            "--2024-12-14 20:59:21--  http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
            "Resolving nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)... 91.189.91.82, 185.125.190.82, 185.125.190.83, ...\n",
            "Connecting to nz2.archive.ubuntu.com (nz2.archive.ubuntu.com)|91.189.91.82|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1318204 (1.3M) [application/vnd.debian.binary-package]\n",
            "Saving to: ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’\n",
            "\n",
            "libssl1.1_1.1.1f-1u 100%[===================>]   1.26M  7.67MB/s    in 0.2s    \n",
            "\n",
            "2024-12-14 20:59:22 (7.67 MB/s) - ‘libssl1.1_1.1.1f-1ubuntu2_amd64.deb’ saved [1318204/1318204]\n",
            "\n",
            "Selecting previously unselected package libssl1.1:amd64.\n",
            "(Reading database ... 123633 files and directories currently installed.)\n",
            "Preparing to unpack libssl1.1_1.1.1f-1ubuntu2_amd64.deb ...\n",
            "Unpacking libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "Setting up libssl1.1:amd64 (1.1.1f-1ubuntu2) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "./\n",
            "./redis-stack-server-6.2.6-v7/\n",
            "./redis-stack-server-6.2.6-v7/bin/\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-benchmark\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-cli\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-sentinel\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-stack-server\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-check-rdb\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-check-aof\n",
            "./redis-stack-server-6.2.6-v7/bin/redis-server\n",
            "./redis-stack-server-6.2.6-v7/share/\n",
            "./redis-stack-server-6.2.6-v7/share/RSAL_LICENSE\n",
            "./redis-stack-server-6.2.6-v7/share/APACHE_LICENSE\n",
            "./redis-stack-server-6.2.6-v7/lib/\n",
            "./redis-stack-server-6.2.6-v7/lib/redisgraph.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redistimeseries.so\n",
            "./redis-stack-server-6.2.6-v7/lib/rejson.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redisbloom.so\n",
            "./redis-stack-server-6.2.6-v7/lib/redisearch.so\n",
            "./redis-stack-server-6.2.6-v7/etc/\n",
            "./redis-stack-server-6.2.6-v7/etc/README\n",
            "./redis-stack-server-6.2.6-v7/etc/redis-stack.conf\n",
            "./redis-stack-server-6.2.6-v7/etc/redis-stack-service.conf\n",
            "Starting redis-stack-server, database path ./redis-stack-server-6.2.6-v7/var/db/redis-stack\n",
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting tensorflow<2.19,>=2.18.0 (from tensorflow_text)\n",
            "  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow<2.19,>=2.18.0->tensorflow_text)\n",
            "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.1.2)\n",
            "Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow, tensorflow_text\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0 tensorflow_text-2.18.0\n",
            "Requirement already satisfied: tensorflow_hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (1.26.4)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (4.25.5)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow_hub) (2.17.0)\n",
            "Collecting tensorflow<2.18,>=2.17 (from tf-keras>=2.14.1->tensorflow_hub)\n",
            "  Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (24.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (1.68.1)\n",
            "Collecting tensorboard<2.18,>=2.17 (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub)\n",
            "  Downloading tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17->tf-keras>=2.14.1->tensorflow_hub) (0.1.2)\n",
            "Downloading tensorflow-2.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.17.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.0 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.17.1 tensorflow-2.17.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow)\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Using cached tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n",
            "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "Installing collected packages: tensorboard, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.1\n",
            "    Uninstalling tensorboard-2.17.1:\n",
            "      Successfully uninstalled tensorboard-2.17.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.1\n",
            "    Uninstalling tensorflow-2.17.1:\n",
            "      Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorboard-2.18.0 tensorflow-2.18.0\n",
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: tensorflow<2.19,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.5.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.19,>=2.18.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18.0->tensorflow_text) (0.1.2)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.10.3)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.33.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.28.2)\n",
            "Requirement already satisfied: tokenizers<=0.20.3,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.20.3)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.15)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.26.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.33.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (442 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.1-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=e2a146cd37fccfb798d405fbe6251bfceede075a008e551c8778a435624286ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, python-dotenv, pyproject_hooks, protobuf, overrides, opentelemetry-util-http, mmh3, humanfriendly, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, starlette, posthog, opentelemetry-proto, opentelemetry-api, coloredlogs, build, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.5\n",
            "    Uninstalling protobuf-4.25.5:\n",
            "      Successfully uninstalled protobuf-4.25.5\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.28.2\n",
            "    Uninstalling opentelemetry-api-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-api-1.28.2\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.49b2\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.49b2:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.49b2\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.28.2\n",
            "    Uninstalling opentelemetry-sdk-1.28.2:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.28.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.1 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 fastapi-0.115.6 httptools-0.6.4 humanfriendly-10.0 kubernetes-31.0.0 mmh3-5.0.1 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 posthog-3.7.4 protobuf-5.29.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.41.3 uvicorn-0.33.0 uvloop-0.21.0 watchfiles-1.0.3\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.11)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.24 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.24)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.24->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.24->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.3-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.8.30)\n",
            "Downloading youtube_transcript_api-0.6.3-py3-none-any.whl (622 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.3/622.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.3\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.6)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.46.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.8.30)\n",
            "Collecting python-decouple==3.8\n",
            "  Downloading python_decouple-3.8-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting llm-templates\n",
            "  Downloading llm_templates-0.1.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: requests>=2.28.0 in /usr/local/lib/python3.10/dist-packages (from llm-templates) (2.32.3)\n",
            "Requirement already satisfied: Jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llm-templates) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.11.3->llm-templates) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28.0->llm-templates) (2024.8.30)\n",
            "Downloading python_decouple-3.8-py3-none-any.whl (9.9 kB)\n",
            "Downloading llm_templates-0.1.12-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: python-decouple, llm-templates\n",
            "Successfully installed llm-templates-0.1.12 python-decouple-3.8\n"
          ]
        }
      ],
      "source": [
        "# Para base de datos de grafos\n",
        "!pip install networkx matplotlib redis redisgraph\n",
        "!wget http://nz2.archive.ubuntu.com/ubuntu/pool/main/o/openssl/libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!sudo dpkg -i libssl1.1_1.1.1f-1ubuntu2_amd64.deb\n",
        "!curl -fsSL https://packages.redis.io/redis-stack/redis-stack-server-6.2.6-v7.focal.x86_64.tar.gz -o redis-stack-server.tar.gz\n",
        "!tar -xvf redis-stack-server.tar.gz\n",
        "!./redis-stack-server-6.2.6-v7/bin/redis-stack-server --daemonize yes\n",
        "\n",
        "# Para base de datos vectorial\n",
        "!pip install tensorflow_text\n",
        "!pip install tensorflow_hub\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade tensorflow_text\n",
        "!pip install chromadb\n",
        "!pip install langchain\n",
        "!pip install youtube-transcript-api\n",
        "!pip install PyPDF2\n",
        "\n",
        "# Para web-scrapping\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "# Para uso general\n",
        "!pip install gdown\n",
        "\n",
        "# Para clasificación\n",
        "!pip install sentence_transformers\n",
        "\n",
        "# Para RAG\n",
        "!pip install python-decouple==3.8 llm-templates\n",
        "\n",
        "# Para el Agent\n",
        "# Descarga de Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "# Iniciamos Ollama en background\n",
        "!rm -f ollama_start.sh\n",
        "!echo '#!/bin/bash' > ollama_start.sh\n",
        "!echo 'ollama serve' >> ollama_start.sh\n",
        "# Make the script executable\n",
        "!chmod +x ollama_start.sh\n",
        "!nohup ./ollama_start.sh &\n",
        "!ollama pull llama3.2 > ollama.log\n",
        "!pip install llama-index-llms-ollama llama-index\n",
        "!nohup litellm --model ollama/llama3.2:latest --port 8000 > litellm.log 2>&1 &"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carga de librerías"
      ],
      "metadata": {
        "id": "W0-ywdcgs-8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para uso general\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Para base de datos de grafos\n",
        "import redis\n",
        "from redisgraph import Graph, Node, Edge\n",
        "import networkx as nx\n",
        "\n",
        "# Para base de datos vectorial\n",
        "import tensorflow_text\n",
        "import tensorflow_hub as hub\n",
        "import chromadb\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "import PyPDF2\n",
        "\n",
        "# Para web-scrapping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "\n",
        "# Para clasificación\n",
        "from huggingface_hub import InferenceClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Para re-ranking\n",
        "from sentence_transformers import CrossEncoder\n",
        "\n",
        "# Para RAG\n",
        "import requests\n",
        "from decouple import config\n",
        "from google.colab import userdata\n",
        "from llm_templates import Formatter, Conversation\n",
        "\n",
        "# Para el agente\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core import Settings\n",
        "from llama_index.llms.ollama import Ollama\n",
        "from llama_index.core.agent import ReActAgent\n",
        "from llama_index.core.tools import FunctionTool\n",
        "from llama_index.core.agent.react.formatter import ReActChatFormatter"
      ],
      "metadata": {
        "id": "c8RqDlfNtBUc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generación de Bases de Datos"
      ],
      "metadata": {
        "id": "i6WFbNiVvj2_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuentes de las bases de datos:\n",
        "*   [Board Game Geek](https://boardgamegeek.com/boardgame/220877/rajas-of-the-ganges)\n",
        "*   [Meesut Meeple](https://misutmeeple.com/2018/01/resena-rajas-of-the-ganges/)\n",
        "\n"
      ],
      "metadata": {
        "id": "YgMAJwFCxMce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Documentos de texto\n",
        "\n",
        "Contienen reglas del juego en sus diferentes modos y reseñas."
      ],
      "metadata": {
        "id": "NQ7AQaF6vnI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición de clases y funciones."
      ],
      "metadata": {
        "id": "Xdn3xFEs4IdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getTranscripts(video_ids: list) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con transcripciones de videos de reseñas.\n",
        "\n",
        "  Parámetros:\n",
        "    - video_ids: Lista de identificadores de videos.\n",
        "\n",
        "  Retorno:\n",
        "    - transcript_list: Lista con transcripciones de los videos.\n",
        "  '''\n",
        "  # Incializar lista\n",
        "  transcript_list = []\n",
        "\n",
        "  # Extraer transcripciones\n",
        "  for video_id in video_ids:\n",
        "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
        "    transcript_list.append(transcript)\n",
        "\n",
        "  # Inicializar lista\n",
        "  final_transcripts = []\n",
        "\n",
        "  # Extraer cadenas de texto\n",
        "  for transcript in transcript_list:\n",
        "    text = \"\"\n",
        "    for line in transcript:\n",
        "      text += line['text'] + \" \"\n",
        "\n",
        "    # Limpieza de texto\n",
        "    text.replace('\\n', '')\n",
        "    text.replace('  ', ' ')\n",
        "\n",
        "    # Transcripciones finales\n",
        "    final_transcripts.append(text)\n",
        "\n",
        "  return final_transcripts\n",
        "\n",
        "\n",
        "def getPDFs(file_ids: list) -> list[str]:\n",
        "  '''\n",
        "  Crea una lista con documentos de texto.\n",
        "\n",
        "  Parámetros:\n",
        "    - file_ids: Lista de identificadores de archivos.\n",
        "\n",
        "  Retorno:\n",
        "    - file_transcripts: Lista con documentos de texto.\n",
        "  '''\n",
        "\n",
        "  pdfs_transcripts = []\n",
        "  i = 0\n",
        "\n",
        "  for file_id in file_ids:\n",
        "\n",
        "    # Creación la URL de descarga\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    # Descarga del archivo\n",
        "    output = f'file{i}.pdf'\n",
        "    gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "    # Abre el archivo en modo binario de lectura ('rb')\n",
        "    with open(f'file{i}.pdf', 'rb') as file:\n",
        "      # Crea un objeto PdfFileReader\n",
        "      reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "      # Inicializa una cadena vacía para almacenar el texto\n",
        "      text = ''\n",
        "\n",
        "      # Itera sobre todas las páginas del PDF\n",
        "      for i in range(len(reader.pages)):\n",
        "        # Obtiene la página\n",
        "        pagina = reader.pages[i]\n",
        "\n",
        "        # Extrae el texto de la página y lo añade a la cadena de texto\n",
        "        text += pagina.extract_text()\n",
        "\n",
        "      # Limpieza de texto\n",
        "      text.replace('\\n', '')\n",
        "      text.replace('  ', ' ')\n",
        "\n",
        "    pdfs_transcripts.append(text)\n",
        "    i += 1\n",
        "\n",
        "  return pdfs_transcripts\n",
        "\n",
        "\n",
        "def splitter(texts: list, metadatas: list, ids_names: str) -> tuple[list]:\n",
        "  '''\n",
        "  Crea splits de los textos recibidos, además de crear sus metadatas e IDs.\n",
        "\n",
        "  Parámetros:\n",
        "    - texts: lista con elementos a splittear.\n",
        "    - metadatas: lista con los nombres a usar como metadata.\n",
        "    - ids_names: lista con los nombres a usar en los IDs.\n",
        "\n",
        "  Retorna:\n",
        "    Listas con los splits de los textos, sus metadatas y sus IDs.\n",
        "  '''\n",
        "\n",
        "  # Creación de splitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "\n",
        "  # Definición de listas de salida\n",
        "  splitted_texts = []\n",
        "  splitted_metadatas = []\n",
        "  splitted_ids = []\n",
        "\n",
        "  # Creación de splits de texto y metadatas\n",
        "  for i in range(len(texts)):\n",
        "    splitted_text = text_splitter.split_text(texts[i])\n",
        "    splitted_texts += splitted_text\n",
        "    for j in range(len(splitted_text)):\n",
        "      splitted_metadatas.append(metadatas[i] + '_' + str(j))\n",
        "      # splitted_metadatas.append(metadatas[i])\n",
        "\n",
        "  # Creación de IDs\n",
        "  for k in range(len(splitted_texts)):\n",
        "    splitted_ids.append(ids_names + '_' + str(k))\n",
        "\n",
        "  return splitted_texts, splitted_metadatas, splitted_ids\n",
        "\n",
        "\n",
        "def createCollection(client: chromadb.api.client.Client) -> chromadb.api.models.Collection.Collection:\n",
        "  '''\n",
        "  Crea una colección de documentos con sus respectivos embeddings, metadata y IDs.\n",
        "\n",
        "  Parámetros:\n",
        "    - client: cliente de ChromaDB.\n",
        "\n",
        "  Retorno:\n",
        "    - collection: colección de documentos.\n",
        "  '''\n",
        "\n",
        "  # Carga de transcripciones de videos de reseñas\n",
        "  video_ids = [\"EpG7XML-Vm8\", \"mSRN0DShdFM\", \"yd5DEeEv59U\", \"18__Sw5Jq54\"]\n",
        "  video_transcripts = getTranscripts(video_ids)\n",
        "\n",
        "  # Carga de PDFs de manuales\n",
        "  normal_rules_file_id = '1xKZv2r58VlZ2JzFyYhKtBbQc0mVBWo-1'\n",
        "  solo_rules_file_id = '1NLZE1hS9TmU-O7lTLgt0kVv9AohZ64io'\n",
        "  solo_ai_rules_file_id = '1p2geJvRyIYCdZSGmP403qqHApyibbvnH'\n",
        "  automa_rules_file_id = '1oyYWB_O2mQcUc8AHHflySVMqAWpjvo48'\n",
        "  pdf_ids = [normal_rules_file_id, solo_rules_file_id, solo_ai_rules_file_id, automa_rules_file_id]\n",
        "  pdfs_transcripts = getPDFs(pdf_ids)\n",
        "\n",
        "  # Definición de fuentes (metadata)\n",
        "  video_metadatas = [\"review_1\", \"review_2\", \"review_3\", \"review_4\"]\n",
        "  # video_metadatas = [\"review\", \"review\", \"review\", \"review\"]\n",
        "  pdf_metadatas = [\"normal_rules\", \"solo_rules\", \"ai_rules\", \"automa_rules\"]\n",
        "\n",
        "  # Definición de IDs para collections\n",
        "  # video_coll_ids = [f\"vid{i}\" for i in range(1, len(video_transcripts)+1)]\n",
        "  # pdf_coll_ids = [f\"pdf{i}\" for i in range(1, len(pdfs_transcripts)+1)]\n",
        "\n",
        "  # Splits de transcripciones\n",
        "  video_transcripts, video_metadatas, video_coll_ids = splitter(video_transcripts, video_metadatas, \"vid\")\n",
        "  pdfs_transcripts, pdf_metadatas, pdf_coll_ids = splitter(pdfs_transcripts, pdf_metadatas, \"pdf\")\n",
        "\n",
        "  # Calcular embeddings para los documentos\n",
        "  collection = client.get_or_create_collection(\"all-my-documents\")\n",
        "\n",
        "  texts = video_transcripts + pdfs_transcripts\n",
        "  metadatas = video_metadatas + pdf_metadatas\n",
        "  texts_ids = video_coll_ids + pdf_coll_ids\n",
        "\n",
        "  embeddings = embed(texts).numpy().tolist()  # Convertir a lista para que sea serializable\n",
        "\n",
        "  collection.add(\n",
        "      documents=texts,\n",
        "      metadatas=[{\"source\": metadata} for metadata in metadatas],\n",
        "      ids=texts_ids,\n",
        "      embeddings=embeddings\n",
        "  )\n",
        "\n",
        "  return collection"
      ],
      "metadata": {
        "id": "xIxb7AKY0HUW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tablas\n",
        "\n",
        "Contienen atributos del juego."
      ],
      "metadata": {
        "id": "jekVJmQhv8xd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de clases y funciones"
      ],
      "metadata": {
        "id": "pnwo7HfDhOtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getDataFrame(file_id: str, encoding: str ='utf-8', delimiter: str = ',', show_head: bool = False) -> pd.DataFrame:\n",
        "  '''\n",
        "  Crea un DataFrame a partir de un archivo CSV alojado en Google Drive.\n",
        "\n",
        "  Parámetros:\n",
        "    - file_id: ID del archivo (codificado según Google Drive).\n",
        "    - encoding: Codificación de caracteres que se utilizará para leer el archivo.\n",
        "    - delimiter: Especifica el carácter que separa los valores en el archivo.\n",
        "    - show_head: Si es True, muestra las primeras filas del DataFrame.\n",
        "\n",
        "  Retorno:\n",
        "    - DataFrame: El DataFrame creado a partir del archivo.\n",
        "  '''\n",
        "\n",
        "  # Creación la URL de descarga\n",
        "  download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "  # Descarga del archivo\n",
        "  output = 'file'\n",
        "  gdown.download(download_url, output, quiet=True)\n",
        "\n",
        "  df= pd.read_csv('file', encoding='utf-8', delimiter=',')\n",
        "\n",
        "  if show_head:\n",
        "    df.head()\n",
        "\n",
        "  return df\n",
        "\n",
        "def createTables() -> pd.DataFrame:\n",
        "  '''\n",
        "  Genera DataFrames con datos estadísticos y atributos del juego.\n",
        "\n",
        "  Retorno:\n",
        "    - Múltiples DataFrames con los datos de los juegos.\n",
        "  '''\n",
        "\n",
        "  # File IDs\n",
        "  # collection_stats_file_id = '153_EsmrYPTsnps_Y7t77FCbOUimN3YY6'\n",
        "  # game_attributes_file_id = '1f-_itA2DiGfUBYHrYKyW_Gakv1GN9PW5'\n",
        "  # game_ranks_file_id = '1EOJzdOAIQTrfeoL2Ryv0ZJe1IgQuH6GC'\n",
        "  # game_stats_file_id = '1ShOsowQX4BjsE2q7SdFcvbNXMoVvivKJ'\n",
        "  # parts_exchange_file_id = '1Qy6RV1a5Tv2wQIUCSPnDMJ2stqMRRBdN'\n",
        "  # play_stats_file_id = '1cjAHtDmMkGOL6OXv00GGMKSOAot6yKP1'\n",
        "  table_file_id = '1TBVbwYMja-DPXW8QrL8dwdwU586iEOO7'\n",
        "\n",
        "  # Descarga de DataFrames\n",
        "  # collection_stats = getDataFrame(collection_stats_file_id)\n",
        "  # game_attributes = getDataFrame(game_attributes_file_id)\n",
        "  # game_ranks = getDataFrame(game_ranks_file_id)\n",
        "  # game_stats = getDataFrame(game_stats_file_id)\n",
        "  # parts_exchange = getDataFrame(parts_exchange_file_id)\n",
        "  # play_stats = getDataFrame(play_stats_file_id)\n",
        "  table = getDataFrame(table_file_id)\n",
        "\n",
        "  return table # collection_stats, game_attributes, game_ranks, game_stats, parts_exchange, play_stats"
      ],
      "metadata": {
        "id": "2PUSIuPJv-9X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grafos\n",
        "\n",
        "Contienen información de los creadores y artistas."
      ],
      "metadata": {
        "id": "ifQkA3t3v_V-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de clases y funciones."
      ],
      "metadata": {
        "id": "B2U44GKROLxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RequestError(Exception):\n",
        "    '''Excepción personalizada para errores en las solicitudes.'''\n",
        "    pass\n",
        "\n",
        "def getValidation(url: str, retries: int = 3, backoff_factor: float = 0.5) -> str:\n",
        "  '''Valida la petición GET a la URL y en caso exitoso retorna la respuesta.\n",
        "     Implementa reintentos con backoff exponencial para manejar errores de timeout.'''\n",
        "  for i in range(retries):\n",
        "      try:\n",
        "          # Generación de la solicitud GET a la URL\n",
        "          response = requests.get(url)\n",
        "          # Verificación de si la respuesta tiene un código de error\n",
        "          response.raise_for_status()\n",
        "      except requests.exceptions.HTTPError as http_err:\n",
        "          if response.status_code == 504 and i < retries - 1:\n",
        "              time.sleep(backoff_factor * (2 ** i))  # Se espera antes de reintentar\n",
        "              print(f\"Reintentando {url} (intento {i + 1}/{retries})\")\n",
        "              continue # Se reintenta la solicitud\n",
        "          raise RequestError(f\"Error HTTP: {http_err}\")\n",
        "      except requests.exceptions.RequestException as err:\n",
        "          raise RequestError(f\"Error en la solicitud: {err}\")\n",
        "      else:\n",
        "          # Si no hay error se retorna la respuesta\n",
        "          return response\n",
        "\n",
        "def createGameList(url: str, credit_type: str) -> list:\n",
        "  '''\n",
        "  Crea una lista con nombres de juegos mediante web-scrapping.\n",
        "\n",
        "  Parámetros:\n",
        "    - url: Dirección URL hacia donde realizar el web-scrapping.\n",
        "    - credit_type: Cadena de texto que indica el tipo de crédido (creador o artista).\n",
        "\n",
        "  Retorno:\n",
        "    - game_list: Lista con nombres de juegos.\n",
        "  '''\n",
        "\n",
        "  # Petición GET a la URL.\n",
        "  response = getValidation(url)\n",
        "\n",
        "  # Extraigo el contenido\n",
        "  data = response.text\n",
        "\n",
        "  # Convertir el HTML a JSON (simulación del proceso que realmente realiza el frontend)\n",
        "  start_marker = 'GEEK.geekitemPreload = '\n",
        "  end_marker = 'GEEK.geekitemSettings'\n",
        "  start_idx = data.find(start_marker) + len(start_marker)\n",
        "  end_idx = data.find(end_marker, start_idx)\n",
        "  json_data = json.loads(data[start_idx:end_idx-3])\n",
        "\n",
        "  # Extraer nombres de los juegos de mesa\n",
        "  game_list = [item['name'] for item in json_data['item']['links'][credit_type]]\n",
        "\n",
        "  return game_list\n",
        "\n",
        "def createCreditData(game_names_inka_brand: list, game_names_markus_brand: list, game_names_dennis_lohaussen: list) -> pd.DataFrame:\n",
        "  '''\n",
        "  Genera DataFrames con datos de créditos.\n",
        "\n",
        "  Parámetros:\n",
        "    - game_names_inka_brand: Lista con nombres de juegos de mesa de Inka Brand.\n",
        "    - game_names_markus_brand: Lista con nombres de juegos de mesa de Markus Brand.\n",
        "    - game_names_dennis_lohaussen: Lista con nombres de juegos de mesa de Dennis Lohausen.\n",
        "\n",
        "  Retorno:\n",
        "    - Múltiples DataFrames con los datos de los créditos del juego.\n",
        "  '''\n",
        "  # Datos de créditos\n",
        "  designers = pd.DataFrame({\n",
        "      'name': ['Inka Brand', 'Markus Brand'],\n",
        "      'sex': ['Female', 'Male']\n",
        "  })\n",
        "\n",
        "  artists = pd.DataFrame({\n",
        "      'name': ['Dennis Lohausen'],\n",
        "      'sex': ['Male']\n",
        "  })\n",
        "\n",
        "  primary_names = pd.DataFrame({\n",
        "      'name': ['Rajas of the Ganges'],\n",
        "      'year': [2017]\n",
        "  })\n",
        "\n",
        "  alternative_names = pd.DataFrame({\n",
        "      'name': ['I Ragià del Gange', 'Rijs van de Gang', 'Rajas de la Gange', 'Rajas der Gange', 'Rajas del Ganges', 'Раджи Ганга', 'ガンジスの霊王', '갠지스의 라자', 'Rajas do Gange'],\n",
        "      'language': ['Italian', 'Dutch', 'French', 'German', 'Spanish', 'Russian', 'Japanese', 'Korean', 'Portuguese']\n",
        "  })\n",
        "\n",
        "  categories = pd.DataFrame({\n",
        "      'name': ['Dice', 'Economic', 'Renaissance', 'Territory Building']\n",
        "  })\n",
        "\n",
        "  mechanisms = pd.DataFrame({\n",
        "      'name': ['Connections', 'Dice Rolling', 'Race', 'Tile Placement', 'Track Movement', 'Worker Placement', 'Worker Placement with Dice Workers']\n",
        "  })\n",
        "\n",
        "  publishers = pd.DataFrame({\n",
        "      'name': ['HUCH!', '999 Games', 'Devir', 'Dice Realm', 'DV Games', 'Egmont Polska', 'Fabrika Igr', 'Game Harbor', 'HOT Games', 'nostalgia III', 'R&R Games']\n",
        "  })\n",
        "\n",
        "  inka_brand_designed_games = pd.DataFrame({\n",
        "      'name': game_names_inka_brand\n",
        "  })\n",
        "\n",
        "  markus_brand_designed_games = pd.DataFrame({\n",
        "      'name': game_names_markus_brand\n",
        "  })\n",
        "\n",
        "  dennis_lohaussen_artworked_games = pd.DataFrame({\n",
        "      'name': game_names_dennis_lohaussen\n",
        "  })\n",
        "\n",
        "  expansions = pd.DataFrame({\n",
        "      'name': ['Rajas of the Ganges: Goodie Box 2', 'Rajas of the Ganges: Goodie Box 1', 'Rajas of the Ganges: Blessings of Kedarnath', 'Rajas of the Ganges: Tiger Expansion',\n",
        "               'Rajas of the Ganges: Snake Expansion', 'Deutscher Spielepreis 2018 Goodie Box', 'Brettspiel Adventskalender 2018', 'Rajas of the Ganges: Mango Village', 'Brettspiel Adventskalender 2017'],\n",
        "      'year': ['2020', '2019', '2019', '2018', '2018', '2018', '2018', '2017', '2017']\n",
        "  })\n",
        "\n",
        "  return designers, artists, primary_names, alternative_names, categories, mechanisms, publishers, inka_brand_designed_games, markus_brand_designed_games, dennis_lohaussen_artworked_games, expansions\n",
        "\n",
        "def createGraph(graph: Graph):\n",
        "  '''\n",
        "  Crea un grafo de RedisGraph.\n",
        "\n",
        "  Parámetros:\n",
        "    - graph: Objeto Graph de RedisGraph.\n",
        "  '''\n",
        "  # URLs de la páginas de donde quiero extraer las infos\n",
        "  url_inka = 'https://boardgamegeek.com/boardgamedesigner/6940/inka-brand/linkeditems/boardgamedesigner'\n",
        "  url_markus = 'https://boardgamegeek.com/boardgamedesigner/6941/markus-brand/linkeditems/boardgamedesigner'\n",
        "  url_dennis = 'https://boardgamegeek.com/boardgameartist/12484/dennis-lohausen/linkeditems/boardgameartist'\n",
        "\n",
        "  # Extraer nombres de los juegos de mesa\n",
        "  game_names_inka_brand = createGameList(url_inka, 'boardgamedesigner')\n",
        "  game_names_markus_brand = createGameList(url_markus, 'boardgamedesigner')\n",
        "  game_names_dennis_lohaussen = createGameList(url_dennis, 'boardgameartist')\n",
        "\n",
        "  # Crear DataFrames de créditos\n",
        "  designers, artists, primary_names, alternative_names, categories, mechanisms, publishers, inka_brand_designed_games, markus_brand_designed_games, dennis_lohaussen_artworked_games, expansions = createCreditData(game_names_inka_brand, game_names_markus_brand, game_names_dennis_lohaussen)\n",
        "\n",
        "  # Crear nodos\n",
        "  designer_node_list = []\n",
        "  for index, row in designers.iterrows():\n",
        "      designer_node_list.append(Node(label='Designer', properties={'name': row['name'], 'sex': row['sex']}))\n",
        "\n",
        "  artist_node_list = []\n",
        "  for index, row in artists.iterrows():\n",
        "      artist_node_list.append(Node(label='Artist', properties={'name': row['name'], 'sex': row['sex']}))\n",
        "\n",
        "  primary_name_node_list = []\n",
        "  for index, row in primary_names.iterrows():\n",
        "      primary_name_node_list.append(Node(label='Primary_Name', properties={'name': row['name'], 'year': row['year']}))\n",
        "\n",
        "  alternative_name_node_list = []\n",
        "  for index, row in alternative_names.iterrows():\n",
        "      alternative_name_node_list.append(Node(label='Alternative_Name', properties={'name': row['name'], 'language': row['language']}))\n",
        "\n",
        "  category_node_list = []\n",
        "  for index, row in categories.iterrows():\n",
        "      category_node_list.append(Node(label='Category', properties={'name': row['name']}))\n",
        "\n",
        "  mechanism_node_list = []\n",
        "  for index, row in mechanisms.iterrows():\n",
        "      mechanism_node_list.append(Node(label='Mechanism', properties={'name': row['name']}))\n",
        "\n",
        "  publisher_node_list = []\n",
        "  for index, row in publishers.iterrows():\n",
        "      publisher_node_list.append(Node(label='Publisher', properties={'name': row['name']}))\n",
        "\n",
        "  inka_brand_designed_games_list = []\n",
        "  for index, row in inka_brand_designed_games.iterrows():\n",
        "      inka_brand_designed_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  markus_brand_designed_games_list = []\n",
        "  for index, row in markus_brand_designed_games.iterrows():\n",
        "      markus_brand_designed_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  dennis_lohaussen_artworked_games_list = []\n",
        "  for index, row in dennis_lohaussen_artworked_games.iterrows():\n",
        "      dennis_lohaussen_artworked_games_list.append(Node(label='Game', properties={'name': row['name']}))\n",
        "\n",
        "  expansions_node_list = []\n",
        "  for index, row in expansions.iterrows():\n",
        "      expansions_node_list.append(Node(label='Expansion', properties={'name': row['name'], 'year': row['year']}))\n",
        "\n",
        "  # Agregar los nodos al gráfico\n",
        "  for node in designer_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in artist_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in primary_name_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in alternative_name_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in category_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in mechanism_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in publisher_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in inka_brand_designed_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in markus_brand_designed_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in dennis_lohaussen_artworked_games_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  for node in expansions_node_list:\n",
        "      graph.add_node(node)\n",
        "\n",
        "  # Crear más relaciones (aristas)\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for designer in designer_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_DESIGNER', designer))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for artist in artist_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_ARTIST', artist))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for alternative_name in alternative_name_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_ALTERNATIVE_NAME', alternative_name))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for category in category_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_CATEGORY', category))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for mechanism in mechanism_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_MECHANISM', mechanism))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for publisher in publisher_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_PUBLISHER', publisher))\n",
        "\n",
        "  for game in inka_brand_designed_games_list:\n",
        "          graph.add_edge(Edge(designer_node_list[0], 'DESIGNED', game))\n",
        "\n",
        "  for game in markus_brand_designed_games_list:\n",
        "          graph.add_edge(Edge(designer_node_list[1], 'DESIGNED', game))\n",
        "\n",
        "  for game in dennis_lohaussen_artworked_games_list:\n",
        "          graph.add_edge(Edge(artist_node_list[0], 'ARTWORKED', game))\n",
        "\n",
        "  for primary_name in primary_name_node_list:\n",
        "      for expansion in expansions_node_list:\n",
        "          graph.add_edge(Edge(primary_name, 'HAS_EXPANSION', expansion))"
      ],
      "metadata": {
        "id": "eILPIJNYJZi4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pruebas sobre la base de datos de Grafos"
      ],
      "metadata": {
        "id": "-_h8KbWCZVP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ejecutar una consulta para comprobar los nodos\n",
        "# query = \"\"\"MATCH (p:Designer) RETURN p.name, p.sex\"\"\"\n",
        "# result = graph.query(query)\n",
        "\n",
        "# # Imprimir los resultados de la consulta\n",
        "# for record in result.result_set:\n",
        "#     print(f\"Person: {record[0]}, Sex: {record[1]}\")"
      ],
      "metadata": {
        "id": "FP7ZY9WXCKB0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico de la base de datos de Grafos"
      ],
      "metadata": {
        "id": "_RMC6CWeZZ1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Realizar la consulta para obtener nodos y relaciones\n",
        "# query = \"\"\"MATCH (a)-[r]->(b) RETURN a.name, b.name, type(r)\"\"\"\n",
        "# result = graph.query(query)\n",
        "\n",
        "# # Crear un grafo dirigido con networkx\n",
        "# G = nx.DiGraph()\n",
        "\n",
        "# # Agregar los nodos y relaciones a networkx\n",
        "# for record in result.result_set:\n",
        "#     record_1 = record[0]\n",
        "#     record_2 = record[1]\n",
        "#     relation = record[2]\n",
        "\n",
        "#     G.add_node(record_1)\n",
        "#     G.add_node(record_2)\n",
        "#     G.add_edge(record_1, record_2, label=relation)\n",
        "\n",
        "# # Ajustar layout para una mayor separación entre los nodos\n",
        "# pos = nx.spring_layout(G, k=1.5, iterations=50)  # 'k' controla la distancia entre nodos\n",
        "\n",
        "# plt.figure(figsize=(10, 8))  # Aumentar el tamaño de la figura\n",
        "\n",
        "# # Dibujar nodos y etiquetas\n",
        "# nx.draw(G, pos, with_labels=True, node_size=3000, node_color=\"lightblue\", font_size=12, font_weight=\"bold\", arrows=True)\n",
        "\n",
        "# # Dibujar las etiquetas de las relaciones (aristas) con un desplazamiento\n",
        "# edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "# nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', label_pos=0.7)  # Ajustar label_pos para desplazar etiquetas\n",
        "\n",
        "# # Mejorar la disposición de los nodos para que no se superpongan\n",
        "# for label in pos:\n",
        "#     pos[label][1] += 0.05  # Elevar un poco las etiquetas de los nodos\n",
        "\n",
        "# plt.title(\"Visualización del grafo de RedisGraph\", fontsize=14)\n",
        "# plt.show()\n",
        "\n",
        "# # Imprimir todas las relaciones encontradas en formato tríada (Node)-[Relation]->(Node)\n",
        "# for edge in G.edges(data=True):\n",
        "#     print(f\"({edge[0]})-[{edge[2]['label']}]->({edge[1]})\")"
      ],
      "metadata": {
        "id": "8WH-pR7HEU8e"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificador"
      ],
      "metadata": {
        "id": "fZmkhuNOo8uf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basada en modelo entrenado con ejemplos y embeddings"
      ],
      "metadata": {
        "id": "Am4bHggtpmMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def randomForestClassifierCreator(production: bool = False) -> RandomForestClassifier:\n",
        "  '''\n",
        "  Genera un clasificador de textos basado en RandomForest.\n",
        "  Clasifica en cuatro categorías: review, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - production: si es False, se calculan y muestran las métricas de prueba.\n",
        "\n",
        "  Retorno:\n",
        "    - Modelo de RandomForest de clasificación.\n",
        "  '''\n",
        "  # Carga del modelo desde HuggingFace\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "  # Carga del dataset\n",
        "  file_id = '1j2MCy0CACVHFVa3YfirqssUNcbQmq2Gn'\n",
        "  df = getDataFrame(file_id)\n",
        "\n",
        "  # Generación de estructura con el dataset recibido\n",
        "  dataset = []\n",
        "  for row in df.itertuples():\n",
        "      dataset.append((row.prompt, row.classification))\n",
        "\n",
        "  # Preparación de X e y\n",
        "  X = [prompt.lower() for prompt, classification in dataset]\n",
        "  y = [classification for prompt, classification in dataset]\n",
        "\n",
        "  # División del dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)#, stratify=y)\n",
        "\n",
        "  # Obtención los embeddings de BERT para los conjuntos de entrenamiento\n",
        "  X_train_vectorized = model.encode(X_train)\n",
        "\n",
        "  ## Creación y entrenamiento del modelo de RandomForest\n",
        "\n",
        "  # Definición de los parámetros\n",
        "  param_grid = {\n",
        "      'n_estimators': [200, 500, 700, 900],   #[50, 100, 150, 200, 500, 700],\n",
        "      'max_depth': [3, 5, 10, 15, 20]         #[None, 1, 2, 3, 4, 5, 10, 15]\n",
        "  }\n",
        "\n",
        "  # Creación el objeto GridSearchCV\n",
        "  grid_search_cv = GridSearchCV(\n",
        "      estimator=RandomForestClassifier(random_state=42),  # Modelo Random Forest\n",
        "      param_grid=param_grid,\n",
        "      scoring='accuracy',  # Métrica para evaluar\n",
        "      n_jobs=-1,           # Usar todos los núcleos disponibles\n",
        "      cv=5,                # 5 particiones para validación cruzada\n",
        "      verbose=0\n",
        "  )\n",
        "\n",
        "  # Ajuste del modelo con los datos de entrenamiento\n",
        "  grid_search_cv.fit(X_train_vectorized, y_train)\n",
        "\n",
        "  # Obtención del mejor estimador\n",
        "  modelo = grid_search_cv.best_estimator_\n",
        "  best_params = grid_search_cv.best_params_\n",
        "\n",
        "  # Entrenamiento del modelo\n",
        "  modelo.fit(X_train_vectorized, y_train)\n",
        "\n",
        "  if not production:\n",
        "    # Obtención de los embeddings de BERT para los conjuntos de prueba\n",
        "    X_test_vectorized = model.encode(X_test)\n",
        "\n",
        "    # Evaluación del modelo\n",
        "    y_pred_train = modelo.predict(X_train_vectorized)\n",
        "    y_pred_test = modelo.predict(X_test_vectorized)\n",
        "\n",
        "    train_acc = accuracy_score(y_train, y_pred_train)\n",
        "    test_acc = accuracy_score(y_test, y_pred_test)\n",
        "    matrix = confusion_matrix(y_test, y_pred_test)\n",
        "    report = classification_report(y_test, y_pred_test, zero_division=1)\n",
        "\n",
        "    print(\"\\nExactitud Entrenamiento:\", train_acc)\n",
        "    print(\"Exactitud Prueba:\", test_acc)\n",
        "    print(\"\\nMejores parámetros:\", best_params)\n",
        "    print(\"\\nMatriz de confusión:\", matrix)\n",
        "    print(\"\\nReporte de clasificación:\", report)\n",
        "\n",
        "  return modelo\n",
        "\n",
        "\n",
        "def rfClassifier(modelo: RandomForestClassifier, prompt: str) -> str:\n",
        "  '''\n",
        "  Clasificador de textos basado en RandomForest. Recibe un prompt por parte del usuario\n",
        "  y lo clasifica en cuatro categorías: reviews, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - modelo: Modelo de clasificación.\n",
        "    - prompt: Texto a clasificar.\n",
        "\n",
        "  Retorno:\n",
        "    - Clasificación del texto.\n",
        "  '''\n",
        "  # Carga del modelo desde HuggingFace\n",
        "  model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
        "\n",
        "  # Adaptación del texto\n",
        "  prompt = [prompt.lower()]\n",
        "\n",
        "  # Preprocesamiento y vectorización de las nuevas frases\n",
        "  prompt_vectorized = model.encode(prompt)\n",
        "\n",
        "  # Predicción con el modelo entrenado\n",
        "  classification = modelo.predict(prompt_vectorized)\n",
        "\n",
        "  # Impresión del prompt y su etiquetado\n",
        "  # print(f\"\\nPrompt: '{prompt}'\")\n",
        "  # print(f\"Clasificación predicha: {classification}\\n\")\n",
        "\n",
        "  return classification"
      ],
      "metadata": {
        "id": "6P9acraLpAXU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basada en LLM"
      ],
      "metadata": {
        "id": "hkzICjMlpu6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def llmClassifier(query_str: str, client: InferenceClient) -> str:\n",
        "  '''\n",
        "  Clasificador de textos basado en LLM. Recibe un prompt por parte del usuario\n",
        "  y lo clasifica en cuatro categorías: review, rules, stats y credits.\n",
        "\n",
        "  Parámetros:\n",
        "    - query_str: Cadena de texto que representa la consulta del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Clasificación del texto.\n",
        "  '''\n",
        "  # Armado de la query\n",
        "  prompt = (\n",
        "        \"Classify text strictly into just one of the following words: review, rules, stats and credits.\\n\"\n",
        "        \"Do not use any other words on your answer.\\n\"\n",
        "        \"Categories and mechanisms should be classified as credits.\\n\"\n",
        "        \"Amount of players, playing time and ages should be classified as stats.\\n\"\n",
        "        f\"Question: {query_str}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        "  )\n",
        "\n",
        "  return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "mFfU8I0PpxOf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever"
      ],
      "metadata": {
        "id": "lFJrYJ2KTnTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retriever(query: str) -> list[str]:\n",
        "  '''\n",
        "  Función que decide a qué base de datos extraer en base a la clasificación, obteniendo los textos contextuales.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Textos contextuales.\n",
        "  '''\n",
        "  # Caso 1: Base de datos vectorial\n",
        "  if classification == \"review\" or classification == \"rules\":\n",
        "    results = vectorialDbRetriever(query)\n",
        "\n",
        "    # Re-ranking\n",
        "    results = reranker(query, results[0], 3)\n",
        "\n",
        "  # Caso 2: Base de datos tabular\n",
        "  elif classification == \"stats\":\n",
        "    results = tableDbRetriever(query)\n",
        "\n",
        "  # Caso 3: Base de datos de grafos\n",
        "  elif classification == \"credits\":\n",
        "    results = graphDbRetriever(query)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def vectorialDbRetriever(query: str) -> list[str]:\n",
        "  '''\n",
        "  Retriever de texto basado en vectores. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Lista de textos contextuales.\n",
        "  '''\n",
        "\n",
        "  llm_classifier_api_key = \"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\"\n",
        "  client = InferenceClient(api_key=llm_classifier_api_key)\n",
        "\n",
        "  # Inicialización de lista de retorno\n",
        "  retriever_list = []\n",
        "\n",
        "  # Generación de embedding de la query\n",
        "  embedding_query = embed([query]).numpy().tolist()\n",
        "\n",
        "  # Refinado de metadata de reglas\n",
        "  if classification == \"rules\":\n",
        "    # Armado de la query\n",
        "    prompt = (\n",
        "          f\"Classify text strictly into just one of the following rules category: normal, solo, ai and automa.\\n\"\n",
        "          \"If none of the classifications seems correct enough, classify the text as normal.\"\n",
        "          \"Do not use any other words on your answer.\\n\"\n",
        "          f\"Question: {query}\\n\"\n",
        "          \"Answer: \"\n",
        "      )\n",
        "\n",
        "    # Armado de los mensajes\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "        },\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Inferencia\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "      messages=messages,\n",
        "      max_tokens=1000\n",
        "    )\n",
        "\n",
        "    # Categría elegida\n",
        "    category = completion.choices[0].message.content\n",
        "  else:\n",
        "    category = classification\n",
        "\n",
        "  # Filtrado de metadatas\n",
        "  filtered_metadatas = [\n",
        "      metadata\n",
        "      for metadata in collection.get(include=[\"metadatas\"])[\"metadatas\"]\n",
        "      if category in metadata.get(\"source\", \"\")\n",
        "  ]\n",
        "\n",
        "  # Extraer sources\n",
        "  sources = []\n",
        "  for metadata in filtered_metadatas:\n",
        "      sources.append(metadata[\"source\"])\n",
        "\n",
        "  # Búsqueda de resultados\n",
        "  results = collection.query(\n",
        "      query_embeddings = embedding_query,  # Aquí pasamos el embedding de la consulta\n",
        "      n_results = 10,  # Traemos los 10 resultados más cercanos\n",
        "      where = {\"source\": {\"$in\": sources}}  # Filtrar por metadatos\n",
        "  )\n",
        "\n",
        "  # Adjuntado de resultados\n",
        "  for doc in results[\"documents\"]:\n",
        "    retriever_list.append(doc)\n",
        "\n",
        "  return retriever_list\n",
        "\n",
        "\n",
        "def tableDbRetriever(query: str) -> str:\n",
        "  '''\n",
        "  Retriever de texto basado en tablas. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - DataFrame con la información recuperada.\n",
        "  '''\n",
        "\n",
        "  llm_classifier_api_key = \"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\"\n",
        "  client = InferenceClient(api_key=llm_classifier_api_key)\n",
        "\n",
        "  # Columnas del DataFrame\n",
        "  columns = table_df.columns.tolist()\n",
        "\n",
        "  # Armado de la query\n",
        "  prompt = (\n",
        "        f\"Classify text strictly into just one of the following words: {columns}.\\n\"\n",
        "        \"Do not use any other words on your answer.\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=1000\n",
        "  )\n",
        "\n",
        "  # Columna elegida\n",
        "  selected_column = completion.choices[0].message.content\n",
        "\n",
        "  # Selección de resultados\n",
        "  results = selected_column + \": \" + str(table_df[selected_column].iloc[0])\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def graphDbRetriever(query: str) -> str:\n",
        "  '''\n",
        "  Retriever de texto basado en grafos. Recibe un prompt por parte del usuario\n",
        "  y recupera información asociada.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "\n",
        "  Retorno:\n",
        "    - Lista de textos contextuales.\n",
        "  '''\n",
        "\n",
        "  llm_classifier_api_key = \"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\"\n",
        "  client = InferenceClient(api_key=llm_classifier_api_key)\n",
        "\n",
        "  # Lista de relaciones\n",
        "  relations = [\n",
        "      (\"Inka Brand\", \"DESIGNED\", [\"Sausage Sizzle!\", \"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\",\n",
        "                                  \"Andor: The Family Fantasy Game\", \"Andor: The Family Fantasy Game – The Danger in the Shadows\",\n",
        "                                  \"Bibi & Tina: Das Spiel zum Film\", \"Bitte nicht öffnen: Bissig!\"]),\n",
        "      (\"Markus Brand\", \"DESIGNED\", [\"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\", \"Andor: The Family Fantasy Game\",\n",
        "                                    \"Andor: The Family Fantasy Game – The Danger in the Shadows\", \"Bibi & Tina: Das Spiel zum Film\",\n",
        "                                    \"Bitte nicht öffnen: Bissig!\", \"Black Stories: Das Spiel\"]),\n",
        "      (\"Dennis Lohausen\", \"ARTWORKED\", [\"Coal Baron: The Great Card Game\", \"100!\", \"1001 Karawane\",\n",
        "                                        \"1001 Karawane: Sonderchips\", \"112: Brandgefährlich\", \"15\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_DESIGNER\", [\"Inka Brand\", \"Markus Brand\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_ARTIST\", [\"Dennis Lohausen\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_ALTERNATIVE_NAME\", [\"I Ragià del Gange\", \"Rijs van de Gang\", \"Rajas de la Gange\",\n",
        "                                                       \"Rajas der Gange\", \"Rajas del Ganges\", \"Раджи Ганга\", \"ガンジスの霊王\",\n",
        "                                                       \"갠지스의 라자\", \"Rajas do Gange\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_CATEGORY\", [\"Dice\", \"Economic\", \"Renaissance\", \"Territory Building\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_MECHANISM\", [\"Connections\", \"Dice Rolling\", \"Race\", \"Tile Placement\", \"Track Movement\",\n",
        "                                                \"Worker Placement\", \"Worker Placement with Dice Workers\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_PUBLISHER\", [\"HUCH!\", \"999 Games\", \"Devir\", \"Dice Realm\", \"DV Games\", \"Egmont Polska\",\n",
        "                                                \"Fabrika Igr\", \"Game Harbor\", \"HOT Games\", \"nostalgia III\", \"R&R Games\"]),\n",
        "      (\"Rajas of the Ganges\", \"HAS_EXPANSION\", [\"Rajas of the Ganges: Goodie Box 2\", \"Rajas of the Ganges: Goodie Box 1\",\n",
        "                                                \"Rajas of the Ganges: Blessings of Kedarnath\", \"Rajas of the Ganges: Tiger Expansion\",\n",
        "                                                \"Rajas of the Ganges: Snake Expansion\", \"Deutscher Spielepreis 2018 Goodie Box\",\n",
        "                                                \"Brettspiel Adventskalender 2018\", \"Rajas of the Ganges: Mango Village\",\n",
        "                                                \"Brettspiel Adventskalender 2017\"])\n",
        "  ]\n",
        "\n",
        "  # Lista de entidades\n",
        "  entities = {\n",
        "      \"Game\": [\"Sausage Sizzle!\", \"Andor Junior: Die Fährtenleserin Fennah / Der Fährtenleser Fenn\",\n",
        "                \"Andor: The Family Fantasy Game\", \"Andor: The Family Fantasy Game – The Danger in the Shadows\",\n",
        "                \"Bibi & Tina: Das Spiel zum Film\", \"Bitte nicht öffnen: Bissig!\", \"Black Stories: Das Spiel\",\n",
        "                \"Coal Baron: The Great Card Game\", \"100!\", \"1001 Karawane\", \"1001 Karawane: Sonderchips\",\n",
        "                \"112: Brandgefährlich\", \"15\"],\n",
        "      \"Primary_Name\": [\"Rajas of the Ganges\"],\n",
        "      \"Designer\": [\"Inka Brand\", \"Markus Brand\"],\n",
        "      \"Artist\": [\"Dennis Lohausen\"],\n",
        "      \"Alternative_Name\": [\"I Ragià del Gange\", \"Rijs van de Gang\", \"Rajas de la Gange\", \"Rajas der Gange\",\n",
        "                           \"Rajas del Ganges\", \"Раджи Ганга\", \"ガンジスの霊王\", \"갠지스의 라자\", \"Rajas do Gange\"],\n",
        "      \"Category\": [\"Dice\", \"Economic\", \"Renaissance\", \"Territory Building\"],\n",
        "      \"Mechanism\": [\"Connections\", \"Dice Rolling\", \"Race\", \"Tile Placement\", \"Track Movement\",\n",
        "                    \"Worker Placement\", \"Worker Placement with Dice Workers\"],\n",
        "      \"Publisher\": [\"HUCH!\", \"999 Games\", \"Devir\", \"Dice Realm\", \"DV Games\", \"Egmont Polska\",\n",
        "                    \"Fabrika Igr\", \"Game Harbor\", \"HOT Games\", \"nostalgia III\", \"R&R Games\"],\n",
        "      \"Expansion\": [\"Rajas of the Ganges: Goodie Box 2\", \"Rajas of the Ganges: Goodie Box 1\",\n",
        "                    \"Rajas of the Ganges: Blessings of Kedarnath\", \"Rajas of the Ganges: Tiger Expansion\",\n",
        "                    \"Rajas of the Ganges: Snake Expansion\", \"Deutscher Spielepreis 2018 Goodie Box\",\n",
        "                    \"Brettspiel Adventskalender 2018\", \"Rajas of the Ganges: Mango Village\", \"Brettspiel Adventskalender 2017\"]\n",
        "  }\n",
        "\n",
        "  # Armado de la query\n",
        "  prompt = (\"Write down a Cypher query based on a question from a user.\\n\"\n",
        "        \"It must strictly be just one query, it is important that you respect all of Cypher's syntax rules and use the entities and relationships provided.\\n\"\n",
        "        f\"Question: {query}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "\n",
        "  # Armado de los mensajes\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\\n\"\n",
        "          \"You are an expert in board games and understand their descriptive characteristics.\\n\"\n",
        "          \"As an expert on the game Rajas of the Ganges, you are also an expert on Cypher queries based on a question made by a user.\\n\"\n",
        "          \"The database is composed by entities and relations. The entities from the database can be found in the following Python dictionary:\\n\"\n",
        "          f\"{entities}, where the keys are the entities and the labels are the same ones they have on the database.\\n\"\n",
        "          \"The relations from the database can be found in the following Python list:\\n\"\n",
        "          f\"{relations}.\\n\"\n",
        "          \"Make sure to give a full statement, for example, if the question is \"\"What are the mechanisms of the game?\"\"\\n\"\n",
        "          \"the answer should be: cypher MATCH (game:Primary_Name {name: 'Rajas of the Ganges'})-[:HAS_MECHANISM]->(mechanism:Mechanism) RETURN mechanism.name\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": prompt},\n",
        "  ]\n",
        "\n",
        "  # Inferencia\n",
        "  completion = client.chat.completions.create(\n",
        "      model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "    messages=messages,\n",
        "    max_tokens=30000\n",
        "  )\n",
        "\n",
        "  # Query para Cypher\n",
        "  cypher_query = completion.choices[0].message.content\n",
        "  cypher_query = cypher_query.replace(\"`\", \"\")\n",
        "  cypher_query = cypher_query.replace(\"'''\", \"\")\n",
        "  cypher_query = cypher_query.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "  # Resultado de la búsqueda\n",
        "  result = graph.query(cypher_query)\n",
        "\n",
        "  # Guardar los resultados de la consulta\n",
        "  results = \"\"\n",
        "  first = True\n",
        "  for record in result.result_set:\n",
        "    if first:\n",
        "      for key, values in entities.items():\n",
        "        if record[0] in values:\n",
        "          results += f\"{key}: {record[0]}\"\n",
        "      first = False\n",
        "    else:\n",
        "      for key, values in entities.items():\n",
        "        if record[0] in values:\n",
        "          results += f\", {key}: {record[0]}\"\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "AJZC8_EHTqQk"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-Ranker"
      ],
      "metadata": {
        "id": "ahBg486gAfbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reranker(query: str, documents: list, top_k: int = 3):\n",
        "  '''\n",
        "  Realiza un re-ranking sobre los documentos obtenidos tras una búsqueda en\n",
        "  la base de datos vectorial.\n",
        "\n",
        "  Parámetros:\n",
        "    - query: prompt del usuario.\n",
        "    - documents: resultados obtenidos tras la búsqueda en base de datos vectorial.\n",
        "    - top_k: número de documentos a retornar.\n",
        "\n",
        "  Retorna:\n",
        "    - Lista de documentos ordenados por relevancia.\n",
        "  '''\n",
        "  # Carga del modelo para re-ranking\n",
        "  model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "  # Emparejamiento de prompt del usuario con documentos para hacer predicciones\n",
        "  pairs = [(query, document) for document in documents]\n",
        "  scores = model.predict(pairs)\n",
        "\n",
        "  # Emparejamiento de puntajes de predicción con documentos y ordenado\n",
        "  scored_docs = list(zip(scores, documents))\n",
        "  scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "  # Toma de los documentos mejor puntuados\n",
        "  scored_docs = scored_docs[:top_k]\n",
        "\n",
        "  # Armado de string contextual final\n",
        "  complete_context = \"\"\n",
        "  for score, doc in scored_docs:\n",
        "    complete_context += doc + \";\"\n",
        "\n",
        "  return complete_context"
      ],
      "metadata": {
        "id": "p9k7ZMT-Akk7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG"
      ],
      "metadata": {
        "id": "VKOdtw9S94_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RAG(query_str: str, context_str: str, api_key: str, add_generation_prompt: bool = True, max_new_tokens: int = 768) -> str:\n",
        "  '''\n",
        "  Realiza un RAG sobre los documentos obtenidos tras una búsqueda en\n",
        "  las bases de datos.\n",
        "\n",
        "  Parámetros:\n",
        "    - query_str: prompt del usuario.\n",
        "    - context_str: datos contextuales obtenidos de las bases de datos.\n",
        "    - add_generation_prompt: indica si se debe agregar el prompt que lo generó.\n",
        "\n",
        "  Retorna:\n",
        "    - String con el resultado del LLM.\n",
        "  '''\n",
        "  # Armado del prompt\n",
        "  TEXT_QA_PROMPT_TMPL = (\n",
        "      \"The contextual information is as follows:\\n\"\n",
        "      \"---------------------\\n\"\n",
        "      f\"{context_str}\\n\"\n",
        "      \"---------------------\\n\"\n",
        "      \"Given said contextual information and without using any previous knowledge, answer the following question.\\n\"\n",
        "      f\"Question: {query_str}\\n\"\n",
        "      \"Answer: \"\n",
        "  )\n",
        "\n",
        "  # Armado del mensaje\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You are a helpful assistant who always responds with truthful, helpful and fact-based answers. You are an expert in board games and understand their descriptive characteristics.\",\n",
        "      },\n",
        "      {\"role\": \"user\", \"content\": TEXT_QA_PROMPT_TMPL.format(context_str=context_str, query_str=query_str)},\n",
        "  ]\n",
        "\n",
        "  # Formateador del prompt para usar con Zephyr\n",
        "  formatter = Formatter()\n",
        "  conversation = Conversation(model='zephyr', messages=messages)\n",
        "  final_prompt = formatter.render(conversation, add_assistant_prompt=add_generation_prompt)\n",
        "\n",
        "  # URL de la API de Hugging Face para la generación de texto\n",
        "  api_url = \"https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "  # Cabeceras para la solicitud\n",
        "  headers = {\"Authorization\": f\"Bearer {api_key}\"}\n",
        "\n",
        "  # Datos para enviar en la solicitud POST\n",
        "  # Sobre los parámetros: https://huggingface.co/docs/transformers/main_classes/text_generation\n",
        "  data = {\n",
        "      \"inputs\": final_prompt,\n",
        "      \"parameters\": {\n",
        "          \"max_new_tokens\": max_new_tokens,\n",
        "          \"temperature\": 0.5,\n",
        "          \"top_k\": 50,\n",
        "          \"top_p\": 0.95\n",
        "      }\n",
        "  }\n",
        "\n",
        "  # Realizamos la solicitud POST\n",
        "  response = requests.post(api_url, headers=headers, json=data)\n",
        "\n",
        "  # Extraer respuesta\n",
        "  respuesta = response.json()[0][\"generated_text\"][len(final_prompt):]\n",
        "\n",
        "  return respuesta"
      ],
      "metadata": {
        "id": "db3eyI0396Ln"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agente"
      ],
      "metadata": {
        "id": "hnWHcDAyosKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent() -> str:\n",
        "  # Configurar el LLM de Ollama\n",
        "  llm = Ollama(\n",
        "      model=\"llama3.2:latest\",\n",
        "      request_timeout=30.0, # Aumentado el timeout\n",
        "      temperature=0.1, # Reducida la temperatura para respuestas más deterministas\n",
        "      context_window=4096 # Aumentado el contexto\n",
        "  )\n",
        "  Settings.llm = llm\n",
        "\n",
        "  # Crear las herramientas para el agente\n",
        "  tools = [\n",
        "      FunctionTool.from_defaults(fn=vectorialDbRetriever, description=\"It looks for information regarding game rules and game reviews.\"),\n",
        "      FunctionTool.from_defaults(fn=tableDbRetriever, description=\"It looks for information regarding amount of players, playing time, ages and stats of the game.\"),\n",
        "      FunctionTool.from_defaults(fn=graphDbRetriever, description=\"It looks for information regarding credits, mechanisms and categories of the game.\"),\n",
        "  ]\n",
        "\n",
        "  # Crear el agente ReAct\n",
        "  agent = ReActAgent.from_tools(\n",
        "      tools,\n",
        "      llm=llm,\n",
        "      verbose=True,\n",
        "      chat_formatter=ReActChatFormatter(),\n",
        "      system_prompt=\"\"\"\"You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\n",
        "  You are an expert in board games and understand their descriptive characteristics. You must STRICTLY follow the following format:\n",
        "\n",
        "  Thought: Here, I explain what I need to do.\n",
        "  Action: nameOfTheTool\n",
        "  Action Input: \"user prompt\"\n",
        "\n",
        "  Correct examples:\n",
        "  - For vectorialDbRetriever: Action Input: \"user prompt\"\n",
        "  - For tableDbRetriever: Action Input: \"user prompt\"\n",
        "  - For graphDbRetriever: Action Input: \"user prompt\"\n",
        "\n",
        "  Observation: [Result of the tool]\n",
        "  ... [Repeat process if necessary]\n",
        "  Final Answer: The final answer combining all the information.\n",
        "\n",
        "  For each query:\n",
        "  1. Analyze which information do you need.\n",
        "  2. Use the available tools one by one with the right format.\n",
        "  3. Combine results in a final answer.\n",
        "\n",
        "  IMPORTANT:\n",
        "  - Always follow the exact format of the examples for each tool.\n",
        "  - The user prompt will only require using JUST ONE of ALL the tools to obtain the correct answer.\n",
        "  - If you got an answer from one of the functions, ASUME IT'S THE CORRECT ANSWER AND FINISH.\"\"\",\n",
        "      react_chat_history=False,\n",
        "      context=\"\"\"\n",
        "      You are a helpful assistant who always responds with truthful, helpful and fact-based answers.\n",
        "      You are an expert in board games and understand their descriptive characteristics.\n",
        "      Remember the outputs of the tools to use them in your final answer.\n",
        "      \"\"\"\n",
        "  )\n",
        "\n",
        "  # Pregunta del usuario:\n",
        "  prompt = str(input('\\nPlease, enter your question: '))\n",
        "\n",
        "  print(f\"\\nQuery: {prompt}\")\n",
        "\n",
        "  try:\n",
        "      if not prompt.strip():\n",
        "          return \"The query is empty.\"\n",
        "      response = agent.chat(prompt)\n",
        "  except Exception as e:\n",
        "      response = f\"Error processing the query: {str(e)}\"\n",
        "\n",
        "  print(f\"Answer: {response}\")\n",
        "  print(\"------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "oMJmjax2rtEK"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 1"
      ],
      "metadata": {
        "id": "SAf4UFtrhu-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "jqqNcNrhZoge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creación de bases de datos de Grafos"
      ],
      "metadata": {
        "id": "3tf185wdSRF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS DE GRAFOS           ##\n",
        "#########################################################\n",
        "try:\n",
        "  graph\n",
        "  print(\"✔ Base de datos de grafos cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos de grafos.\")\n",
        "\n",
        "  # Conexión a Redis\n",
        "  redis_client = redis.Redis(host='localhost', port=6379)\n",
        "\n",
        "  # Crear una gráfica en RedisGraph\n",
        "  graph = Graph('credits', redis_client)\n",
        "\n",
        "  # Agregar nodos y aristas\n",
        "  createGraph(graph)\n",
        "\n",
        "  # Guardar los datos en RedisGraph\n",
        "  graph.commit()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##           CREACIÓN BASE DE DATOS TABULAR            ##\n",
        "#########################################################\n",
        "# Generación de tablas\n",
        "try:\n",
        "  table_df\n",
        "  print(\"✔ Base de datos tabular cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos tabular.\")\n",
        "  # collection_stats_df, game_attributes_df, game_ranks_df, game_stats_df, parts_exchange_df, play_stats_df = createTables()\n",
        "  table_df = createTables()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS VECTORIAL           ##\n",
        "#########################################################\n",
        "# Generación de collection\n",
        "try:\n",
        "  collection\n",
        "  print(\"✔ Base de datos vectorial cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos vectorial.\")\n",
        "  # Cargar Universal Sentence Encoder\n",
        "  embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "\n",
        "  # Configuración inicial de ChromaDB\n",
        "  client = chromadb.Client()\n",
        "\n",
        "  # Generación de collection\n",
        "  collection = createCollection(client)\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##              CLASIFICACIÓN DE PROMPT                ##\n",
        "#########################################################\n",
        "\n",
        "# # Clasificación con modelo de Random Forest\n",
        "# # # Creación del modelo\n",
        "# # modelo_rf = randomForestClassifierCreator()\n",
        "# # # Descarga del modelo\n",
        "# # joblib.dump(modelo_rf, 'modelo_rf.pkl')\n",
        "# # File ID del modelo\n",
        "# model_file_id = '1t_P6sziWieZcUvNvEujveRfguCVH8LNE'\n",
        "# # Creación la URL de descarga\n",
        "# download_url = f'https://drive.google.com/uc?id={model_file_id}'\n",
        "# # Descarga del archivo\n",
        "# output = 'modelo_rf.pkl'\n",
        "# gdown.download(download_url, output, quiet=True)\n",
        "# # Crga del modelo\n",
        "# modelo_rf = joblib.load('modelo_rf.pkl')\n",
        "# print(rfClassifier(modelo_rf, query_str))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##                     INTERFACE                       ##\n",
        "#########################################################\n",
        "# Prompt del usuario\n",
        "prompt = str(input('\\nPlease, enter your question: '))\n",
        "\n",
        "# Clasificación\n",
        "llm_classifier_api_key = \"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\"\n",
        "llm_class_client = InferenceClient(api_key=llm_classifier_api_key)\n",
        "classification = llmClassifier(prompt, llm_class_client)\n",
        "\n",
        "# Retrieving\n",
        "context = retriever(prompt)\n",
        "\n",
        "# RAG\n",
        "RAG_api_key = \"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\"\n",
        "results = RAG(prompt, context, RAG_api_key)\n",
        "\n",
        "print('\\n')\n",
        "print(results)"
      ],
      "metadata": {
        "id": "GolyC2bUEtDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7459d43-0ea2-4e74-abc1-31498768cbd9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Base de datos de grafos cargada.\n",
            "✔ Base de datos tabular cargada.\n",
            "✔ Base de datos vectorial cargada.\n",
            "\n",
            "Please, enter your question: What's the consensus about the game?\n",
            "\n",
            "\n",
            "The consensus about the game, based on the contextual information provided, is that it is a Euro-style game with a unique point system that involves balancing income and points. The game's replayability is highlighted, as players can sub out river tiles and explore different strategies. The art is described as good, and the game's simplicity and accessibility for new players are noted. The reviewer's initial opinion of the game was not favorable, but after better understanding the game, they now rate it similarly to another reviewer's score of 8.6. The reviewer is interested in trying the game with more players to see how it plays with a larger group. Overall, the reviewer seems to enjoy the game and finds it different enough from other worker placement or resource management games.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problema 2"
      ],
      "metadata": {
        "id": "e_gAyUImh1_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main"
      ],
      "metadata": {
        "id": "BJKQ1Mnfh3tX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS DE GRAFOS           ##\n",
        "#########################################################\n",
        "try:\n",
        "  graph\n",
        "  print(\"✔ Base de datos de grafos cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos de grafos.\")\n",
        "\n",
        "  # Conexión a Redis\n",
        "  redis_client = redis.Redis(host='localhost', port=6379)\n",
        "\n",
        "  # Crear una gráfica en RedisGraph\n",
        "  graph = Graph('credits', redis_client)\n",
        "\n",
        "  # Agregar nodos y aristas\n",
        "  createGraph(graph)\n",
        "\n",
        "  # Guardar los datos en RedisGraph\n",
        "  graph.commit()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##           CREACIÓN BASE DE DATOS TABULAR            ##\n",
        "#########################################################\n",
        "# Generación de tablas\n",
        "try:\n",
        "  table_df\n",
        "  print(\"✔ Base de datos tabular cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos tabular.\")\n",
        "  # collection_stats_df, game_attributes_df, game_ranks_df, game_stats_df, parts_exchange_df, play_stats_df = createTables()\n",
        "  table_df = createTables()\n",
        "\n",
        "\n",
        "#########################################################\n",
        "##          CREACIÓN BASE DE DATOS VECTORIAL           ##\n",
        "#########################################################\n",
        "# Generación de collection\n",
        "try:\n",
        "  collection\n",
        "  print(\"✔ Base de datos vectorial cargada.\")\n",
        "\n",
        "except NameError:\n",
        "  print(\"⚠ Cargando base de datos vectorial.\")\n",
        "  # Cargar Universal Sentence Encoder\n",
        "  embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
        "\n",
        "  # Configuración inicial de ChromaDB\n",
        "  client = chromadb.Client()\n",
        "\n",
        "  # Generación de collection\n",
        "  collection = createCollection(client)\n",
        "\n",
        "#########################################################\n",
        "##                     INTERFACE                       ##\n",
        "#########################################################\n",
        "# Ejecución del agente\n",
        "agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nQ3Ji_oh6pT",
        "outputId": "280c4b24-beff-483b-c862-cbfbf56b23c1"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:llama_index.core.agent.react.formatter:ReActChatFormatter.from_context is deprecated, please use `from_defaults` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Base de datos de grafos cargada.\n",
            "✔ Base de datos tabular cargada.\n",
            "✔ Base de datos vectorial cargada.\n",
            "\n",
            "Please, enter your question: What do people think about the game?\n",
            "\n",
            "Query: What do people think about the game?\n",
            "> Running step 32ba148f-0db0-4773-b831-f7d34e4d8869. Step input: What do people think about the game?\n",
            "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
            "Action: vectorialDbRetriever\n",
            "Action Input: {'title': 'What do people think about the game?', 'type': 'string'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: vectorialDbRetriever() got an unexpected keyword argument 'title'\n",
            "\u001b[0m> Running step 74cd9277-6d20-4a90-b251-e73f95362aa4. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The tool vectorialDbRetriever does not accept a 'title' keyword argument. I need to adjust my input.\n",
            "Action: vectorialDbRetriever\n",
            "Action Input: {'query': 'What do people think about the game?'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: [[\"medium-light worker placement dice managing in game something like that I really like this game I have so much fun playing this game it's so quick it plays with four new players in an hour 15 minutes it plays with two players in like 40 minute and it works really great with all player accounts I love it at - and I love it at four it's so great it's yeah let's just start at the beginning it's already components let's do that the components for this game are okay the components or when you look at it you'll you will understand this is gonna be a euro game this is like my friend saw it the first time it's gonna play it was like mmm this looks like a classic euro game and that's basically what it is you you see you get what you see this is a game where you will see it and you will you will instantly know if this is something you will enjoy or not if you see this and you think this isn't for me maybe it won't be but I think you should check it out because it is different enough from a lot\", \"it later but i think in the base game there's even those river overlay tiles yeah there's some of them there so it does change the game up a bit which is exciting i love the um the little animal the art i enjoy the art in the game i think it looks really nice uh quality of components is strong it's not like crazy not an overproduced game but they're not nothing is too cheap feeling to me and i like the variety of different um buildings that come out how you can adjust how you're gonna score you know so you have to think ahead on that too of what buildings you're gonna upgrade so that you get more fame when you end up deploying those and they change all the time when people take from the stack so all together i really enjoy playing this game uh the first time i felt it was very overwhelming as nick said to try to remember what every spot did and it actually took a couple of tries before i understood the market and what i was selling i'd ask you a lot like so wait what am i doing and\", \"it out because it is different enough from a lot of other work replacement or you grow games but this game really looks like the euro game that it is but the art is good it's it's not mind-blowing but it is it's good art I enjoy it it's clear the economy akin ography is it's clear everything is it's easy to understand everything is clearly laid out the rules are simple everything is is made so that this game will be easy to play also for players who don't enjoy heavier games who doesn't play that many games it's gonna be an easy game to get into and he's a game to understand so game play as I said I really enjoy it one thing I really like about this game is I was really looking forward to checking out and seeing how would this work with the two things going and them the points and the money going to cross each other how would this feel would it feel like something else than just getting most points or getting to the end of the point track and it actually does it makes this game feel\", \"point of this game because most games are about getting the most points and while that is also Impulse important in this game that's not actually the point of the game there isn't six rounds and then the person with the most points is the winner the point of this game is to be the player who gets the point marker here to cross with their money mark you're here so you had to balance your income the money you get the moaning you use while also getting points after that has happened everyone else gonna get one more turn placing one more action one more worker and then if more players have made it happen that I haven't reached each other then the person who I have have them Forest apart from each other is the winner of the game but how do you play this game you play it by placing workers and doing actions as I of course already said but that's isn't really helpful so let's go through the different aspects of this game the game is a lot revolving about around these dice getting the dice\", \"and it actually does it makes this game feel like a race it really really feel like you have to optimize your actions like you have to really try to do the best every turn and you would also do that in all the games but you have to manage I can't just use money all the time because then I I'm gonna have to get so many points and you're not gonna get so many points so we see in this game it's easy to get some money and a lot of points or to get to balance it out or to try to get your heavy money strategy and just also get some points and there's feeling the feels like there's different ways going on how you play this game and I have felt that the game is different enough every time but I'm not sure like how much replayability it is it is all still fun yeah I think it's gonna be fun for like 10 12 plays and then it doesn't really matter because most gamers especially like me we don't play games that many times if you want to gain if you can play 50 times I'm not sure how goodies would\", \"otherwise keep playing i like that as well so i i really like this game i like it a lot um the board is a little busy but you know it can be intimidating to new players but it's really not that difficult the game i'll say so despite the look of the game i don't think you're going to struggle too much with it um even you were able to learn it in a cafe in one sitting really yeah yeah once you realize okay at first maybe because you think it's gonna be a little struggle because you're saying what every spot does this is the quarry you do this there but once you get clicking i mean there's only a few things you're gonna be doing it's pretty intuitive once you get going yeah um so i feel like it's not an entry level i'm not saying that but i am saying it's uh just a step above that i would say um and which is a good thing because there's still plenty of decisions to be made it's not a light game in that um you know you're just going through the motions here you actually have quite a bit\", \"ask you a lot like so wait what am i doing and then it finally dawned on me okay you're using the value of the dices how many things you can buy or sell yeah so my score for this game it's very similar to nick 8.6 so very close to what nick scored it um it went up for me i didn't like it the first time i didn't like it but i didn't think it was that great the first time we played it but then now that i understand the game better and i feel we were had more time to explore it now we only played it with two players we haven't tried it at more than two so i'd be interested to see how this game does play with more people because it is it can be quite competitive yeah for those spots we'll play with four soon um other things uh well i feel like the replayability's there like i said you can sub out those river tiles and things like that you can go about your game in different ways you can get you can really go what vic was saying with upgrading your buildings you can go really strong in the\", \"with certain actions so you know you get it back yeah it's um it's a lot of fun I like how it looks um and it you know it plays pretty pretty smoothly it just keeps going around and around until uh well there is the first player token so we do a round but the first player token doesn't move unless someone takes it yeah as a lot of these games are yeah unless I'm thinking of it backwards where it passes unless someone took it no I don't think it moved okay um either way it's gonna be a lot of fun the first player token is a little dude riding an elephant yeah yeah super cute that's cool I like it um it's it's a great game it's a lot of fun and you know if you like work replacement this is a unique worker placement kind of game and if you like tile placement it has that as well and collection of things it's it's you know very diverse um it came out in 2017 and uh still plays great today so definitely check it out yeah so I agree it's a good place this has been our review of rajas of the\", \"can play 50 times I'm not sure how goodies would be after 50 place but then again I don't know there's just speculations and but I think the replayability is pretty high with this one because you have different dyes you have different players you have different buildings coming out you will go for different strategies will go for a heavy building strategy especially when playing the variant where you have the other side of the board and you will start with different Brown special yield tiles that will give you maybe more of a way to where you want to go what strategy you want to go for and that will also make the game interesting and different every time you play it and everything is fun this game is fun and as I said I am not usually into racing games but this feels like a work your placement racing game because it really feels like a race and also one thing that makes it feel so much like a race is that the actions go very quick like you have an overarching strategy or you have some\", \"hello everyone this is Johannes and you are watching board gaming ramblings and today we are taking a look at Rogers of the Ganges from Inca and Marcus brand a euro game that came out at essence pealed 2017 a game I was really looking forward to playing and now I have played it and that's why I'm making this video to tell you about this game let's see how you play Rogers of the Yankees Rogers of the Ganges is a worker placement game so you're gonna take your workers you're gonna place them and do some actions like you're doing most worker placement games but this game also uses dice and some of the actions you take with your worker we'll need to discard dice some actions need dive so specific color some actions need dice with a specific pip number and some actions just want to do this card dice a die and get rid of it and some actions will give you more dice so you can use them for your actions so let's just for first talk about the point of this game because most games are about\"]]\n",
            "\u001b[0m> Running step b3e04db8-f771-49dd-b253-cb39fd668060. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The output from vectorialDbRetriever contains multiple reviews with similar sentiments. I need to analyze the reviews to identify common themes and opinions.\n",
            "Action: tableDbRetriever\n",
            "Action Input: {'query': 'Rajahs of the Ganges game review', 'type': 'string'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: tableDbRetriever() got an unexpected keyword argument 'type'\n",
            "\u001b[0m> Running step 5b576ad1-dbc7-4184-872b-cd141ee0488d. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The tool vectorialDbRetriever does not accept a 'type' keyword argument. I need to adjust my input.\n",
            "Action: vectorialDbRetriever\n",
            "Action Input: {'query': 'Rajahs of the Ganges game review'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: [[\"place this has been our review of rajas of the Ganges of Ben Jared I'm Holly see you next time\", \"stands out immediately is this is going to be a dice placing game you see there are dice the Shiva character is manipulating dices different colors because that's exactly what ganas is you have the board where you will be placing Dice and workers I'll show you a little bit how it works but I would like to give you some flavor here before we go in so in the 16th century Indian the powerful Empire of the great Mongols Rises between the Indus and the ganes rivers taking on the role of the rajas and runnies runnies the female rajas the country's influen Nobles players race against each other in support of the Empire by developing their states into wealthy and magnificent provinces players must use their dice wisely and carefully plot where to place their workers while never underestimating the benefits of Good Karma success will bring them riches great riches and fame in their quests to become legendary rulers that's a good intro yep so let's see what is inside the Raja of gz so I have I\", \"thank you hello and welcome to meeple Mentor reviews I'm Jared and with me is Holly today we're reviewing rajas of the ganjis this game is from Hutch and r r games and it's for two to four players plays in about an hour to hour and a half recommends ages 12 and up and this is a worker placement game set in the you know the the India River Ganges uh setting and you are but all trying to gain wealth and fame uh as a Raja you know and you know exerting your influence over the region now you have uh three workers I think you start with three you can get a fourth or did I start with two and get a third I think it'd start with three and get a fourth you start with three workers you can gain up to two more um but there's three ways that you can gain workers um there's this is a game with two tracks there's an income track around one side and then a Fame track that goes starts on the other side and so they run parallel to each other in opposite directions and allows you to basically have this\", \"is somebody that has done that at the same point in the game you're going to see the difference and whoever has the most difference in the collision like whichever track has the most separation that is going to be the winner and that's how you play it raja's the ganges all right and that's how you play so welcome back for the review of rajas of the ganges so we played this base game um at least three times and with the expansion as well we did a goodie box they call them goodie boxes we did the first one uh maybe we'll talk about that in a different video but just base game uh we played it when we're on our vacation arizona our honeymoon we played it thanks latte's there we played it uh now that we got it we played it a few times um i like work replacement games in general i like how we're in this game we're using dice as a resource essentially yeah um you know different values can give you different things and you're not really assigning dice out there it's not a dice placement game\", \"the motions here you actually have quite a bit of decisions so my score for this one is an 8-7 what about you vic so a lot of the things you said i like the colors of the game i love rolling dice so i always have fun even though i don't always have the best luck at least they have a karma mechanism to adjust that dice and make changes to the dice i like the cleverness of having to think on the river track when you're going to stop you don't want to just blow through the whole river necessarily which i had done one time went for that sixth spot moved in the river and then looked back at all the stuff i didn't get and i want to do that um but i was you know thinking i want to get to the end and start scoring on karma but then you have to think about having those karma intervals like are having that amount of the value of karma to then score the uh the multipliers there that they have at the end and in the expansion which we will talk about it later but i think in the base game there's\", \"to scores you can make it more strategy going for one in particular thing to try to get more points or a lot of money later in the game and also every time you get a yield you're gonna have to take it you will not get it immediately and you will put it on your boards you can upgrade the play spaces that are there already of the same color and then when you connect it you will get that special yield and that's basically the the the add-ons it makes the game more interesting and a little bit more to think about also there are a couple of more things there are some bonus spaces on the coin track and also on the point track that you will get when you land on them for example upgrading a building getting some dice moving on the river and stuff like that and that is basically how you play routers of the Ganga so let's get it back up to the table and see what I think about this game and that is how you play Rogers on the ganges pretty simple medium-light worker placement dice managing in\", \"hello everyone this is Johannes and you are watching board gaming ramblings and today we are taking a look at Rogers of the Ganges from Inca and Marcus brand a euro game that came out at essence pealed 2017 a game I was really looking forward to playing and now I have played it and that's why I'm making this video to tell you about this game let's see how you play Rogers of the Yankees Rogers of the Ganges is a worker placement game so you're gonna take your workers you're gonna place them and do some actions like you're doing most worker placement games but this game also uses dice and some of the actions you take with your worker we'll need to discard dice some actions need dive so specific color some actions need dice with a specific pip number and some actions just want to do this card dice a die and get rid of it and some actions will give you more dice so you can use them for your actions so let's just for first talk about the point of this game because most games are about\", \"ask you a lot like so wait what am i doing and then it finally dawned on me okay you're using the value of the dices how many things you can buy or sell yeah so my score for this game it's very similar to nick 8.6 so very close to what nick scored it um it went up for me i didn't like it the first time i didn't like it but i didn't think it was that great the first time we played it but then now that i understand the game better and i feel we were had more time to explore it now we only played it with two players we haven't tried it at more than two so i'd be interested to see how this game does play with more people because it is it can be quite competitive yeah for those spots we'll play with four soon um other things uh well i feel like the replayability's there like i said you can sub out those river tiles and things like that you can go about your game in different ways you can get you can really go what vic was saying with upgrading your buildings you can go really strong in the\", \"hello welcome back to the channel and join me in a trip and Voyage to India and to visit the ganes river with rajas of the ganji a little bit of dust in the cover I want to look at pristine although it's a used copy that I would like to show everyone in the channel what is inside rajas of the gas so what is this game uh it's a bit of heavy box here so it's a 12 12 to 99 years old it's always funny when you see 99 so if you are 100 you should not play because you are too old 224 75 minutes maximum and this version is from Uchi I love the artwork that's even before knowing what this game was all about I was captured by the artwork because it's super inviting to take a look inside right so we see here um Indian God I'm not specialist I would say shiv maybe that's an name is proba in my mind shiv but I may be 100% totally Incorrect and we can see the river Grandes here and the typical monuments which are The Monuments in the game one thing that stands out immediately is this is going to be\", \"see what is inside the Raja of gz so I have I have a few expansions here that I'll leave it to the end let me show you the base game only see if I don't forget to show you that so this is the manual in different language I want to show you English let's focus on English now it's small you have the little of the components there is a very characteris first player Market here this game which is the elephant the elephant states with the first player okay that's how the board looks very visual nothing special so far the actions the end of the game and there are modular Chang that you can activate if you want the nav let me try to read that navaratnas version where you play more workers more D and special upgrades there is the ganga module where you modify the river will understand by the most you see the river there's the Mongol no the this is not an expansion this is the Mongol lexicon so a little bit of Easter year where is the Mongol Empire was the Mongol Empire great Mongol Kali so\"]]\n",
            "\u001b[0m> Running step 6cc4e6ee-77ea-4489-a0cf-1e5c7dbb154b. Step input: None\n",
            "\u001b[1;3;38;5;200mThought: The output from vectorialDbRetriever contains multiple reviews with similar sentiments. I need to analyze the reviews to identify common themes and opinions.\n",
            "Action: graphDbRetriever\n",
            "Action Input: {'query': 'Rajahs of the Ganges game mechanics', 'type': 'string'}\n",
            "\u001b[0m\u001b[1;3;34mObservation: Error: graphDbRetriever() got an unexpected keyword argument 'type'\n",
            "\u001b[0mAnswer: Error processing the query: Reached max iterations.\n",
            "------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}